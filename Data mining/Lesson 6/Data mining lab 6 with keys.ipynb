{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzSZcvASEQX-"
   },
   "source": [
    "# Data mining\n",
    "\n",
    "# Lesson 6\n",
    "\n",
    "# Graph Neural Networks (GNN) and Reinforcement Learning (RL)\n",
    "\n",
    "### **Objectives:**\n",
    "- Understand the basics of Graph Neural Networks (GNNs) and their applications.\n",
    "- Explore Reinforcement Learning (RL) concepts such as agents, rewards, and policies.\n",
    "- Implement GNNs for node classification on synthetic graph data.\n",
    "- Use RL to solve a graph traversal problem.\n",
    "\n",
    "### **Description**\n",
    "\n",
    "Graph Neural Networks (GNNs) and Reinforcement Learning (RL) are emerging tools in modern machine learning. GNNs enable effective learning on graph-structured data, such as social networks, molecular structures, or transportation systems, while RL allows for decision-making through rewards in dynamic environments.\n",
    "\n",
    "In this lab:\n",
    "\n",
    "1) We will simulate graph data and train a Graph Neural Network (GNN) to perform node classification.\n",
    "2) We will integrate Reinforcement Learning to train an agent to traverse a graph while maximizing cumulative rewards.\n",
    "\n",
    "\n",
    "### Libraries that we use:\n",
    "\n",
    "- [Torch](https://pytorch.org/) - for GNN implementation.\n",
    "- [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) - for data visualization and identifying interesting patterns.\n",
    "- [networkx](https://networkx.org/) - for graph generation and visualization.\n",
    "- [gym](https://github.com/openai/gym) - for Reinforcement Learning environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMabQkd90DVa"
   },
   "source": [
    "#### Structure: Synthetic graph dataset.\n",
    "\n",
    "We will simulate a random graph with labeled nodes for a node classification task. Each node will belong to one of N classes.\n",
    "\n",
    "- Generate a graph with networkx.\n",
    "- Add synthetic node features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "id": "S5d91on31Gu4",
    "outputId": "dcf94f27-2e68-490a-bb74-75b7f7ec6f19"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "n_nodes = 100    # Number of nodes\n",
    "n_edges = 200    # Number of edges\n",
    "n_classes = 3    # Number of node classes\n",
    "n_features = 5   # Number of node features\n",
    "\n",
    "# Generate a random graph\n",
    "G = nx.gnm_random_graph(n_nodes, n_edges, seed=42)\n",
    "\n",
    "# Assign random features to nodes\n",
    "node_features = torch.randn((n_nodes, n_features))\n",
    "\n",
    "# Assign random labels (classes) to nodes\n",
    "node_labels = torch.randint(0, n_classes, (n_nodes,))\n",
    "\n",
    "# Convert edges to tensor\n",
    "edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "\n",
    "# Create a PyTorch Geometric data object\n",
    "data = Data(x=node_features, edge_index=edge_index, y=node_labels)\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, with_labels=True, node_color=node_labels, cmap='coolwarm', node_size=100)\n",
    "plt.title(\"Synthetic Graph with Node Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3jbRK7L13Bs"
   },
   "source": [
    "## **Exercise 1:** Implement a Graph Neural Network (GNN)\n",
    "- Create a GNN model using the torch_geometric library.\n",
    "- Train the GNN to classify the nodes based on their features.\n",
    "- Evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Define a simple GNN model\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Model, optimizer, and loss\n",
    "model = GNN(in_channels=n_features, hidden_channels=16, out_channels=n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the GNN model\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluate the GNN model\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "    acc = (pred == data.y).sum().item() / n_nodes\n",
    "    return acc\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk0xQNEY2dYb"
   },
   "source": [
    "## **Exercise 2:** Reinforcement Learning on a Graph Traversal Problem\n",
    "- We will design a Reinforcement Learning environment where an agent navigates through the graph to maximize cumulative rewards.\n",
    "1. Define a graph environment: Nodes are states, edges are actions, and rewards are provided for specific nodes.\n",
    "2. Train an RL agent to navigate the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create a Custom Graph Environment Using OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owc5lQdf7if5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class GraphTraversalEnv(gym.Env):\n",
    "    def __init__(self, graph, start_node, goal_node):\n",
    "        super(GraphTraversalEnv, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.start_node = start_node\n",
    "        self.goal_node = goal_node\n",
    "        self.current_node = start_node\n",
    "\n",
    "        # Action and observation space\n",
    "        self.action_space = spaces.Discrete(len(graph.nodes))  # Move to any node\n",
    "        self.observation_space = spaces.Discrete(len(graph.nodes))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_node = self.start_node\n",
    "        return self.current_node\n",
    "\n",
    "    def step(self, action):\n",
    "        if (self.current_node, action) in self.graph.edges:\n",
    "            self.current_node = action\n",
    "\n",
    "        # Reward: +10 for reaching goal, -1 otherwise\n",
    "        reward = 10 if self.current_node == self.goal_node else -1\n",
    "        done = self.current_node == self.goal_node\n",
    "\n",
    "        return self.current_node, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Current Node: {self.current_node}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Train a Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Initialize environment\n",
    "start_node = 0\n",
    "goal_node = n_nodes - 1\n",
    "env = GraphTraversalEnv(G, start_node, goal_node)\n",
    "\n",
    "# Q-Learning parameters\n",
    "Q = defaultdict(lambda: np.zeros(len(G.nodes)))\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "episodes = 500\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # Exploit\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "        state = next_state\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3:** Visualize the Agent’s Path\n",
    "- Simulate the trained agent navigating the graph.\n",
    "- Plot the agent’s path from the start node to the goal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate agent's path\n",
    "state = env.reset()\n",
    "path = [state]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state])\n",
    "    state, _, done, _ = env.step(action)\n",
    "    path.append(state)\n",
    "\n",
    "print(f\"Agent's Path: {path}\")\n",
    "\n",
    "# Visualize the path on the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "node_colors = ['green' if i in path else 'lightblue' for i in range(n_nodes)]\n",
    "nx.draw(G, with_labels=True, node_color=node_colors, node_size=300)\n",
    "plt.title(\"Agent's Path Through the Graph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consclusion:\n",
    "\n",
    "We learned: \n",
    "\n",
    "- Understand the basics of Graph Neural Networks (GNNs) and their applications.\n",
    "- Explore Reinforcement Learning (RL) concepts such as agents, rewards, and policies.\n",
    "- Implement GNNs for node classification on synthetic graph data.\n",
    "- Use RL to solve a graph traversal problem.\n",
    "\n",
    "This lab introduces students to GNNs for node classification and RL for graph traversal, providing a solid foundation for graph-based learning tasks.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Data mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

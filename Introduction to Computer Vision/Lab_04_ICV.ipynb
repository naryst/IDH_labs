{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "305d13b7-c61d-4eb6-9d38-4be9c6727779",
      "metadata": {
        "id": "305d13b7-c61d-4eb6-9d38-4be9c6727779"
      },
      "source": [
        "# Introduction to Computer Vision. Lab 04: Back-propagation With Scalar Variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c6bc54-dad2-4834-ab83-3cbd99588087",
      "metadata": {
        "id": "a5c6bc54-dad2-4834-ab83-3cbd99588087"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this lesson, we will explore the back-propagation algorithm and how it can be applied to find optimal parameters for image classification models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fbe9397-1d49-48ce-9454-a42a3bfcc9f3",
      "metadata": {
        "id": "4fbe9397-1d49-48ce-9454-a42a3bfcc9f3"
      },
      "source": [
        "### Finding a Good $W$\n",
        "\n",
        "We saw from last week that finding a ‘good’ $W$ that minimizes the average loss function using brute force search is so time-consuming that it is almost impractical to do even for small images. Is there a mathematical approach to finding a good $W$?\n",
        "\n",
        "From optimization, you may know that there is an algorithm, based on mathematics and hence optimal, on how to minimize a convex (maximize a concave) function, which is very efficient. Is our average loss function a convex function with respect to $W$, so that we can apply that very efficient minimization algorithm? Well, that depends on the function $f(x, W)$, i.e., for some $f(x, W)$ it is and for some it is not.\n",
        "\n",
        "But even if the average loss function is a non-convex function with respect to $W$, we will again use this minimization algorithm, simply because no one invented an efficient algorithm for minimization of non-convex functions that we can use (maybe you will invent one and become famous and rich!).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9754c9-3e25-4a14-8d50-3e75499a86df",
      "metadata": {
        "id": "8d9754c9-3e25-4a14-8d50-3e75499a86df"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Create two functions: One that performs forward propagation over the softmax (without the normalization layer included). The other that performs backward propagation over the softmax (without the normalization layer included).\n",
        "\n",
        "- You can, if you wish, combine both functions into one.\n",
        "- These functions must be created for any number of labels $C$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75710e47-f9fd-4e2d-b4f6-76e8b9b2a414",
      "metadata": {
        "id": "75710e47-f9fd-4e2d-b4f6-76e8b9b2a414"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to perform forward propagation over the softmax\n",
        "def softmax_forward(logits):\n",
        "    exps = np.exp(logits)\n",
        "    return ''' TO DO '''\n",
        "\n",
        "# Function to perform backward propagation over the softmax\n",
        "def softmax_backward(softmax_output, true_labels):\n",
        "    return ''' TO DO '''\n",
        "\n",
        "# Example usage\n",
        "logits = np.array([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]])\n",
        "true_labels = np.array([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "softmax_output = softmax_forward(logits)\n",
        "print(\"Softmax Output:\\n\", softmax_output)\n",
        "\n",
        "grad = softmax_backward(softmax_output, true_labels)\n",
        "print(\"Gradient:\\n\", grad)\n",
        "\n",
        "# Assert statements to check correctness\n",
        "assert softmax_output.shape == logits.shape, \"Output shape mismatch\"\n",
        "assert grad.shape == logits.shape, \"Gradient shape mismatch\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01dff547-8821-4892-a514-53ca27dae7d7",
      "metadata": {
        "id": "01dff547-8821-4892-a514-53ca27dae7d7"
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "The algorithm that solves this task is called back-propagation. It efficiently updates the parameters of our model to minimize the loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbea43c2-b9ee-48b9-b943-7ec7b052e42b",
      "metadata": {
        "id": "fbea43c2-b9ee-48b9-b943-7ec7b052e42b"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Create functions that perform forward propagation and backward propagation over the softmax with the normalization layer included.\n",
        "\n",
        "- You can, if you wish, combine both functions into one.\n",
        "- These functions must be created for any number of labels $C$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fcb41e-69c3-4613-99c3-79f34487bbff",
      "metadata": {
        "id": "80fcb41e-69c3-4613-99c3-79f34487bbff"
      },
      "outputs": [],
      "source": [
        "# Function to perform forward propagation over the softmax with normalization\n",
        "def softmax_with_normalization_forward(logits):\n",
        "    logits_normalized = ''' TO DO '''\n",
        "    exps = ''' TO DO '''\n",
        "    return ''' TO DO '''\n",
        "\n",
        "# Function to perform backward propagation over the softmax with normalization\n",
        "def softmax_with_normalization_backward(softmax_output, true_labels):\n",
        "    return ''' TO DO '''\n",
        "\n",
        "# Example usage\n",
        "logits = np.array([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]])\n",
        "true_labels = np.array([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "softmax_output = softmax_with_normalization_forward(logits)\n",
        "print(\"Softmax Output with Normalization:\\n\", softmax_output)\n",
        "\n",
        "grad = softmax_with_normalization_backward(softmax_output, true_labels)\n",
        "print(\"Gradient with Normalization:\\n\", grad)\n",
        "\n",
        "# Assert statements to check correctness\n",
        "assert softmax_output.shape == logits.shape, \"Output shape mismatch\"\n",
        "assert grad.shape == logits.shape, \"Gradient shape mismatch\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce8ab78-ed02-4a11-9855-de9f5c51c98d",
      "metadata": {
        "id": "3ce8ab78-ed02-4a11-9855-de9f5c51c98d"
      },
      "source": [
        "### Softmax with Normalization Layer\n",
        "\n",
        "In practical scenarios, when the logits (inputs to the softmax) are very large, exponentiating them can lead to very large numbers, causing numerical instability. To counter this, we normalize the logits by subtracting the maximum logit value from each logit before applying the softmax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd4bf67-961c-4eb7-9e90-71ec2469ae07",
      "metadata": {
        "id": "5cd4bf67-961c-4eb7-9e90-71ec2469ae07"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this lesson, we explored the back-propagation algorithm and its application in optimizing parameters for image classification models. We implemented forward and backward propagation for the softmax function, both with and without normalization, and discussed the importance of normalization in preventing numerical instability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7fb4eb-0d35-4ef9-90ed-7aebf51458e5",
      "metadata": {
        "id": "0b7fb4eb-0d35-4ef9-90ed-7aebf51458e5"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "For further reading and more complex image processing techniques, consider exploring the following resources:\n",
        "\n",
        "- [OpenCV Documentation](https://docs.opencv.org/)\n",
        "- [scikit-image Documentation](https://scikit-image.org/docs/stable/)\n",
        "- [Computer Vision: Algorithms and Applications](https://szeliski.org/Book/)\n",
        "\n",
        "Feel free to search for more information and examples online to enhance your understanding of computer vision.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
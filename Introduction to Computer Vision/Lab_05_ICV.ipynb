{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1e852a8",
      "metadata": {
        "id": "d1e852a8"
      },
      "source": [
        "# **Introduction to Computer Vision. Lab 05. Backpropagation With Vectors, Matrices, and Tensors**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3cec",
      "metadata": {
        "id": "4d0b3cec"
      },
      "source": [
        "## **Theory: Softmax Function**\n",
        "The softmax function is commonly used in the output layer of a neural network to represent a categorical distribution. Given a vector of logits, the softmax function converts them into probabilities. The formula for the softmax function is:\n",
        "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
        "where $x$ is the input vector of logits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60589af",
      "metadata": {
        "id": "c60589af"
      },
      "source": [
        "## **Exercise 1: Backward Propagation for Softmax**\n",
        "For the softmax function, let\n",
        "$$\\frac{\\partial L}{\\partial \\hat{y}} = \\left[ \\frac{\\partial L}{\\partial \\hat{y}_1}, \\frac{\\partial L}{\\partial \\hat{y}_2}, ..., \\frac{\\partial L}{\\partial \\hat{y}_C} \\right]$$\n",
        "be given. Modify the previous lab code such that the output from the softmax is\n",
        "$$\\frac{\\partial L}{\\partial \\hat{x}} = \\left[ \\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}, ..., \\frac{\\partial L}{\\partial x_C} \\right]$$\n",
        "Complete the function `softmax_backward` to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efac8573",
      "metadata": {
        "id": "efac8573"
      },
      "outputs": [],
      "source": [
        "# Function to perform forward propagation over the softmax with normalization\n",
        "import numpy as np\n",
        "\n",
        "def softmax_with_normalization_forward(logits):\n",
        "    logits_normalized = ''' TO DO '''\n",
        "    exps = ''' TO DO '''\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "# Modified backward propagation for the softmax\n",
        "def softmax_with_normalization_backward(dL_dy, logits):\n",
        "    softmax_output = ''' TO DO '''\n",
        "    dL_dx = ''' TO DO '''\n",
        "    return dL_dx\n",
        "\n",
        "# Example usage\n",
        "logits = np.array([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]])\n",
        "dL_dy = np.array([[0.1, 0.2, 0.7], [0.2, 0.3, 0.5]])\n",
        "\n",
        "softmax_output = softmax_with_normalization_forward(logits)\n",
        "print(\"Softmax Output with Normalization:\\n\", softmax_output)\n",
        "\n",
        "dL_dx = softmax_with_normalization_backward(dL_dy, logits)\n",
        "print(\"Softmax Backward with Normalization (dL_dx):\\n\", dL_dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56cbba51",
      "metadata": {
        "id": "56cbba51"
      },
      "source": [
        "## **Theory: ReLU Function**\n",
        "The Rectified Linear Unit (ReLU) function is one of the most commonly used activation functions in neural networks. It is defined as:\n",
        "$$ReLU(x) = \\max(0, x)$$\n",
        "The function outputs the input directly if it is positive; otherwise, it outputs zero."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3820ad3",
      "metadata": {
        "id": "c3820ad3"
      },
      "source": [
        "## **Exercise 2: Forward and Backward Propagation for ReLU**\n",
        "Create two functions: One that performs forward propagation over the ReLU, and another that performs backward propagation over the ReLU given that\n",
        "$$\\frac{\\partial L}{\\partial z} = \\left[ \\frac{\\partial L}{\\partial z_1}, \\frac{\\partial L}{\\partial z_2}, ..., \\frac{\\partial L}{\\partial z_N} \\right]$$\n",
        "is given when $z$ is the output from the ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc461e5",
      "metadata": {
        "id": "5fc461e5"
      },
      "outputs": [],
      "source": [
        "# Function for ReLU forward propagation\n",
        "def relu_forward(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Function for ReLU backward propagation\n",
        "def relu_backward(dL_dz, z):\n",
        "    dz_dx = (z > 0).astype(float)\n",
        "    return ''' TO DO '''\n",
        "\n",
        "# Example usage\n",
        "x = np.array([[-1, 2, -3], [4, -5, 6]])\n",
        "dL_dz = np.array([[1, 0.5, -0.5], [-1, 2, -2]])\n",
        "\n",
        "z = ''' TO DO '''\n",
        "print(\"ReLU Forward:\\n\", z)\n",
        "\n",
        "dL_dx = ''' TO DO '''\n",
        "print(\"ReLU Backward (dL_dx):\\n\", dL_dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f08b09",
      "metadata": {
        "id": "77f08b09"
      },
      "source": [
        "## **Theory: Matrix Multiplication**\n",
        "Matrix multiplication is a fundamental operation in many machine learning algorithms. Given two matrices $W$ (of shape $n \\times k$) and $X$ (of shape $k \\times m$), the product $Z = WX$ is a matrix of shape $n \\times m$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d1730c",
      "metadata": {
        "id": "29d1730c"
      },
      "source": [
        "## **Exercise 3: Forward and Backward Propagation for Matrix Multiplication**\n",
        "Create two functions: One that performs forward propagation over the matrix multiplication node\n",
        "$$Z = WX$$\n",
        "where $W$ is an $n \\times k$ matrix and $X$ is a $k \\times m$ matrix. The other that performs backward propagation over the matrix multiplication node, given that\n",
        "$$\\frac{\\partial L}{\\partial Z}$$ is given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1c4562",
      "metadata": {
        "id": "ea1c4562"
      },
      "outputs": [],
      "source": [
        "# Function for matrix multiplication forward propagation\n",
        "def matmul_forward(W, X):\n",
        "    return np.dot(W, X)\n",
        "\n",
        "# Function for matrix multiplication backward propagation\n",
        "def matmul_backward(dL_dZ, W, X):\n",
        "    dL_dW = ''' TO DO '''\n",
        "    dL_dX = ''' TO DO '''\n",
        "    return ''' TO DO '''\n",
        "\n",
        "# Example usage\n",
        "W = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "X = np.array([[7, 8], [9, 10]])\n",
        "dL_dZ = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
        "\n",
        "Z = matmul_forward(W, X)\n",
        "print(\"Matrix Multiplication Forward:\\n\", Z)\n",
        "\n",
        "dL_dW, dL_dX = ''' TO DO '''\n",
        "print(\"Matrix Multiplication Backward (dL_dW):\\n\", dL_dW)\n",
        "print(\"Matrix Multiplication Backward (dL_dX):\\n\", dL_dX)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a261a8",
      "metadata": {
        "id": "69a261a8"
      },
      "source": [
        "## **Conclusion**\n",
        "In this lab, we explored the forward and backward propagation for different operations commonly used in neural networks, including the softmax function, ReLU activation, and matrix multiplication. Understanding these operations and their derivatives is crucial for implementing and debugging neural networks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1407d943-d2a2-4a7a-895d-dd912039941c",
      "metadata": {
        "id": "1407d943-d2a2-4a7a-895d-dd912039941c"
      },
      "source": [
        "# Natural Language Processing. Lesson 2. Text processing basics\n",
        "\n",
        "In this lab, we will cover a wide range of the Text Processing concepts:\n",
        "- Sentence Segmentation,\n",
        "- Lowercasing,\n",
        "- Stop Words Removal,\n",
        "- Lemmatization,\n",
        "- Stemming,\n",
        "- Byte-Pair Encoding (BPE),\n",
        "- Edit Distance.\n",
        "\n",
        "These methods help to understand how computers can work with human language. In other words, they are essential for unlocking the `meaning` hidden within text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5b5da3-110f-4c6e-8764-63cda762c57b",
      "metadata": {
        "id": "6a5b5da3-110f-4c6e-8764-63cda762c57b"
      },
      "source": [
        "## Sentence Segmentation\n",
        "\n",
        "Sentence segmentation is a fundamental step that involves dividing a block of text into individual sentences, typically separated by punctuation marks. This method was considered in the previous lesson, so you already should be familiar with it. This technique may be used in:\n",
        "- Part-of-Speech (POS): accurate boundaries between sentences are required for assigning grammatical labels like nouns, verbs, and adjectives.\n",
        "- Sentiment Analysis: understanding the sentiment (positive, negative, neutral) of a sentence also relies on exact boundaries\n",
        "\n",
        "And much more tasks need splitting the text on sentences. It can be performed using already known libraries: nltk or spaCy. Let's use nltk here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4ce80953",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ce80953",
        "outputId": "e793416d-0a60-4f2c-f463-ccb43fbd7fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "# install the required library and import it\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0178922e-cba3-41e4-aa83-acdf7fa38181",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0178922e-cba3-41e4-aa83-acdf7fa38181",
        "outputId": "25239404-ac33-4987-852c-f2587cd73c92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This is a sample text.', 'It contains multiple sentences.', 'Can we segment it?']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "text = \"This is a sample text. It contains multiple sentences. Can we segment it?\"\n",
        "\n",
        "# tokenize into sentences using nltk.sent_tokenize()\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d70f81c",
      "metadata": {
        "id": "8d70f81c"
      },
      "source": [
        "#### Task 1\n",
        "\n",
        "Complete the following code. Split the text into sentences and save tokens into the `sentences` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4775af19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4775af19",
        "outputId": "51d3ddb5-f29d-4e38-f577-f2a8a256218c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ce789fb",
      "metadata": {
        "id": "3ce789fb"
      },
      "outputs": [],
      "source": [
        "text = \"There exist some challenges for this technique. The main problem is \\\n",
        "language specificity because sentence segmentation rules can differ across \\\n",
        "languages. For example, Japanese omits spaces between words, making \\\n",
        "segmentation more complex. Punctuation ambiguity also may be the problem because \\\n",
        "of their complexity. Certain punctuation marks like ellipses (...) or colons (:) \\\n",
        "might not always indicate sentence boundaries, requiring context-aware approaches.\"\n",
        "\n",
        "sentences = ...\n",
        "print(sentences[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470d875b",
      "metadata": {
        "id": "470d875b"
      },
      "outputs": [],
      "source": [
        "assert sentences[:2] == [\n",
        "    \"There exist some challenges for this technique.\",\n",
        "    \"The main problem is language specificity because \\\n",
        "sentence segmentation rules can differ across \\\n",
        "languages.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048c1aa6-6680-4ad6-aa0d-278c782580c0",
      "metadata": {
        "id": "048c1aa6-6680-4ad6-aa0d-278c782580c0"
      },
      "source": [
        "## Lowercasing\n",
        "\n",
        "Lowercasing, as the name suggests, is the process of converting all characters in a text string to lowercase. This seemingly simple step plays a crucial role in NLP tasks for several reasons:\n",
        "- Consistency and focus on a word meaning: 'Apple' and 'apple' should be treated identically in terms of their meaning\n",
        "- Improved performance: the same trick with apples reduces the number of unique words representations. Since many NLP algorithms rely on statistical analysis, it allows to avoid overfitting to specific capitalization patterns\n",
        "- Compatibility with NLP tools: many NLP libraries and tools work primarily with lowercase text. Lowercasing ensures compatibility and avoids potential errors or inconsistencies.\n",
        "\n",
        "For applying the Lowercasing we can use a simple Python built-in function for strings: `string.lower()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ab620e1-dbf8-4e06-8966-974c54da2a19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ab620e1-dbf8-4e06-8966-974c54da2a19",
        "outputId": "b95e8ace-b896-46b8-ffb4-27098c6cbd93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is an example text.\n"
          ]
        }
      ],
      "source": [
        "text = \"ThIs Is AN ExaMple Text.\"\n",
        "\n",
        "# apply the .lower() function\n",
        "lowercased_text = text.lower()\n",
        "\n",
        "print(lowercased_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23fb0b08",
      "metadata": {
        "id": "23fb0b08"
      },
      "source": [
        "Except the .lower() the Python has .upper() function for strings. Converting all letters to their capital form is called `Uppercasing` and it also may be applied in NLP (rarely):\n",
        "- Emphasis detection: in some cases, uppercase letters can indicate emphasis in text, like headlines, slogans, or acronyms. Uppercasing can help identify potential emphasis markers\n",
        "- Specific NLP libraries: certain NLP libraries might have functionalities that work better with uppercase text, though this is less common. (Always refer to the documentation for specific tools)\n",
        "- Named entity recognition (NER): in NER tasks, proper nouns (names of people, places, organizations) are often capitalized. Uppercasing text can be a preprocessing step to highlight potential named entities, but additional checks are needed for accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15284c2c",
      "metadata": {
        "id": "15284c2c"
      },
      "source": [
        "#### Task 2.\n",
        "Fill the gaps in the following code. Convert letters in 2 strings according to the meaning in the sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b42bdf8",
      "metadata": {
        "id": "4b42bdf8"
      },
      "outputs": [],
      "source": [
        "# no additional libraries are required\n",
        "upper = \"aPplY thE uPpErCASiNg\"\n",
        "lower = \"appLy ThE LowERcAsINg\"\n",
        "\n",
        "# apply the needed functions:\n",
        "upper_result = ...\n",
        "lower_result = ...\n",
        "\n",
        "print(upper_result)\n",
        "print(lower_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a101932",
      "metadata": {
        "id": "3a101932"
      },
      "outputs": [],
      "source": [
        "assert upper_result == \"APPLY THE UPPERCASING\"\n",
        "assert lower_result == \"apply the lowercasing\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d2fb22-edf9-4e9f-9922-e446343763b7",
      "metadata": {
        "id": "e6d2fb22-edf9-4e9f-9922-e446343763b7"
      },
      "source": [
        "## Stop Words Removal\n",
        "\n",
        "Stop words are frequently occurring words that are often removed during text processing to focus on meaningful words. Examples include articles (\"the\", \"a\", \"an\"), prepositions (\"of\", \"to\", \"in\"), conjunctions (\"and\", \"but\", \"or\"), and pronouns (\"I\", \"you\", \"he\"). While these words are essential for human language construction, they often provide minimal value for NLP tasks, thus there are several reasons to get rid of them:\n",
        "- Focus on the content and improved efficiency: removing stop words allows to keep much meaning and less words amount for optimizing the algorithms\n",
        "- Statistical analysis: stop words can skew the results of statistical analysis in NLP tasks that rely on word frequency. Removing them reduces this bias and promotes to a more accurate representation of the important words\n",
        "\n",
        "More stop-words examples:\n",
        "\n",
        "![Stop words](https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab02/stop-words.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f27f6d73-270d-45da-8619-c8edd2b28800",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f27f6d73-270d-45da-8619-c8edd2b28800",
        "outputId": "f3332adb-4f97-4260-a784-c4bb3a87960e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['example', 'sentence', 'stop', 'words.']\n"
          ]
        }
      ],
      "source": [
        "# Use an available in nltk method stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# download the stop words list\n",
        "# quiet=True hides messages that .download() might display\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# retrieve the list with stop words in english\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "\n",
        "# remove all stop words using the loop\n",
        "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff136b1",
      "metadata": {
        "id": "5ff136b1"
      },
      "source": [
        "#### Task 3.\n",
        "Fill the gaps in the following cells. Get rid of stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8b50e3",
      "metadata": {
        "id": "3c8b50e3"
      },
      "outputs": [],
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# get the stop words\n",
        "stop_words = ...\n",
        "\n",
        "# remove the meaningless words\n",
        "filtered_words = ...\n",
        "\n",
        "print(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91aac5c8",
      "metadata": {
        "id": "91aac5c8"
      },
      "outputs": [],
      "source": [
        "assert filtered_words == [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3d7974",
      "metadata": {
        "id": "9e3d7974"
      },
      "source": [
        "You should always be careful with this method. Consider whether stop word removal is beneficial for your specific NLP task. Stop words removal might promote to a loss of context (\"I don't like it\" vs. \"I like it\" - the sentiment of the sentences can be lost). Use only domain-specific stop words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a96d58ac-d436-45ac-953c-83d012cd4c4b",
      "metadata": {
        "id": "a96d58ac-d436-45ac-953c-83d012cd4c4b"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization involves  reducing words to their base or dictionary form, known as the lemma. This helps to group related words together and improve the accuracy of NLP models.\n",
        "\n",
        "`Lemma` is the canonical form of a word, also referred to as its base or dictionary form (runs - run, keeps - keep, apples - apple).\n",
        "\n",
        "Lemmatization algorithms use a dictionary and morphological analysis and rules to identify the base form of a word.\n",
        "\n",
        "What for?\n",
        "- Improved accuracy: grouping words with the same meaning into their base form helps to handle different variations of the same concept\n",
        "- Reduced vocabulary size and memory usage: lemmatization reduces the number of unique words an NLP model needs to process\n",
        "\n",
        "`WordNetLemmatizer` will help us in this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3d69808e-3b41-4082-9407-53360e8f8c07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d69808e-3b41-4082-9407-53360e8f8c07",
        "outputId": "75eb3428-4aaf-4fab-8986-9e479e47b0f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['rock', 'corpus', 'cry']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"rocks\", \"corpora\", \"cries\"]\n",
        "\n",
        "# apply the lemmatizer to all words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de6457e0",
      "metadata": {
        "id": "de6457e0"
      },
      "source": [
        "#### Task 4.\n",
        "Fill the gaps. Convert given words to their original form. Use spacy. Hint: use lemma_ \\\n",
        "[Useful link](https://www.geeksforgeeks.org/python-pos-tagging-and-lemmatization-using-spacy/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62180d7",
      "metadata": {
        "id": "c62180d7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    ...\n",
        "    return lemmatized_words\n",
        "\n",
        "sentence1 = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "sentence2 = \"He loves playing and running in the park.\"\n",
        "\n",
        "expected_output1 = ['the', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n",
        "expected_output2 = ['he', 'love', 'play', 'and', 'run', 'in', 'the', 'park', '.']\n",
        "\n",
        "assert lemmatize_sentence(sentence1) == expected_output1, \"Test case 1 failed\"\n",
        "assert lemmatize_sentence(sentence2) == expected_output2, \"Test case 2 failed\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f602576b-bc1a-4744-8632-33df9c2a16b4",
      "metadata": {
        "id": "f602576b-bc1a-4744-8632-33df9c2a16b4"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming reduces words to their stems or root form, often by removing suffixes, in a more heuristic approach (running - run, jumped - jump, books - book). Similar to the Lemmatization, but what is the difference?\n",
        " - Stemming relies on suffix removal rules which might lead to a wrong word (running - runn), while Lemmatization uses the morphological analysis\n",
        " - Stemming is faster and simpler\n",
        " - Application: Stemming is more preferable when computational efficiency is a priority and general understanding of the core meaning is sufficient.\n",
        "\n",
        "Stemming and Lemmatization produce quite similar results, however there are differences:\n",
        "\n",
        "![Stemming and lemmatization](https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab02/stemming.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9af22fa2-8780-4ce9-9260-340995893579",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af22fa2-8780-4ce9-9260-340995893579",
        "outputId": "ebbd37dd-378b-4911-cd83-2f97c2445056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['run', 'rock', 'beauti']\n"
          ]
        }
      ],
      "source": [
        "# import a simple module for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# and create its instance\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"rocks\", \"beautifully\"]\n",
        "\n",
        "# apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943d33fb",
      "metadata": {
        "id": "943d33fb"
      },
      "source": [
        "#### Task 5.\n",
        "Complete the following code. Hint: use nltk word tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2918674",
      "metadata": {
        "id": "c2918674"
      },
      "outputs": [],
      "source": [
        "def stem_sentence(sentence):\n",
        "    # YOUR CODE\n",
        "    return stemmed_words\n",
        "\n",
        "\n",
        "sentence1 = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "sentence2 = \"He loves playing and running in the park.\"\n",
        "\n",
        "expected_output1 = [\n",
        "    \"the\",\n",
        "    \"quick\",\n",
        "    \"brown\",\n",
        "    \"fox\",\n",
        "    \"are\",\n",
        "    \"jump\",\n",
        "    \"over\",\n",
        "    \"the\",\n",
        "    \"lazi\",\n",
        "    \"dog\",\n",
        "    \".\",\n",
        "]\n",
        "expected_output2 = [\"he\", \"love\", \"play\", \"and\", \"run\", \"in\", \"the\", \"park\", \".\"]\n",
        "\n",
        "assert stem_sentence(sentence1) == expected_output1, \"Test case 1 failed\"\n",
        "assert stem_sentence(sentence2) == expected_output2, \"Test case 2 failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "887b62c6-1633-4f60-87c2-84797f36aa77",
      "metadata": {
        "id": "887b62c6-1633-4f60-87c2-84797f36aa77"
      },
      "source": [
        "## Byte-Pair Encoding (BPE)\n",
        "\n",
        "BPE is a technique used in Natural Language Processing (NLP) for subword tokenization. Unlike traditional tokenization that splits text into individual words, BPE breaks down text into smaller units considering the vocabulary size and morphology of the language. This approach can be particularly beneficial when dealing with large vocabularies or rare words. The algoritghm:\n",
        "1. Initial vocabulary: BPE starts with the individual characters in the text as the initial vocabulary.\n",
        "2. Merging frequent pairs: it iteratively analyzes the training text and identifies the most frequent pair of characters or subwords (considering existing merged units).\n",
        "3. Replacing pairs: this most frequent pair is replaced with a new symbol not present in the vocabulary. The new symbol represents the merged subword.\n",
        "4. Vocabulary update: the vocabulary is updated to include the newly created symbol.\n",
        "5. Repeat: steps 2-4 are repeated for a predefined number of iterations or until a desired vocabulary size is reached.\n",
        "\n",
        "Applications:\n",
        "- Machine translation: for effective handling vocabulary differences between languages\n",
        "- Text classification and summarization: BPE proves a richer representation of words and captures morphological information\n",
        "- Large Language Models (LLMs): BPE allowes LLMs to handle the vast vocabulary encountered in real-world text data\n",
        "\n",
        "The visual explanation:\n",
        "\n",
        "![Byte pair encoding](https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab02/bytepair.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7e4135cf-365f-4e28-b903-538669439f53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e4135cf-365f-4e28-b903-538669439f53",
        "outputId": "1eccd35d-a6c2-49ba-b3c4-1414cfc89a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "945b3193-5242-4e8d-a797-53dabcbd6539",
      "metadata": {
        "id": "945b3193-5242-4e8d-a797-53dabcbd6539"
      },
      "outputs": [],
      "source": [
        "# import TemplateProcessing for templates\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "# these tokens have specific meanings within the tokenizer's\n",
        "# vocabulary and are not part of the regular text\n",
        "# UNK - unknown words not encountered during training\n",
        "# CLS - indicate the beginning of a sentence\n",
        "# SEP - separates sentences\n",
        "# PAD - for padding sequences to a fixed length\n",
        "# MASK - employed in tasks like masked language modeling,\n",
        "# where certain words are masked and the model predicts them\n",
        "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "\n",
        "# TemplateProcessing's instances define a template for how the tokenizer should\n",
        "# handle text during the encoding and decoding process\n",
        "temp_proc = TemplateProcessing(\n",
        "    # single - specifies the format for encoding single sentences\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
        "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5348f7ef-20f7-4aac-9fb6-f46087066338",
      "metadata": {
        "id": "5348f7ef-20f7-4aac-9fb6-f46087066338"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import BPEDecoder\n",
        "\n",
        "# create the instance of the Tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.decoder = BPEDecoder()\n",
        "tokenizer.post_processor = temp_proc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "adc9e61d-1c9c-4c8d-84ec-149bd123d622",
      "metadata": {
        "id": "adc9e61d-1c9c-4c8d-84ec-149bd123d622"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8e5d4f76-feba-42ec-a8e3-d9f9e0c61f41",
      "metadata": {
        "id": "8e5d4f76-feba-42ec-a8e3-d9f9e0c61f41"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "nltk.download(\"gutenberg\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=5000, special_tokens=special_tokens)\n",
        "shakespeare = [\" \".join(s) for s in gutenberg.sents(\"shakespeare-macbeth.txt\")]\n",
        "tokenizer.train_from_iterator(shakespeare, trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ea8efe49-2cbe-4bf0-8f91-f6b84f1570f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea8efe49-2cbe-4bf0-8f91-f6b84f1570f8",
        "outputId": "99e6ea77-769a-4fd6-dcb7-1172f135b391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'b', 'pe', 'is', 'a', 'd', 'at', 'a', 'com', 'pre', 'ss', 'ion', 'te', 'ch', 'ni', 'que', 'use', 'd', 'in', 'n', 'lp', 'for', 'to', 'ken', 'iz', 'ation', '.', '[SEP]']\n",
            "['[CLS]', 'is', 'this', 'a', 'danger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'handle', 'toward', 'my', 'hand', '?', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    tokenizer.encode(\n",
        "        \"BPE is a data compression technique used in NLP for tokenization.\"\n",
        "    ).tokens\n",
        ")\n",
        "print(\n",
        "    tokenizer.encode(\n",
        "        \"Is this a danger which I see before me, the handle toward my hand?\"\n",
        "    ).tokens\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c88ba7",
      "metadata": {
        "id": "48c88ba7"
      },
      "source": [
        "#### Task 6.\n",
        "Complete the following code to get functions required for BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b6c80c38",
      "metadata": {
        "id": "b6c80c38"
      },
      "outputs": [],
      "source": [
        "def get_stats(ids):\n",
        "    \"\"\"\n",
        "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
        "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
        "    Optionally allows to update an existing dictionary of counts\n",
        "    \"\"\"\n",
        "    counts = {}\n",
        "    # YOUR CODE\n",
        "    # YOUR CODE\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fb0d612e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "fb0d612e",
        "outputId": "8606c8ed-04a9-42cc-b5f8-3c00b7d14caf"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0dca8e23441a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "assert get_stats([1, 2, 3, 1, 2]) == {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
        "assert get_stats([]) == {}\n",
        "assert get_stats([1]) == {}\n",
        "assert get_stats([1, 1, 1, 1]) == {(1, 1): 3}\n",
        "assert get_stats([1, 2, 1, 2, 1, 2]) == {(1, 2): 3, (2, 1): 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1b8800f8",
      "metadata": {
        "id": "1b8800f8"
      },
      "outputs": [],
      "source": [
        "def merge(ids, pair, idx):\n",
        "    \"\"\"\n",
        "    In the list of integers (ids), replace all consecutive occurrences\n",
        "    of pair with the new integer token idx\n",
        "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
        "    \"\"\"\n",
        "    newids = []\n",
        "    i = 0\n",
        "    # YOUR CODE\n",
        "    # YOUR CODE\n",
        "    return newids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec76164",
      "metadata": {
        "id": "0ec76164"
      },
      "outputs": [],
      "source": [
        "assert merge([1, 2, 3, 1, 2], (1, 2), 4) == [4, 3, 4]\n",
        "assert merge([1, 3, 4, 5], (1, 2), 4) == [1, 3, 4, 5]\n",
        "assert merge([1, 2, 3, 4, 5], (1, 2), 6) == [6, 3, 4, 5]\n",
        "assert merge([3, 4, 1, 2], (1, 2), 6) == [3, 4, 6]\n",
        "assert merge([1, 2, 1, 2, 1, 2], (1, 2), 6) == [6, 6, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a22796e-92f0-4d6b-85c0-eca37e819ee3",
      "metadata": {
        "id": "2a22796e-92f0-4d6b-85c0-eca37e819ee3"
      },
      "source": [
        "## Levenshtein edit distance\n",
        "\n",
        "Edit distance measures the similarity between two strings by counting the minimum number of operations needed to transform one string into the other.\n",
        "\n",
        "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance#Example)\n",
        "\n",
        "![Levenstein](https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab02/leven.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "76bc7092-c2be-4324-b674-2ae1e6a91b16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76bc7092-c2be-4324-b674-2ae1e6a91b16",
        "outputId": "b0932a37-64a5-4440-b2d9-42bd219db926"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "s1 = \"abc\"\n",
        "s2 = \"aec\"\n",
        "nltk.edit_distance(s1, s2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc162dc5",
      "metadata": {},
      "source": [
        "#### Task 7.\n",
        "Write a function levenshtein_distance that takes two strings as input and returns their Levenshtein distance. The Levenshtein distance is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other.\\\n",
        "You can use the following [link](https://www.baeldung.com/cs/levenshtein-distance-computation#:~:text=Levenshtein%20distance%20is%20the%20smallest,insertions%2C%20deletions%2C%20and%20substitutions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "aae6754f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def levenshteinDistance(A, B):\n",
        "    N, M = len(A), len(B)\n",
        "\n",
        "    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n",
        "\n",
        "    # Base Case: When N = 0\n",
        "    for j in range(M + 1):\n",
        "        dp[0][j] = j\n",
        "    # YOUR CODE\n",
        "\n",
        "    return dp[N][M]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "57641fee",
      "metadata": {},
      "outputs": [],
      "source": [
        "s1 = \"kitty\"\n",
        "s2 = \"kotyy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "96dbff96",
      "metadata": {},
      "outputs": [],
      "source": [
        "assert levenshteinDistance(s1, s2) == nltk.edit_distance(s1, s2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7357f91",
      "metadata": {
        "id": "c7357f91"
      },
      "source": [
        "# Useful links:\n",
        "- https://github.com/karpathy/minbpe.git BPE implementation from Genius\n",
        "- https://arxiv.org/pdf/1508.07909.pdf BPE article"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d4a169b-7437-43fe-9bb7-f705037289f4",
      "metadata": {
        "id": "9d4a169b-7437-43fe-9bb7-f705037289f4"
      },
      "source": [
        "# Task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d9fb6a-d744-4950-a9de-9b03febf7e70",
      "metadata": {
        "id": "34d9fb6a-d744-4950-a9de-9b03febf7e70"
      },
      "source": [
        "[Competition](https://www.kaggle.com/t/6dcb6f9def724f9f82050e9092952dd6)\n",
        "\n",
        "The aim of the competition is to count the 10 most frequent words in the plays presented in the `data.txt` file.\n",
        "\n",
        "In order to count the frequent words correctly, you must perform lemmatization and remove stop words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7c6461-6389-4c36-be91-63c493ee1e13",
      "metadata": {
        "id": "aa7c6461-6389-4c36-be91-63c493ee1e13"
      },
      "outputs": [],
      "source": [
        "with open(\"data.txt\") as f:\n",
        "    data = f.read()\n",
        "plays = data.split(\"\\n\")\n",
        "plays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b669c3d-5f63-460c-af95-ff71bc42f597",
      "metadata": {
        "id": "2b669c3d-5f63-460c-af95-ff71bc42f597"
      },
      "outputs": [],
      "source": [
        "plays_dict = {}\n",
        "\n",
        "for play in plays:\n",
        "    plays_dict[play] = gutenberg.raw(play)\n",
        "    print(play, len(plays_dict[play]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bababbbd-c25e-4c02-bcfd-3dcb69f107b5",
      "metadata": {
        "id": "bababbbd-c25e-4c02-bcfd-3dcb69f107b5"
      },
      "outputs": [],
      "source": [
        "def top_frequent_words(text, topk=10):\n",
        "    # your implementation\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f4d7ab-915d-4b6b-a537-6373f47e8a86",
      "metadata": {
        "id": "c2f4d7ab-915d-4b6b-a537-6373f47e8a86"
      },
      "outputs": [],
      "source": [
        "top_words = {}\n",
        "for play, text in plays_dict.items():\n",
        "    top_words[play] = top_frequent_words(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e1d600-f6cb-4df8-9522-52810cd349bc",
      "metadata": {
        "id": "a5e1d600-f6cb-4df8-9522-52810cd349bc"
      },
      "outputs": [],
      "source": [
        "with open(\"submission.csv\", \"w\") as f:\n",
        "    f.write(\"id,count\\n\")\n",
        "    for play, counts in top_words.items():\n",
        "        for i, count in enumerate(counts):\n",
        "            f.write(f\"{play}_{i},{count[1]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ee6243",
      "metadata": {
        "id": "38ee6243"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059ba844",
      "metadata": {
        "id": "059ba844"
      },
      "source": [
        "In this lesson basic text preprocessing techniques were considered. We've explored a range of fundamental methods that serve as the building blocks for Natural Language Processing (NLP) tasks:\n",
        "    \n",
        "- **Sentence** Segmentation: We delved into the art of dividing text into meaningful units â€“ sentences. This crucial step allows us to analyze the structure and meaning within each sentence.\n",
        "- **Lowercasing**: By converting all text to lowercase, we ensure consistency and facilitate comparisons between words, streamlining various NLP processes.\n",
        "- **Stop Word Removal**: We tackled the challenge of high-frequency, uninformative words (stop words) by filtering them out. This step helps focus on the core meaning of the text and improves the performance of NLP models.\n",
        "- **Lemmatization**: We explored the concept of reducing words to their base or dictionary forms (lemmas). This technique enhances consistency and allows us to capture the core meaning regardless of inflectional variations.\n",
        "- **Stemming**: As an alternative to lemmatization, we investigated stemming, which reduces words to their base forms but might not always result in actual words. Stemming offers a balance between efficiency and accuracy.\n",
        "- **Byte-Pair Encoding (BPE)**: We ventured into the world of subword units by exploring BPE. This technique breaks down words into smaller units, particularly valuable for handling rare words or complex vocabularies in large datasets.\n",
        "- **Edit Distance**: We introduced the concept of edit distance, a metric for measuring the similarity between two sequences of text. This measure finds applications in tasks like spell checking, machine translation evaluation, and identifying textual variations.\n",
        "\n",
        "By mastering these methods, you've gained the ability to clean, normalize, and analyze textual data effectively. This foundation empowers you to tackle more advanced NLP tasks and unlock the hidden treasures of meaning within language. Remember, the choice of technique depends on your specific NLP needs, and often a combination of these methods leads to optimal results.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

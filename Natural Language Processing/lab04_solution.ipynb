{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIJNHvIuedKy"
      },
      "source": [
        "# NLP. Week 4. Language models. N-grams\n",
        "\n",
        "## Probabilistic Language Models -\n",
        "\n",
        "designed to predict the likelihood of a sequence of words.\n",
        "\n",
        "----------\n",
        "\n",
        "Language models offer a way to **assign a probability to a sentence** or other sequence of words, and to predict a word from preceding words.\n",
        "- Machine Translation: P(high winds tonite) > P(large winds tonite)\n",
        "- Spell Correction: The office is about fifteen **minuets** from my house. P(about fifteen minutes from) > P(about fifteen minuets from)\n",
        "- Speech Recognition: P(I saw a van) >> P(eyes awe of an)\n",
        "\n",
        "## Types of Language Models\n",
        "There are primarily two types of Language Models:\n",
        "\n",
        "1. Statistical Language Models: these models use traditional statistical techniques like N-grams, Hidden Markov Models (HMM) and certain linguistic rules to learn the probability distribution of words\n",
        "2. Neural Language Models: these use different kinds of Neural Networks to model language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VO3fhiTedK0"
      },
      "source": [
        "# Statistical Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCk4vQpHedK1"
      },
      "source": [
        "### N-grams -\n",
        "\n",
        "probabilistic language models (Markov models) that estimate the likelihood of a word based on the preceding N-1 words. In other words, they model the conditional probability of a word given its context. There are unigram (consideration of a single word), bi gram (2 words), trigram (3 words), etc.\n",
        "\n",
        "> An N-gram is a sequence of N tokens (or words)\n",
        "\n",
        "\n",
        "![Uni/bi/tri grams](https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab04/Ngram.png)\n",
        "\n",
        "* **n-gram** language models are evaluated extrinsically in some task, or intrinsically using perplexity\n",
        "\n",
        "\n",
        "#### Example\n",
        "\n",
        "Consider the following sentence: `“Innopolis University is a university located in the city of Innopolis.”`\n",
        "\n",
        "* A 1-gram (or unigram) is a one-word sequence. For the above sentence, the unigrams would simply be: “Innopolis“,  “University“, “is“, “a“, “university“, “located“, “in“, “the“, “city“, “of“, “Innopolis“.\n",
        "\n",
        "* A 2-gram (or bigram) is a two-word sequence of words, like “Innopolis University”, “university located”, or “located in”.\n",
        "\n",
        "\n",
        "### Model\n",
        "\n",
        "> If we have a good N-gram model, we can predict `p(w | h)` — the probability of seeing the word w given a history of previous words `h` — where the history contains `n-1` words.\n",
        "\n",
        "\n",
        "Probability of a sentence after applying chain rule, assuming each word is independent of others:\n",
        "$$ P(x_0...x_m) = P(x_0) * P(x_1|x_0) * P(x_2|x_0x_1) ... = \\prod_0^{m-1}{P(x_i|x_0...x_{i-1})} \\text{, where } m \\text{ - the sentence length.}$$\n",
        "\n",
        "N-gram model simplifies this by limiting the preceding text to length N:\n",
        "\n",
        "$$ P(x_m|x_0...x_{m-1}) \\approx P(x_m|x_{m-n}...x_{m-1})$$\n",
        "\n",
        "For a trigram model, this is:\n",
        "\n",
        "$$P(x_0x_1x_2 ... x_m) = P(x_0) * P(x_1|x_0) * P(x_2|x_0x_1) * \\prod_{i=3}^{m-1}P(x_i|x_{i-2}x_{i-1})$$\n",
        "\n",
        "\n",
        "### Estimation of probabilities\n",
        "The probabilities in n-gram models are typically estimated using maximum likelihood estimation (MLE) from a training corpus. For a bigram model, this is:\n",
        "$$ P(x_i|x_{i-1}) = \\frac{Count(x_{i-1}, x_i)}{Count(x_{i-1})}$$\n",
        "where Count() function defines:\n",
        "- $Count(x_{i-1})$ - number of times the word $x_{i-1}$​ appears in the training corpus\n",
        "- $Count(x_{i-1}, x_i)$ - number of times the word pair $(x_{i-1}, x_i)$ appears in the training corpus\n",
        "\n",
        "For russian speakers:\n",
        "> [The link](https://ru.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0) for understanding of estimation of probabilities\n",
        "\n",
        "### Smoothing techniques\n",
        "\n",
        "To handle zero probabilities for unseen n-grams in the training data, various smoothing techniques are used:\n",
        "1. Additive Smoothing (Laplace Smoothing):\n",
        "$P(x_i​∣x_{i−1​})=\\frac{Count(x_{i−1​},x_i​)+\\alpha}{Count(x_{i−1​})+\\alpha*V​}$\n",
        "2. Good-Turing Smoothing: adjusts the probability of unseen events based on the frequency of frequencies.\n",
        "3. Kneser-Ney Smoothing: a more advanced method that adjusts probabilities based on both the frequency of words and the diversity of contexts in which they appear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTN93NrTedK1"
      },
      "source": [
        "### Advantages and Application\n",
        "N-grams models are:\n",
        "- Simple\n",
        "- Efficient in terms of computation and storage for smaller n\n",
        "- Suffer from data sparsity with high N, because many possible word sequences will not appear in the training data\n",
        "- Limited by fixed context window size (n−1) decreasing the ability to capture long-range dependencies\n",
        "\n",
        "Application:\n",
        "- Speech recognition: rredicts the likelihood of word sequences to improve transcription accuracy\n",
        "- Machine translation: helps in generating fluent translations by predicting probable word sequences in the target language\n",
        "- Text prediction: used in keyboards and text editors to suggest the next word or phrase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJPWJIW7edK1"
      },
      "source": [
        "### Task 1.\n",
        "Given the .txt file with some text. Count the probability of a randomly chosen word to be the exact one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI4bObyLedK2",
        "outputId": "080734e4-10ec-4553-a99d-593086af5e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-30 09:38:59--  https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/big.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‘big.txt.1’\n",
            "\n",
            "big.txt.1           100%[===================>]   6.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-30 09:38:59 (64.7 MB/s) - ‘big.txt.1’ saved [6488666/6488666]\n",
            "\n",
            "--2024-06-30 09:39:00--  https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/w2_.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16660652 (16M) [text/plain]\n",
            "Saving to: ‘w2_.txt.1’\n",
            "\n",
            "w2_.txt.1           100%[===================>]  15.89M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-30 09:39:00 (134 MB/s) - ‘w2_.txt.1’ saved [16660652/16660652]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/big.txt\n",
        "!wget https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/w2_.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OiRzyRC-edK3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# dictionary {(word):(its amount in the text)}\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word: str, N=sum(WORDS.values())) -> float:\n",
        "    \"\"\" Probability of `word` computation function.\n",
        "\n",
        "    Args:\n",
        "        word (str): the word for which the probability need to be found\n",
        "        N (int): amount of all words in the text. Defaults to sum(WORDS.values()).\n",
        "\n",
        "    Returns:\n",
        "        float: the probability\n",
        "    \"\"\"\n",
        "    return WORDS[word] / N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RMLddyVedK3",
        "outputId": "896e002e-7475-4d71-d5cc-1e9efa333d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00023485435892379335\n",
            "0.0002895341905816231\n",
            "0.0014772518454443185\n",
            "0.0036420353446846273\n",
            "0.018935356785901566\n"
          ]
        }
      ],
      "source": [
        "print(P('name'))\n",
        "print(P('woman'))\n",
        "print(P('man'))\n",
        "print(P('this'))\n",
        "print(P('a'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Xu6WAGMAedK3"
      },
      "outputs": [],
      "source": [
        "def almost_equal(x,y,threshold=0.0001):\n",
        "  return abs(x-y) < threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UrfKqmFRedK3"
      },
      "outputs": [],
      "source": [
        "assert almost_equal(P('name'), 0.0002348, 0.000001)\n",
        "assert almost_equal(P('woman'), 0.0002895, 0.000001)\n",
        "assert almost_equal(P('man'), 0.0014772, 0.000001)\n",
        "assert almost_equal(P('this'), 0.0036420, 0.000001)\n",
        "assert almost_equal(P('a'), 0.0189353, 0.000001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acrEXjz6edK3"
      },
      "source": [
        "### Task 2.\n",
        "Among a given list of random words choose only those which appear in the 'big.txt' file. `Hint:` use the WORDS dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tvGuh5rTedK3"
      },
      "outputs": [],
      "source": [
        "def known(words):\n",
        "    \"Return the subset of `words` that appear in the 'big.txt'\"\n",
        "    return set(w for w in words if w in WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dw0Nu0cmedK4"
      },
      "outputs": [],
      "source": [
        "assert known(['this', 'is', 'my', 'test', 'adc']) == {'is', 'my', 'test', 'this'}\n",
        "assert known(['this', 'message', 'car', 'price', 'computer', 'pytorch']) == {'this', 'message', 'car', 'price', 'computer'}\n",
        "assert known(['tensor', 'wandb', 'float', 'binary']) == {'float', 'binary'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rh5y12qedK4"
      },
      "source": [
        "### Task 3.\n",
        "\n",
        "Given the file 'w2_.txt'. It contains 1020385 lines. Each line is in the format: \"int`\\t`word`\\t`word'\". Count the amount of each word-pair (bigram) in this file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WROjWKZCedK4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "bigrams = defaultdict(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UQ8Kj8OMedK4"
      },
      "outputs": [],
      "source": [
        "with open('w2_.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "    for line in lines:\n",
        "        line = line.strip().split('\\t')\n",
        "\n",
        "        frequency = int(line[0])\n",
        "\n",
        "        bigram = tuple(line[1:])\n",
        "\n",
        "        bigrams[bigram] += frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "W4eayn4YedK4"
      },
      "outputs": [],
      "source": [
        "assert bigrams[('a', 'bombing')] == 320\n",
        "assert bigrams[('a', 'most')] == 1988\n",
        "assert bigrams[('c', 'can')] == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-1X5joSedK4"
      },
      "source": [
        "## Model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:25.563088Z",
          "iopub.status.busy": "2024-02-09T10:09:25.562389Z",
          "iopub.status.idle": "2024-02-09T10:09:25.568382Z",
          "shell.execute_reply": "2024-02-09T10:09:25.567587Z",
          "shell.execute_reply.started": "2024-02-09T10:09:25.563048Z"
        },
        "trusted": true,
        "id": "9kOPG_jgedK4"
      },
      "outputs": [],
      "source": [
        "# for those of you who run locally\n",
        "# import nltk\n",
        "# nltk.download('reuters', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:25.570745Z",
          "iopub.status.busy": "2024-02-09T10:09:25.570011Z",
          "iopub.status.idle": "2024-02-09T10:09:28.774974Z",
          "shell.execute_reply": "2024-02-09T10:09:28.773971Z",
          "shell.execute_reply.started": "2024-02-09T10:09:25.570707Z"
        },
        "trusted": true,
        "id": "WIazhJXwedK4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# for those of you who run on kaggle\n",
        "import subprocess\n",
        "import nltk\n",
        "# Download and unzip reuters\n",
        "try:\n",
        "    nltk.data.find('reuters.zip')\n",
        "except:\n",
        "    nltk.download('reuters', download_dir='/kaggle/working/', quiet=True, force=True)\n",
        "    command = \"unzip /kaggle/working/corpora/reuters.zip -d /kaggle/working/corpora\"\n",
        "    result = subprocess.run(command.split(), capture_output = True, text = True )\n",
        "    nltk.data.path.append('/kaggle/working/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:28.776914Z",
          "iopub.status.busy": "2024-02-09T10:09:28.776211Z",
          "iopub.status.idle": "2024-02-09T10:09:45.132703Z",
          "shell.execute_reply": "2024-02-09T10:09:45.131485Z",
          "shell.execute_reply.started": "2024-02-09T10:09:28.776874Z"
        },
        "trusted": true,
        "id": "5G8FTJxIedK5"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "# Create a placeholder for model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance\n",
        "for sentence in reuters.sents():\n",
        "    lower_sentence = [word.lower() for word in sentence]\n",
        "    for word1, word2, word3 in trigrams(lower_sentence, pad_right=True, pad_left=True):\n",
        "        model[(word1, word2)][word3] += 1\n",
        "\n",
        "\n",
        "# Let's transform the counts to probabilities\n",
        "for word1_word2 in model:\n",
        "    total_count = float(sum(model[word1_word2].values()))\n",
        "    for word3 in model[word1_word2]:\n",
        "        model[word1_word2][word3] /= total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:45.136446Z",
          "iopub.status.busy": "2024-02-09T10:09:45.136111Z",
          "iopub.status.idle": "2024-02-09T10:09:45.146311Z",
          "shell.execute_reply": "2024-02-09T10:09:45.144894Z",
          "shell.execute_reply.started": "2024-02-09T10:09:45.136419Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Nv4P2iedK5",
        "outputId": "6e2eaa40-9036-4ad8-ca92-d61397bd8e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today the options ( for china , two pct , of gencorp ' s land assets are 5 . 48 mln in the quarter ended may 31 net shr profit 78 . 7 billion stg fall in 1987 brazil ' s foreign exchange controls is not granted samjens would win last week that ferruzzi was considering taking the midpoint of usda ' s plan but have not been enough to warrant an early round of talks in london .\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "    # select a random probability threshold\n",
        "    probability_threshold = random.random()\n",
        "    accumulator = .0\n",
        "\n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        "\n",
        "        # select words that are above the probability threshold\n",
        "        if accumulator >= probability_threshold:\n",
        "            text.append(word)\n",
        "            break\n",
        "\n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        "\n",
        "print (' '.join([t for t in text if t]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Y_4XRPedK5"
      },
      "source": [
        "## Alternative model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:45.148390Z",
          "iopub.status.busy": "2024-02-09T10:09:45.147971Z",
          "iopub.status.idle": "2024-02-09T10:09:45.167059Z",
          "shell.execute_reply": "2024-02-09T10:09:45.165962Z",
          "shell.execute_reply.started": "2024-02-09T10:09:45.148352Z"
        },
        "trusted": true,
        "id": "EDpNzYVtedK5"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "\n",
        "#  remove the n-grams with removable words\n",
        "def remove_stopwords(ngrams, removal_list):\n",
        "    y = []\n",
        "    for pair in ngrams:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in removal_list:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return (y)\n",
        "\n",
        "def pick_word(counter):\n",
        "    \"Chooses a random element.\"\n",
        "    return random.choice(list(counter.elements()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:45.169361Z",
          "iopub.status.busy": "2024-02-09T10:09:45.168197Z",
          "iopub.status.idle": "2024-02-09T10:09:45.494992Z",
          "shell.execute_reply": "2024-02-09T10:09:45.493881Z",
          "shell.execute_reply.started": "2024-02-09T10:09:45.169329Z"
        },
        "trusted": true,
        "id": "LE2MnSVeedK5"
      },
      "outputs": [],
      "source": [
        "# input the reuters sentences\n",
        "sents  = reuters.sents()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# write the removal characters such as : Stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—'\n",
        "string.punctuation\n",
        "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:45.497356Z",
          "iopub.status.busy": "2024-02-09T10:09:45.496949Z",
          "iopub.status.idle": "2024-02-09T10:09:56.215896Z",
          "shell.execute_reply": "2024-02-09T10:09:56.214500Z",
          "shell.execute_reply.started": "2024-02-09T10:09:45.497324Z"
        },
        "trusted": true,
        "id": "GGZYgLBwedK5"
      },
      "outputs": [],
      "source": [
        "# generate unigrams bigrams trigrams\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "tokenized_text=[]\n",
        "for sentence in sents:\n",
        "    sentence = list(map(lambda x:x.lower(),sentence))\n",
        "    for word in sentence:\n",
        "        if word== '.':\n",
        "            sentence.remove(word)\n",
        "        else:\n",
        "            unigram.append(word)\n",
        "\n",
        "    tokenized_text.append(sentence)\n",
        "    bigram.extend(list(nltk.ngrams(sentence, 2,pad_left=True, pad_right=True)))\n",
        "    trigram.extend(list(nltk.ngrams(sentence, 3, pad_left=True, pad_right=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:09:56.218052Z",
          "iopub.status.busy": "2024-02-09T10:09:56.217724Z",
          "iopub.status.idle": "2024-02-09T10:10:40.848286Z",
          "shell.execute_reply": "2024-02-09T10:10:40.847054Z",
          "shell.execute_reply.started": "2024-02-09T10:09:56.218023Z"
        },
        "trusted": true,
        "id": "qr0_NtuGedK5"
      },
      "outputs": [],
      "source": [
        "unigram = remove_stopwords(unigram, removal_list)\n",
        "bigram = remove_stopwords(bigram,removal_list)\n",
        "trigram = remove_stopwords(trigram,removal_list)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:10:40.850368Z",
          "iopub.status.busy": "2024-02-09T10:10:40.849917Z",
          "iopub.status.idle": "2024-02-09T10:10:45.535341Z",
          "shell.execute_reply": "2024-02-09T10:10:45.534334Z",
          "shell.execute_reply.started": "2024-02-09T10:10:40.850328Z"
        },
        "trusted": true,
        "id": "YalntwmUedK5"
      },
      "outputs": [],
      "source": [
        "# generate frequency of n-grams\n",
        "freq_bi = FreqDist(bigram)\n",
        "freq_tri = FreqDist(trigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:10:45.537245Z",
          "iopub.status.busy": "2024-02-09T10:10:45.536895Z",
          "iopub.status.idle": "2024-02-09T10:10:56.258097Z",
          "shell.execute_reply": "2024-02-09T10:10:56.256877Z",
          "shell.execute_reply.started": "2024-02-09T10:10:45.537216Z"
        },
        "trusted": true,
        "id": "cjsxKZFwedK5"
      },
      "outputs": [],
      "source": [
        "d = defaultdict(Counter)\n",
        "for a, b, c in freq_tri:\n",
        "    if(a != None and b!= None and c!= None):\n",
        "        d[(a, b)] += {(a, b, c) : freq_tri[a, b, c]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-09T10:10:56.259866Z",
          "iopub.status.busy": "2024-02-09T10:10:56.259508Z",
          "iopub.status.idle": "2024-02-09T10:10:56.269801Z",
          "shell.execute_reply": "2024-02-09T10:10:56.268446Z",
          "shell.execute_reply.started": "2024-02-09T10:10:56.259836Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGCYxR7YedK6",
        "outputId": "6501aeb1-bbc8-4efc-b217-826bf6876efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today the\n",
            "today the overseas\n",
            "today the overseas offshoot\n",
            "today the overseas offshoot of\n",
            "today the overseas offshoot of its\n",
            "today the overseas offshoot of its west\n",
            "today the overseas offshoot of its west german\n",
            "today the overseas offshoot of its west german short\n",
            "today the overseas offshoot of its west german short -\n",
            "today the overseas offshoot of its west german short - range\n",
            "today the overseas offshoot of its west german short - range computers\n",
            "today the overseas offshoot of its west german short - range computers fell\n",
            "today the overseas offshoot of its west german short - range computers fell about\n",
            "today the overseas offshoot of its west german short - range computers fell about 13\n",
            "today the overseas offshoot of its west german short - range computers fell about 13 8\n",
            "today the overseas offshoot of its west german short - range computers fell about 13 8 billion\n",
            "today the overseas offshoot of its west german short - range computers fell about 13 8 billion dlrs\n",
            "today the overseas offshoot of its west german short - range computers fell about 13 8 billion dlrs from\n",
            "today the overseas offshoot of its west german short - range computers fell about 13 8 billion dlrs from 3\n",
            "today the overseas offshoot of its west german short - range computers fell about 13 8 billion dlrs from 3 05\n"
          ]
        }
      ],
      "source": [
        "# Next word prediction\n",
        "s=''\n",
        "\n",
        "prefix = \"today\", \"the\"\n",
        "print(\" \".join(prefix))\n",
        "s = \" \".join(prefix)\n",
        "for i in range(19):\n",
        "    suffix = pick_word(d[prefix])[-1]\n",
        "    s=s+' '+suffix\n",
        "    print(s)\n",
        "    prefix = prefix[1], suffix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsEtXZtmedK6"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this lesson we started diving into Language Models and considered the 1st type - Statistical LMs (N-grams).\n",
        "\n",
        "N-grams play a pivotal role in natural language processing by enabling the analysis of contiguous sequences of words.\n",
        "- By breaking down text into sequences of n consecutive words or characters, we can estimate the probabilities of these sequences to predict and generate text.\n",
        "- By estimating the probabilities of these n-grams, we can build robust models for various NLP tasks such as text generation, speech recognition, and machine translation.\n",
        "- Understanding and implementing n-grams is essential for creating systems that can effectively interpret and generate human language, forming the foundation for more advanced NLP techniques and applications. As we move forward, mastering these basic concepts will empower us to tackle more complex challenges in the field of natural language processing.\n",
        "\n",
        "> As we delve deeper into the complexities of language, n-grams serve as a stepping stone to more advanced models, such as neural networks and transformers. Mastering n-grams provides a solid foundation for understanding how machines can process and generate human language."
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7686177,
          "sourceId": 70220,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
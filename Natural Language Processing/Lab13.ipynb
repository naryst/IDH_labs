{"cells":[{"cell_type":"markdown","metadata":{"id":"4b09a54b","papermill":{"duration":0.012021,"end_time":"2024-04-11T18:10:20.969870","exception":false,"start_time":"2024-04-11T18:10:20.957849","status":"completed"},"tags":[]},"source":["# NLP. Lesson 13. LLMs. Fine-tuning with LORA\n"]},{"cell_type":"markdown","metadata":{"id":"68203ae4"},"source":["## LLM\n","\n","A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and perform various NLP tasks.\n","\n","### Generation with LLMs\n","\n","LLMs, or Large Language Models, are the key component behind text generation. In a nutshell, they consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text. Since they predict one token at a time, you need to do something more elaborate to generate new sentences other than just calling the model — you need to do autoregressive generation.\n","\n","**Autoregressive generation** is the inference-time procedure of iteratively calling a model with its own generated outputs, given a few initial inputs.\n","\n","### Autoregressive generation\n","\n","> Casual language modeling - predicts the next token in a sequence of tokens, and the model can only attend to tokens on the left. This means the model cannot see future tokens. GPT-2 is an example of a causal language model.\n","\n","A language model trained for causal language modeling takes a sequence of text tokens as input and returns the probability distribution for the next token.\n","\n","A critical aspect of autoregressive generation with LLMs is how to select the next token from this probability distribution. Anything goes in this step as long as you end up with a token for the next iteration. This means it can be as simple as selecting the most likely token from the probability distribution or as complex as applying a dozen transformations before sampling from the resulting distribution. The process of autoregressive prediction is repeated iteratively until some stopping condition is reached. Ideally, the stopping condition is dictated by the model, which should learn when to output an end-of-sequence (EOS) token. If this is not the case, generation stops when some predefined maximum length is reached.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/AutoRegressiveGen.png\" alt=\"Autoregressive generation\" width=\"800\"/>\n","\n","\n","Some cool LLM links: [HF Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), [HF coding assistant](https://huggingface.co/spaces/HuggingFaceH4/starchat2-playground)\n","\n","LLMs: [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom), [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#gemma), [LLaMa](https://huggingface.co/docs/transformers/model_doc/llama), [GPT-2](https://huggingface.co/openai-community/gpt2)\n"]},{"cell_type":"markdown","metadata":{"id":"ce592163"},"source":["## Parameter Efficient Fine Tuning (PEFT)\n","\n","The traditional fine tuning method, in which all parameters of a pre-trained model are tuned, becomes impractical and computationally expensive when working with modern LLM models.\n","\n","PEFT is a technique designed to fine-tune models while minimizing the need for extensive resources and cost. PEFT is a great choice when dealing with domain-specific tasks that necessitate model adaptation. By employing PEFT, we can strike a balance between retaining valuable knowledge from the pre-trained model and adapting it effectively to the target task with fewer parameters. There are various ways of achieving Parameter efficient fine-tuning. Low Rank Parameter or LoRA & QLoRA are most widely used and effective.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/LLMtraining.png\" alt=\"LLM training and tuning\" width=\"800\"/>\n","\n","\n","#### Why PEFT?\n","- **Accelerated training time:** PEFT allows you to reduce the amount of time spent on training by fine-tuning a small number of parameters rather than the entire model.\n","- **Reduced Compute and Storage Costs:** PEFT fine-tunes only a small subset of parameters, significantly reducing compute and storage costs and reducing hardware requirements.\n","- **Less risk of overfitting:** By freezing most of the parameters of the pre-trained model, we can avoid overfitting on new data.\n","- **Overcoming catastrophic forgetting:** With PEFT, the model can adapt to new tasks while retaining previously learned knowledge by freezing most parameters.\n","\n","#### Based on their operations, PEFT algorithms can be categorized into:\n","1. **Additive finetuning** - the parameters of the pre-trained model are supplemented with new ones, and training takes place on them, while the original data is frozen. Prefix finetuning is based on this approach (check the picture).\n","    \n","    - Adapter-based Fine-tuning: This approach involves insertion of small adapter layers within Transformer blocks. Since during fine-tuning, only a minimal number of trainable parameters that are strategically positioned within the model architecture are updated, that results in reduction of storage, memory and compute requirements.\n","    - Soft Prompt-based Fine-tuning: Approach to refine model and improve its performance via fine-tuning. As part of this approach, adjustable vectors known as soft prompts are appended to the start of input sequence.\n","\n","Additive PEFT:\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/AdditivePEFT.png\" alt=\"Additive finetuning\" width=\"300\"/>\n","\n","2. **Selective PEFT** - Instead of adding more parameters as done in additive PEFT, this approach finetunes a subset of the existing parameters to enhance model performance over downstream tasks.\n","    - Structural masking: Structured mask organize parameter masking in regular patterns, unlike unstructured ones that apply it randomly, thus can enhances computational and hardware efficiency during training.\n","    - Unstructural masking\n"]},{"cell_type":"markdown","metadata":{"id":"35ada1b2"},"source":["#### Some PEFT techniques:\n","\n","- LoRA - Low-Rank Adaptation\n","- Prefix tuning - uses additive method. We add a sequence of training vectors (continuous task-specific vectors), called a prefix, to each transformer block and train only it, without touching the rest of the data. Prefix parameters are inserted in all of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The creators of this method, based on the results of the experiment based on GPT-2, concluded that, by training only 0.1% of parameters, Prefix tuning shows performance comparable to additional training, in which all parameters of the model are adjusted, and is superior to it when fine tuning with small volume of data.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/PrefixTuning.png\" alt=\"Prefix tuning\" width=\"800\"/>\n","\n","- Prompt tuning - additive method, soft-prompt based, simplified version of Prefix tuning. Prompt tokens have their own parameters that are updated independently. This means you can keep the pretrained model’s parameters frozen, and only update the gradients of the prompt token embeddings.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/SoftPrompt.png\" alt=\"Prompt finetuning\" width=\"800\"/>\n","\n","- Adapter - additive method, similar to the Prefix tuning (adapters are added, not prefixes). Adds extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training.\n","Structure: Within the adapter, the original d-dimensional features are first projected into the smaller dimension m, then nonlinearity is applied, and then projected back into the d-dimensional dimension. There is also a skip connection here.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/Adapters.png\" alt=\"Adapter Architecture\" width=\"800\"/>\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/AdapterInTransformer.png\" alt=\"Adapter\" width=\"800\"/>\n","\n","How is this parameter efficient? For example, assume the first fully connected layer projects a 1024-dimensional input down to 24 dimensions, and the second fully connected layer projects it back into 1024 dimensions. This means we introduced 1,024 x 24 + 24 x 1,024 = 49,152 weight parameters. In contrast, a single fully connected layer that reprojects a 1024-dimensional input into a 1,024-dimensional space would have 1,024 x 1024 = 1,048,576 parameters.\n","\n","Follow [this link](https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters#:~:text=The%20idea%20of%20parameter%2Defficient,the%20pretrained%20LLM%20remain%20frozen.) for a better understanding and code examples.\n","\n","\n","- P-tuning - adds a trainable embedding tensor that can be optimized to find better prompts, and it uses a prompt encoder (a bidirectional long-short term memory network or LSTM) to optimize the prompt parameters.\n","- IA3 - rescales inner activations with learned vectors. These learned vectors are injected in the attention and feedforward modules in a typical transformer-based architecture.\n","\n","Some useful links: [Adapters](https://huggingface.co/docs/peft/en/conceptual_guides/adapter), [Soft prompts](https://huggingface.co/docs/peft/en/conceptual_guides/prompting), [IA3](https://huggingface.co/docs/peft/en/conceptual_guides/ia3)\n","\n","More theory about Efficient Tuning is [here](https://vinija.ai/nlp/parameter-efficient-fine-tuning/)"]},{"cell_type":"markdown","metadata":{"id":"c78845a3"},"source":["## LoRA\n","\n","[Paper](https://arxiv.org/pdf/2106.09685.pdf)\n","\n","Low-Rank Adaptation (LoRA) is a reparametrization method that aims to reduce the number of trainable parameters with low-rank representations. The weight matrix is broken down into low-rank matrices that are trained and updated. All the pretrained model parameters remain frozen. After training, the low-rank matrices are added back to the original weights. This makes it more efficient to store and train a LoRA model because there are significantly fewer parameters.\n","\n","In other words: it simplifies the fine-tuning of large models by **decomposing** complex, high-dimensional weight matrices into lower-dimensional forms. This technique, akin to methods like PCA and SVD, allows for the retention of critical information while significantly reducing the size and complexity of the weights, thus enhancing fine-tuning efficiency on resource-constrained settings.\n","\n","Benefits: This approach offers considerable time and memory efficiency, as a large portion of the model’s parameters are kept frozen, reducing both training time and GPU memory requirements. It also avoids additional inference latency and facilitates easy task-switching during deployment, requiring changes only in a small subset of weights.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/FineTuning.png\" alt=\"LoRA\" width=\"507\"/>\n","\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/LoRAandFinetuning.png\" alt=\"LoRA\" width=\"600\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"8643febc"},"source":["### LoRA configuration\n","\n","By using LoRA, you are unfreezing the attention `Weight_delta` matrix and only updating `W_a` and `W_b`.\n","\n","<img src=\"https://files.training.databricks.com/images/llm/lora.png\" width=500>\n","\n","You can treat `r` (rank) as a hyperparameter. LoRA can perform well with very small ranks based on [Hu et a 2021's paper](https://arxiv.org/abs/2106.09685). GPT-3's validation accuracies across tasks with ranks from 1 to 64 are quite similar.\n","\n","From [PyTorch Lightning's documentation](https://lightning.ai/pages/community/article/lora-llm/):\n","\n","> A smaller `r` leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation. This can lead to faster training and potentially reduced computational requirements. However, with a smaller `r`, the capacity of the low-rank matrix to capture task-specific information decreases. This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher `r`.\n","\n","Other arguments:\n","\n","- `lora_dropout`:\n","  - Dropout is a regularization method that reduces overfitting by randomly and temporarily removing nodes during training.\n","  - It works like this: <br>\n","    - Apply to most type of layers (e.g. fully connected, convolutional, recurrent) and larger networks\n","    - Temporarily and randomly remove nodes and their connections during each training cycle\n","- `target_modules`:\n","  - Specifies the module names to apply to\n","  - This is dependent on how the foundation model names its attention weight matrices.\n","  - Typically, this can be:\n","    - `query`, `q`, `q_proj`\n","    - `key`, `k`, `k_proj`\n","    - `value`, `v` , `v_proj`\n","    - `query_key_value`\n"]},{"cell_type":"markdown","metadata":{"id":"bc081e39"},"source":["### QLoRA\n","\n","[Paper](https://arxiv.org/pdf/2305.14314.pdf), [QLoRA repository](https://github.com/artidoro/qlora), [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n","\n","QLoRA (Quantized Low-Rank Adaptation) extends LoRA to enhance efficiency by quantizing weight values of the original network, from high-resolution data types, such as Float32, to lower-resolution data types like int4. This leads to reduced memory demands and faster calculations.\n","\n","### Key optimizations of QLoRA\n","\n","1. **4-bit NF4 Quantization**. 4-bit NormalFloat4 is an optimized data type that can be used to store weights, which brings down the memory footprint considerably.\n","2. **Normalization & Quantization**. As part of normalization and quantization steps, the weights are adjusted to a zero mean, and a constant unit variance. A 4-bit data type can only store 16 numbers. As part of normalization the weights are mapped to these 16 numbers, zero-centered distributed, and instead of storing the weights, the nearest position is stored.\n","3. **Double quantization**. Double quantization is the process of quantizing the quantization constant to reduce the memory down further to save these constant. To perform dequantization technique we need to store the quantization constants. If we employed blockwise quantization, then we will have n quantization constants in their original datatype. In the case of expansive LLM’s which have substantial number of quantization constants that must be stored, leading to increased memory overhead.\n","\n","<img src=\"https://raw.githubusercontent.com/Dnau15/LabImages/main/images/lab13/LoraQLora.png\" alt=\"LoRA and QLoRA\" width=\"800\"/>\n","\n","In the QLoRA approach, it is the original model’s weights that are quantized to 4-bit precision. The newly added Low-rank Adapter (LoRA) weights are not quantized; they remain at a higher precision and are fine-tuned during the training process. This strategy allows for efficient memory use while maintaining the performance of large language models during finetuning.\n"]},{"cell_type":"markdown","metadata":{"id":"3c4eee87"},"source":["## Code\n","\n","Let's fine-tune LLaMa2 for sentiment analysis on Financial News\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-04T20:28:44.760223Z","iopub.status.busy":"2024-08-04T20:28:44.759444Z","iopub.status.idle":"2024-08-04T20:29:09.533717Z","shell.execute_reply":"2024-08-04T20:29:09.532568Z","shell.execute_reply.started":"2024-08-04T20:28:44.760188Z"},"id":"4873dd3a","outputId":"f95edfc0-df12-4a2b-83a1-7eca6e0c4c3e","papermill":{"duration":21.895023,"end_time":"2024-04-11T18:10:42.876558","exception":false,"start_time":"2024-04-11T18:10:20.981535","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q peft transformers datasets evaluate seqeval accelerate bitsandbytes trl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:29:09.536088Z","iopub.status.busy":"2024-08-04T20:29:09.535760Z","iopub.status.idle":"2024-08-04T20:29:09.540673Z","shell.execute_reply":"2024-08-04T20:29:09.539702Z","shell.execute_reply.started":"2024-08-04T20:29:09.536057Z"},"id":"70ee1046","papermill":{"duration":0.01904,"end_time":"2024-04-11T18:10:42.907732","exception":false,"start_time":"2024-04-11T18:10:42.888692","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:29:09.542225Z","iopub.status.busy":"2024-08-04T20:29:09.541959Z","iopub.status.idle":"2024-08-04T20:29:27.364003Z","shell.execute_reply":"2024-08-04T20:29:27.362988Z","shell.execute_reply.started":"2024-08-04T20:29:09.542203Z"},"id":"c91b8d63","papermill":{"duration":19.489485,"end_time":"2024-04-11T18:11:02.410153","exception":false,"start_time":"2024-04-11T18:10:42.920668","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-04 20:29:17.271848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-04 20:29:17.271957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-04 20:29:17.387447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import Dataset\n","from peft import LoraConfig, PeftConfig\n","from trl import SFTTrainer\n","from trl import setup_chat_format\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:29:27.367553Z","iopub.status.busy":"2024-08-04T20:29:27.366518Z","iopub.status.idle":"2024-08-04T20:29:27.372358Z","shell.execute_reply":"2024-08-04T20:29:27.371348Z","shell.execute_reply.started":"2024-08-04T20:29:27.367514Z"},"id":"ca11e65c","papermill":{"duration":0.020307,"end_time":"2024-04-11T18:11:02.442717","exception":false,"start_time":"2024-04-11T18:11:02.422410","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"132fba46","papermill":{"duration":0.01155,"end_time":"2024-04-11T18:11:02.466104","exception":false,"start_time":"2024-04-11T18:11:02.454554","status":"completed"},"tags":[]},"source":["### Data preparation\n","\n","Read initial data and perform preprocessing: split into train and validation splits, insert prompts to text of news.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-04T20:29:27.374442Z","iopub.status.busy":"2024-08-04T20:29:27.373868Z","iopub.status.idle":"2024-08-04T20:29:29.008003Z","shell.execute_reply":"2024-08-04T20:29:29.007038Z","shell.execute_reply.started":"2024-08-04T20:29:27.374408Z"},"id":"649638d2","outputId":"d405f535-93eb-40a9-dece-36e49dc5a1a7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-08-04 20:29:28--  https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/financial_news.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 672006 (656K) [application/octet-stream]\n","Saving to: 'financial_news.csv'\n","\n","financial_news.csv  100%[===================>] 656.26K  --.-KB/s    in 0.05s   \n","\n","2024-08-04 20:29:28 (12.1 MB/s) - 'financial_news.csv' saved [672006/672006]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/financial_news.csv"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"execution":{"iopub.execute_input":"2024-08-04T20:29:29.010090Z","iopub.status.busy":"2024-08-04T20:29:29.009699Z","iopub.status.idle":"2024-08-04T20:29:29.046484Z","shell.execute_reply":"2024-08-04T20:29:29.045685Z","shell.execute_reply.started":"2024-08-04T20:29:29.010051Z"},"id":"ba27813b","outputId":"a3f90199-6a31-42ba-bf33-69e600583f7c","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>neutral</td>\n","      <td>According to Gran , the company has no plans t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neutral</td>\n","      <td>Technopolis plans to develop in stages an area...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>negative</td>\n","      <td>The international electronic industry company ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>positive</td>\n","      <td>With the new production plant the company woul...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>positive</td>\n","      <td>According to the company 's updated strategy f...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  sentiment                                               text\n","0   neutral  According to Gran , the company has no plans t...\n","1   neutral  Technopolis plans to develop in stages an area...\n","2  negative  The international electronic industry company ...\n","3  positive  With the new production plant the company woul...\n","4  positive  According to the company 's updated strategy f..."]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\n","    \"financial_news.csv\",\n","    names=[\"sentiment\", \"text\"],\n","    encoding=\"utf-8\",\n","    encoding_errors=\"replace\",\n",")\n","df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:29:29.048069Z","iopub.status.busy":"2024-08-04T20:29:29.047812Z","iopub.status.idle":"2024-08-04T20:29:30.406761Z","shell.execute_reply":"2024-08-04T20:29:30.405930Z","shell.execute_reply.started":"2024-08-04T20:29:29.048046Z"},"id":"181ea2be","papermill":{"duration":1.376639,"end_time":"2024-04-11T18:11:03.854222","exception":false,"start_time":"2024-04-11T18:11:02.477583","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train = list()\n","X_test = list()\n","for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n","    train, test = train_test_split(\n","        df[df.sentiment == sentiment], train_size=300, test_size=300, random_state=42\n","    )\n","    X_train.append(train)\n","    X_test.append(test)\n","\n","X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n","X_test = pd.concat(X_test)\n","\n","eval_idx = [\n","    idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)\n","]\n","X_eval = df[df.index.isin(eval_idx)]\n","X_eval = X_eval.groupby(\"sentiment\", group_keys=False).apply(\n","    lambda x: x.sample(n=50, random_state=10, replace=True)\n",")\n","X_train = X_train.reset_index(drop=True)\n","\n","\n","def generate_prompt(data_point):\n","    return f\"\"\"\n","            Analyze the sentiment of the news headline enclosed in square brackets,\n","            determine if it is positive, neutral, or negative, and return the answer as\n","            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n","\n","            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n","            \"\"\".strip()\n","\n","\n","def generate_test_prompt(data_point):\n","    return f\"\"\"\n","            Analyze the sentiment of the news headline enclosed in square brackets,\n","            determine if it is positive, neutral, or negative, and return the answer as\n","            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n","\n","            [{data_point[\"text\"]}] = \"\"\".strip()\n","\n","\n","X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), columns=[\"text\"])\n","X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), columns=[\"text\"])\n","\n","y_true = X_test.sentiment\n","X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n","\n","train_data = Dataset.from_pandas(X_train)\n","eval_data = Dataset.from_pandas(X_eval)"]},{"cell_type":"markdown","metadata":{"id":"ecb64456","papermill":{"duration":0.011437,"end_time":"2024-04-11T18:11:03.877615","exception":false,"start_time":"2024-04-11T18:11:03.866178","status":"completed"},"tags":[]},"source":["### Evaluation\n","\n","Evaluation function for model\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:29:30.408118Z","iopub.status.busy":"2024-08-04T20:29:30.407841Z","iopub.status.idle":"2024-08-04T20:29:30.417053Z","shell.execute_reply":"2024-08-04T20:29:30.416201Z","shell.execute_reply.started":"2024-08-04T20:29:30.408094Z"},"id":"9a19dc53","papermill":{"duration":0.023513,"end_time":"2024-04-11T18:11:03.912566","exception":false,"start_time":"2024-04-11T18:11:03.889053","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def evaluate(y_true, y_pred):\n","    labels = [\"positive\", \"neutral\", \"negative\"]\n","    mapping = {\"positive\": 2, \"neutral\": 1, \"none\": 1, \"negative\": 0}\n","\n","    def map_func(x):\n","        return mapping.get(x, 1)\n","\n","    y_true = np.vectorize(map_func)(y_true)\n","    y_pred = np.vectorize(map_func)(y_pred)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n","    print(f\"Accuracy: {accuracy:.3f}\")\n","\n","    # Generate accuracy report\n","    unique_labels = set(y_true)  # Get unique labels\n","\n","    for label in unique_labels:\n","        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n","        label_y_true = [y_true[i] for i in label_indices]\n","        label_y_pred = [y_pred[i] for i in label_indices]\n","        accuracy = accuracy_score(label_y_true, label_y_pred)\n","        print(f\"Accuracy for label {label}: {accuracy:.3f}\")\n","\n","    # Generate classification report\n","    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n","    print(\"\\nClassification Report:\")\n","    print(class_report)\n","\n","    # Generate confusion matrix\n","    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n","    print(\"\\nConfusion Matrix:\")\n","    print(conf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"dc5a77a3","papermill":{"duration":0.011577,"end_time":"2024-04-11T18:11:03.935549","exception":false,"start_time":"2024-04-11T18:11:03.923972","status":"completed"},"tags":[]},"source":["### Load model\n","\n","Let's create a BitsAndBytesConfig object with the following settings and load LLM with quantization config:\n","\n","- `load_in_4bit`: Load the model weights in 4-bit format.\n","- `bnb_4bit_quant_type`: Use the \"nf4\" quantization type. 4-bit NormalFloat (NF4), is a new data type that is information theoretically optimal for normally distributed weights.\n","- `bnb_4bit_compute_dtype`: Use the float16 data type for computations.\n","- `bnb_4bit_use_double_quant`: Do not use double quantization (reduces the average memory footprint by quantizing also the quantization constants and saves an additional 0.4 bits per parameter.).\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"execution":{"iopub.execute_input":"2024-08-04T20:29:30.418726Z","iopub.status.busy":"2024-08-04T20:29:30.418294Z","iopub.status.idle":"2024-08-04T20:32:38.080180Z","shell.execute_reply":"2024-08-04T20:32:38.079412Z","shell.execute_reply.started":"2024-08-04T20:29:30.418695Z"},"id":"94339816","outputId":"d0a0e85c-1318-445c-dad3-7b48e924fa61","papermill":{"duration":249.017354,"end_time":"2024-04-11T18:15:12.964560","exception":false,"start_time":"2024-04-11T18:11:03.947206","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19640a2b9305448a8f3fe24b9e8ae144","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"\n","compute_dtype = getattr(torch, \"float16\")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=device,\n","    torch_dtype=compute_dtype,\n","    quantization_config=bnb_config,\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","model, tokenizer = setup_chat_format(model, tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"d1a7e2e7","papermill":{"duration":0.011764,"end_time":"2024-04-11T18:15:12.988466","exception":false,"start_time":"2024-04-11T18:15:12.976702","status":"completed"},"tags":[]},"source":["### Predict\n","\n","Perfrom inference of model\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:32:38.084601Z","iopub.status.busy":"2024-08-04T20:32:38.084055Z","iopub.status.idle":"2024-08-04T20:32:38.091304Z","shell.execute_reply":"2024-08-04T20:32:38.090439Z","shell.execute_reply.started":"2024-08-04T20:32:38.084575Z"},"id":"2ba7bd5e","papermill":{"duration":0.023439,"end_time":"2024-04-11T18:15:13.023604","exception":false,"start_time":"2024-04-11T18:15:13.000165","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def predict(test, model, tokenizer):\n","    y_pred = []\n","    for i in tqdm(range(len(X_test))):\n","        prompt = X_test.iloc[i][\"text\"]\n","        pipe = pipeline(\n","            task=\"text-generation\",\n","            model=model,\n","            tokenizer=tokenizer,\n","            max_new_tokens=1,\n","            temperature=0.0,\n","        )\n","        result = pipe(prompt)\n","        answer = result[0][\"generated_text\"].split(\"=\")[-1]\n","        if \"positive\" in answer:\n","            y_pred.append(\"positive\")\n","        elif \"negative\" in answer:\n","            y_pred.append(\"negative\")\n","        elif \"neutral\" in answer:\n","            y_pred.append(\"neutral\")\n","        else:\n","            y_pred.append(\"none\")\n","    return y_pred"]},{"cell_type":"markdown","metadata":{"id":"dcd8d97b"},"source":["### Model predictions without fine-tuning\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T20:32:38.092504Z","iopub.status.busy":"2024-08-04T20:32:38.092256Z"},"id":"2a6e4689","papermill":{"duration":328.043797,"end_time":"2024-04-11T18:20:41.079307","exception":false,"start_time":"2024-04-11T18:15:13.035510","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 407/900 [01:35<01:55,  4.26it/s]"]}],"source":["y_pred = predict(test, model, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57cbb445","papermill":{"duration":0.105496,"end_time":"2024-04-11T18:20:41.265769","exception":false,"start_time":"2024-04-11T18:20:41.160273","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["evaluate(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"6e07385e","papermill":{"duration":0.080626,"end_time":"2024-04-11T18:20:41.427783","exception":false,"start_time":"2024-04-11T18:20:41.347157","status":"completed"},"tags":[]},"source":["### Fine-tuning\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"827e6c69","papermill":{"duration":3.003841,"end_time":"2024-04-11T18:20:44.512682","exception":false,"start_time":"2024-04-11T18:20:41.508841","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["output_dir = \"trained_weigths\"\n","\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    target_modules=\"all-linear\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,  # directory to save and repository id\n","    num_train_epochs=3,  # number of training epochs\n","    per_device_train_batch_size=1,  # batch size per device during training\n","    gradient_accumulation_steps=8,  # number of steps before performing a backward/update pass\n","    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=0,\n","    logging_steps=25,  # log every 10 steps\n","    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n","    weight_decay=0.001,\n","    fp16=True,\n","    bf16=False,\n","    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n","    max_steps=-1,\n","    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n","    group_by_length=True,\n","    lr_scheduler_type=\"cosine\",  # use cosine learning rate scheduler\n","    report_to=\"tensorboard\",  # report metrics to tensorboard\n","    evaluation_strategy=\"epoch\",  # save checkpoint every epoch\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_arguments,\n","    train_dataset=train_data,\n","    eval_dataset=eval_data,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    max_seq_length=1024,\n","    packing=False,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,\n","        \"append_concat_token\": False,\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22bfbac7","papermill":{"duration":3661.482596,"end_time":"2024-04-11T19:21:46.081274","exception":false,"start_time":"2024-04-11T18:20:44.598678","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92a00f73","papermill":{"duration":1.464786,"end_time":"2024-04-11T19:21:47.647242","exception":false,"start_time":"2024-04-11T19:21:46.182456","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["trainer.save_model()\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"markdown","metadata":{"id":"e51e59da","papermill":{"duration":0.08224,"end_time":"2024-04-11T19:21:47.813373","exception":false,"start_time":"2024-04-11T19:21:47.731133","status":"completed"},"tags":[]},"source":["### Prediction of fine-tuned model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff7508ae","papermill":{"duration":0.133543,"end_time":"2024-04-11T19:21:48.028914","exception":false,"start_time":"2024-04-11T19:21:47.895371","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# delete and call garbage collector to free memory\n","import gc\n","\n","del [\n","    model,\n","    tokenizer,\n","    peft_config,\n","    trainer,\n","    train_data,\n","    eval_data,\n","    bnb_config,\n","    training_arguments,\n","]\n","del [df, X_train, X_eval]\n","del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe9d7ea6","papermill":{"duration":26.411987,"end_time":"2024-04-11T19:22:14.523156","exception":false,"start_time":"2024-04-11T19:21:48.111169","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# empty cuda cache several times\n","for _ in range(100):\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"1c6cc5c2"},"source":["### Load trained model and merge with pretrained\n","\n","`merge_and_unload` - merges LoRA adapters into base model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bb57a7e","papermill":{"duration":68.156966,"end_time":"2024-04-11T19:23:22.764964","exception":false,"start_time":"2024-04-11T19:22:14.607998","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","\n","finetuned_model = \"./trained_weigths/\"\n","compute_dtype = getattr(torch, \"float16\")\n","tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\")\n","\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    finetuned_model,\n","    torch_dtype=compute_dtype,\n","    return_dict=False,\n","    low_cpu_mem_usage=True,\n","    device_map=device,\n",")\n","\n","merged_model = model.merge_and_unload()\n","merged_model.save_pretrained(\n","    \"./merged_model\", safe_serialization=True, max_shard_size=\"2GB\"\n",")\n","tokenizer.save_pretrained(\"./merged_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a470a1e2","papermill":{"duration":228.592201,"end_time":"2024-04-11T19:27:11.565929","exception":false,"start_time":"2024-04-11T19:23:22.973728","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["y_pred = predict(test, merged_model, tokenizer)\n","evaluate(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a28fccd8","papermill":{"duration":27.193811,"end_time":"2024-04-11T19:27:38.919429","exception":false,"start_time":"2024-04-11T19:27:11.725618","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["del [merged_model, tokenizer, y_pred, model, y_true, test]\n","\n","for _ in range(100):\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d8de6b0","papermill":{"duration":1.22767,"end_time":"2024-04-11T19:27:40.303931","exception":false,"start_time":"2024-04-11T19:27:39.076261","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:27.311294Z","iopub.status.busy":"2024-08-04T21:26:27.310497Z","iopub.status.idle":"2024-08-04T21:26:30.088435Z","shell.execute_reply":"2024-08-04T21:26:30.087461Z","shell.execute_reply.started":"2024-08-04T21:26:27.311261Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["--2024-08-04 21:26:28--  https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/train.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3733324 (3.6M) [text/plain]\n","Saving to: 'train.csv'\n","\n","train.csv           100%[===================>]   3.56M  --.-KB/s    in 0.08s   \n","\n","2024-08-04 21:26:28 (47.3 MB/s) - 'train.csv' saved [3733324/3733324]\n","\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["--2024-08-04 21:26:29--  https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/test.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 651623 (636K) [text/plain]\n","Saving to: 'test.csv'\n","\n","test.csv            100%[===================>] 636.35K  --.-KB/s    in 0.05s   \n","\n","2024-08-04 21:26:29 (12.4 MB/s) - 'test.csv' saved [651623/651623]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/train.csv\n","!wget https://raw.githubusercontent.com/Dnau15/LabImages/main/data/txt_data/test.csv"]},{"cell_type":"markdown","metadata":{},"source":["## Task 1\n","Read csv files using pandas, divide sentences into tokens and convert tags into int."]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:30.090743Z","iopub.status.busy":"2024-08-04T21:26:30.090395Z","iopub.status.idle":"2024-08-04T21:26:30.497736Z","shell.execute_reply":"2024-08-04T21:26:30.496469Z","shell.execute_reply.started":"2024-08-04T21:26:30.090711Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","import pandas as pd\n","\n","df = pd.read_csv(\"/kaggle/input/nlp-week-13-fine-tuning-with-lora/train.csv\")\n","df[\"tokens\"] = # YOUR CODE\n","df[\"tags\"] = # YOUR CODE\n","dataset = Dataset.from_pandas(df)\n","splitted_dataset = dataset.train_test_split(test_size=0.2)\n","\n","assert all(isinstance(tokens, list) for tokens in df[\"tokens\"]), \"Not all entries in 'tokens' are lists.\"\n","assert all(isinstance(tag_list, list) and all(isinstance(tag, int) for tag in tag_list) for tag_list in df[\"tags\"]), \"Not all entries in 'tags' are lists of integers.\""]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:32.759347Z","iopub.status.busy":"2024-08-04T21:26:32.758456Z","iopub.status.idle":"2024-08-04T21:26:32.764440Z","shell.execute_reply":"2024-08-04T21:26:32.763371Z","shell.execute_reply.started":"2024-08-04T21:26:32.759313Z"},"trusted":true},"outputs":[],"source":["label2id = {\n","    \"O\": 0,\n","    \"B-DNA\": 1,\n","    \"I-DNA\": 2,\n","    \"B-protein\": 3,\n","    \"I-protein\": 4,\n","    \"B-cell_type\": 5,\n","    \"I-cell_type\": 6,\n","    \"B-cell_line\": 7,\n","    \"I-cell_line\": 8,\n","    \"B-RNA\": 9,\n","    \"I-RNA\": 10,\n","}"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:33.236494Z","iopub.status.busy":"2024-08-04T21:26:33.236127Z","iopub.status.idle":"2024-08-04T21:26:33.242441Z","shell.execute_reply":"2024-08-04T21:26:33.241522Z","shell.execute_reply.started":"2024-08-04T21:26:33.236448Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import (\n","    AutoModelForTokenClassification,\n","    AutoTokenizer,\n","    DataCollatorForTokenClassification,\n","    TrainingArguments,\n","    Trainer,\n",")\n","from peft import (\n","    get_peft_config,\n","    PeftModel,\n","    PeftConfig,\n","    get_peft_model,\n","    LoraConfig,\n","    TaskType,\n",")\n","import evaluate\n","import torch\n","import numpy as np\n","\n","model_checkpoint = \"roberta-base\"\n","lr = 1e-3\n","batch_size = 32\n","num_epochs = 10"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:33.772557Z","iopub.status.busy":"2024-08-04T21:26:33.772193Z","iopub.status.idle":"2024-08-04T21:26:33.936281Z","shell.execute_reply":"2024-08-04T21:26:33.935344Z","shell.execute_reply.started":"2024-08-04T21:26:33.772528Z"},"trusted":true},"outputs":[{"data":{"text/plain":["RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["from tokenizers.pre_tokenizers import WhitespaceSplit\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n","tokenizer.pre_tokenizer = WhitespaceSplit()\n","tokenizer"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:15:02.064638Z","iopub.status.busy":"2024-08-04T21:15:02.064265Z","iopub.status.idle":"2024-08-04T21:15:02.069440Z","shell.execute_reply":"2024-08-04T21:15:02.068277Z","shell.execute_reply.started":"2024-08-04T21:15:02.064605Z"}},"source":["## Task 2\n","Check work of the tokenizer\n","Don't forget about padding"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:34.702411Z","iopub.status.busy":"2024-08-04T21:26:34.701577Z","iopub.status.idle":"2024-08-04T21:26:34.708937Z","shell.execute_reply":"2024-08-04T21:26:34.707938Z","shell.execute_reply.started":"2024-08-04T21:26:34.702379Z"},"trusted":true},"outputs":[],"source":["texts = [\"Hi guys!\", \"This is a test one sentence.\"]\n","\n","# Tokenize the texts\n","tokenized_outputs = # YOUR CODE\n","assert (tokenized_outputs['input_ids'] == torch.Tensor([[0,12289,1669,328,2,1,1,1,1],[0,152,16,10,1296,65,3645,4,2]])).all()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Task 3\n","Fill gaps and check code \\\n","Hint: If word_idx is none append(-100) "]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:35.138757Z","iopub.status.busy":"2024-08-04T21:26:35.138181Z","iopub.status.idle":"2024-08-04T21:26:35.145834Z","shell.execute_reply":"2024-08-04T21:26:35.144924Z","shell.execute_reply.started":"2024-08-04T21:26:35.138727Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples[\"tokens\"], truncation=True, is_split_into_words=True\n","    )\n","\n","    labels = []\n","    for i, label in enumerate(examples[f\"tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                # YOUR CODE\n","            elif word_idx != previous_word_idx:\n","                # YOUR CODE\n","            else:\n","                # YOUR CODE\n","            # YOUR CODE\n","        # YOUR CODE\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:35.842987Z","iopub.status.busy":"2024-08-04T21:26:35.842682Z","iopub.status.idle":"2024-08-04T21:26:35.849973Z","shell.execute_reply":"2024-08-04T21:26:35.849106Z","shell.execute_reply.started":"2024-08-04T21:26:35.842963Z"},"trusted":true},"outputs":[],"source":["examples = {\n","    \"tokens\": [[\"Hello\", \"world\"], [\"This\", \"is\", \"a\", \"test\"]],\n","    \"tags\": [[1, 0], [1, 0, 0, 0]]\n","}\n","\n","# Applying the function\n","tokenized_data = # YOUR CODE\n","\n","assert len(tokenized_data[\"input_ids\"]) == len(examples[\"tokens\"]), \"The number of tokenized inputs should match the number of examples.\"\n","\n","assert tokenized_data['input_ids'] == [[0, 20920, 232, 2], [0, 152, 16, 10, 1296, 2]]\n","assert tokenized_data['attention_mask'] == [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:35.906051Z","iopub.status.busy":"2024-08-04T21:26:35.905566Z","iopub.status.idle":"2024-08-04T21:26:39.651730Z","shell.execute_reply":"2024-08-04T21:26:39.650638Z","shell.execute_reply.started":"2024-08-04T21:26:35.906027Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1da15809f1384e16bb35a3b61bad8ba6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/13295 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d13c55f2c6e74e518272c85cd194d734","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3324 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_splitted_dataset = splitted_dataset.map(\n","    tokenize_and_align_labels, batched=True\n",")"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:39.654019Z","iopub.status.busy":"2024-08-04T21:26:39.653643Z","iopub.status.idle":"2024-08-04T21:26:39.658965Z","shell.execute_reply":"2024-08-04T21:26:39.657971Z","shell.execute_reply.started":"2024-08-04T21:26:39.653985Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Task 4\n","Complete function to compute metrics"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:39.660295Z","iopub.status.busy":"2024-08-04T21:26:39.659983Z","iopub.status.idle":"2024-08-04T21:26:39.968138Z","shell.execute_reply":"2024-08-04T21:26:39.967362Z","shell.execute_reply.started":"2024-08-04T21:26:39.660267Z"},"trusted":true},"outputs":[],"source":["seqeval = evaluate.load(\"seqeval\")\n","label_list = list(label2id.keys())\n","\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        # YOUR CODE\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = # YOUR CODE\n","    return {\n","        \"precision\": # YOUR CODE\n","        \"recall\": # YOUR CODE\n","        \"f1\": # YOUR CODE\n","        \"accuracy\": # YOUR CODE\n","    }"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:20:25.586399Z","iopub.status.busy":"2024-08-04T21:20:25.585688Z","iopub.status.idle":"2024-08-04T21:20:25.590048Z","shell.execute_reply":"2024-08-04T21:20:25.589161Z","shell.execute_reply.started":"2024-08-04T21:20:25.586358Z"}},"source":["## Task 5\n","Complete function to convert id to label"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:39.970333Z","iopub.status.busy":"2024-08-04T21:26:39.970040Z","iopub.status.idle":"2024-08-04T21:26:40.322876Z","shell.execute_reply":"2024-08-04T21:26:40.322017Z","shell.execute_reply.started":"2024-08-04T21:26:39.970308Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["id2label = # YOUR CODE\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint, num_labels=11, id2label=id2label, label2id=label2id\n",")\n","\n","id_sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","\n","expected_labels = [\n","    \"O\",        # ID 0\n","    \"B-DNA\",    # ID 1\n","    \"I-DNA\",    # ID 2\n","    \"B-protein\", # ID 3\n","    \"I-protein\", # ID 4\n","    \"B-cell_type\", # ID 5\n","    \"I-cell_type\", # ID 6\n","    \"B-cell_line\", # ID 7\n","    \"I-cell_line\", # ID 8\n","    \"B-RNA\",    # ID 9\n","    \"I-RNA\"     # ID 10\n","]\n","\n","# Convert IDs to labels using id2label mapping\n","converted_labels = [id2label[id_] for id_ in id_sequence]\n","\n","# Assertions to verify the correctness of the conversion\n","assert converted_labels == expected_labels, \"The converted labels do not match the expected labels.\"\n","\n","# Additional assertions to verify individual conversions\n","for id_, expected_label in zip(id_sequence, expected_labels):\n","    assert id2label[id_] == expected_label, f\"ID {id_} should map to label '{expected_label}' but maps to '{id2label[id_]}'.\""]},{"cell_type":"markdown","metadata":{},"source":["## Task 6\n","Fill in the gaps and train the model. Try to get the best scores.\n","\n","Baseline \n","- F1 score: 0.77\n","- Accuracy: 0.94\n","\n","Hint: you can change rank, alpha, dropout, etc."]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:40.324234Z","iopub.status.busy":"2024-08-04T21:26:40.323943Z","iopub.status.idle":"2024-08-04T21:26:40.329125Z","shell.execute_reply":"2024-08-04T21:26:40.328264Z","shell.execute_reply.started":"2024-08-04T21:26:40.324209Z"},"trusted":true},"outputs":[],"source":["peft_config = LoraConfig(\n","    task_type=# YOUR CODE\n","    # YOUR CODE ...\n",")"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:40.330516Z","iopub.status.busy":"2024-08-04T21:26:40.330211Z","iopub.status.idle":"2024-08-04T21:26:40.385544Z","shell.execute_reply":"2024-08-04T21:26:40.384849Z","shell.execute_reply.started":"2024-08-04T21:26:40.330484Z"},"trusted":true},"outputs":[],"source":["model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:40.386967Z","iopub.status.busy":"2024-08-04T21:26:40.386695Z","iopub.status.idle":"2024-08-04T21:26:40.421436Z","shell.execute_reply":"2024-08-04T21:26:40.420573Z","shell.execute_reply.started":"2024-08-04T21:26:40.386944Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"roberta-base-lora-ner\",\n","    learning_rate=# YOUR CODE,\n","    per_device_train_batch_size=# YOUR CODE,\n","    per_device_eval_batch_size=# YOUR CODE,\n","    num_train_epochs=# YOUR CODE,\n","    weight_decay=# YOUR CODE\n","    evaluation_strategy=# YOUR CODE\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    report_to=# YOUR CODE\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-04T21:26:40.422893Z","iopub.status.busy":"2024-08-04T21:26:40.422575Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1559' max='2080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1559/2080 14:58 < 05:00, 1.73 it/s, Epoch 7.49/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.227630</td>\n","      <td>0.603687</td>\n","      <td>0.753119</td>\n","      <td>0.670174</td>\n","      <td>0.920834</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.184079</td>\n","      <td>0.697482</td>\n","      <td>0.778284</td>\n","      <td>0.735671</td>\n","      <td>0.935957</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.265900</td>\n","      <td>0.177199</td>\n","      <td>0.726578</td>\n","      <td>0.773945</td>\n","      <td>0.749514</td>\n","      <td>0.940291</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.265900</td>\n","      <td>0.170859</td>\n","      <td>0.741316</td>\n","      <td>0.787070</td>\n","      <td>0.763508</td>\n","      <td>0.940575</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.165100</td>\n","      <td>0.168991</td>\n","      <td>0.753280</td>\n","      <td>0.797158</td>\n","      <td>0.774598</td>\n","      <td>0.943184</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.165100</td>\n","      <td>0.166087</td>\n","      <td>0.741031</td>\n","      <td>0.811042</td>\n","      <td>0.774458</td>\n","      <td>0.944420</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.165100</td>\n","      <td>0.170062</td>\n","      <td>0.753390</td>\n","      <td>0.807571</td>\n","      <td>0.779540</td>\n","      <td>0.944466</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = Trainer(\n","    model=# YOUR CODE,\n","    args=# YOUR CODE,\n","    train_dataset=# YOUR CODE\n","    eval_dataset=# YOUR CODE\n","    tokenizer=# YOUR CODE,\n","    data_collator=# YOUR CODE,\n","    compute_metrics=# YOUR CODE,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(\"/kaggle/input/nlp-week-13-fine-tuning-with-lora/test.csv\")\n","test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Task 7\n","Complete code for model inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","predictions = []\n","for tokens in tqdm(test_df[\"tokens\"].values):\n","    inputs = # YOUR CODE\n","    with torch.no_grad():\n","        logits = # YOUR CODE\n","    preds = # YOUR CODE\n","    prediction = [\n","        pred\n","        for token, pred in zip(inputs.tokens(), preds[0].cpu().numpy())\n","        if \"Ġ\" in token\n","    ]\n","    # YOUR CODE"]},{"cell_type":"markdown","id":"2a9763e2","metadata":{},"source":["# Conclusion\n","\n","In this lesson, we've delved deeply into the world of LLMs. Let's summarize what we have learnt:\n","- LLM and Autoregressive Generation: Large Language Models (LLMs) leverage autoregressive generation to produce coherent text by predicting the next word based on previous context. The Key for tasks requiring context-aware text generation and understanding.\n","- PEFT and Its Algorithms: Additive and Selective methods enhance model performance by efficiently adjusting pre-trained models. Prefix Tuning, Prompt Tuning, Adapter, P-Tuning, and IA3 are techniques under PEFT that enable effective fine-tuning with minimal data and computational resources.\n","- LoRA (Low-Rank Adaptation): Architecture: Utilizes low-rank matrices to adapt models efficiently. Configuration: Adjusts the rank and adaptation parameters to balance performance and resource usage.\n","- QLoRA (Quantized Low-Rank Adaptation): Differences from LoRA: Incorporates quantization to reduce memory and computational needs. Architecture: Builds upon LoRA with added quantization layers to optimize large-scale models.\n","- Code Examples: Llama: Demonstrated end-to-end process including data preparation, model loading, fine-tuning, and prediction. Trained Model Management: Techniques for loading and merging trained models with pre-trained counterparts to leverage existing knowledge effectively.\n","- Practical Task: Applied concepts in real-world scenarios to solidify understanding and implementation skills in NLP tasks.\n","\n","This summary encapsulates the key points of the lesson, reinforcing the concepts and techniques covered to facilitate a deeper understanding of modern NLP methodologies."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8198147,"sourceId":74883,"sourceType":"competition"},{"datasetId":622510,"sourceId":1192499,"sourceType":"datasetVersion"},{"datasetId":3601853,"sourceId":6266221,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":5778.110887,"end_time":"2024-04-11T19:46:36.160009","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-11T18:10:18.049122","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}

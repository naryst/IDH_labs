{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7f59251",
      "metadata": {},
      "source": [
        "# Information Retrieval\n",
        "\n",
        "# Lesson 3: Master index data structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Vector Index in Information Retrieval\n",
        "\n",
        "A vector index is a crucial data structure in the field of Information Retrieval (IR) that enables efficient querying and retrieval of high-dimensional vectors. It is particularly useful for tasks that involve finding the most relevant or similar items in a large dataset, such as document retrieval, recommendation systems, and semantic search.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Embedding**: In IR, embedding refers to the process of converting text or other types of data into fixed-size vector representations. These vectors capture the semantic meaning of the data and are often generated using models like Word2Vec, GloVe, BERT, or FastText.\n",
        "\n",
        "2. **Nearest Neighbor Search**: This is the task of finding the vectors in the dataset that are closest to a given query vector. Nearest neighbor search is fundamental for IR tasks such as finding similar documents, recommending items, or retrieving relevant information based on a query.\n",
        "\n",
        "3. **Indexing Methods**: Various algorithms and data structures can be used to build a vector index, each with its own strengths and weaknesses:\n",
        "    - **KD-Tree**: A binary tree that partitions the space for efficient range and nearest neighbor searches. Suitable for low-dimensional data.\n",
        "    - **Ball Tree**: Uses hyperspheres to partition the space, which can be more efficient for certain types of data compared to KD-Tree.\n",
        "    - **Annoy**: Builds a forest of random projection trees for fast approximate nearest neighbor searches. Useful for large-scale datasets.\n",
        "    - **HNSW (Hierarchical Navigable Small World)**: A graph-based method that provides efficient and scalable nearest neighbor search.\n",
        "    - **FAISS (Facebook AI Similarity Search)**: A library offering various indexing methods, including Inverted File (IVF) and Product Quantization (PQ), for large-scale similarity search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71cd5f3b-91d1-49dd-90d6-3c5f9d4337a8",
      "metadata": {
        "id": "71cd5f3b-91d1-49dd-90d6-3c5f9d4337a8"
      },
      "source": [
        "## Where is a hospital in Manhattan Downtown?\n",
        "\n",
        "In this lab we will create 2 vector indices to answer a very simple question: if you are in Manhattan downtown, where is the nearest hospital? We will base our soultion on two sources of data:\n",
        "- [Points of Interest dataset](https://drive.google.com/file/d/1LUudtCADqSxRl18ZzCzyPPGfhuUo2ZZs/view?usp=sharing). This is a 10% sample of a bigger dataset. Download and uncompress the file.\n",
        "- [Google Geocoding API](https://developers.google.com/maps/documentation/geocoding/start) or any other [equivalent service](https://gisgeography.com/geocoders/). For Google you will need to obtain a key. **PLEASE DO NOT SUBMIT THE KEY :)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8766214",
      "metadata": {},
      "source": [
        "## To complete this lab we will go through several steps:\n",
        "1. Build **coordinate search index**. We will use it to obtain POI from the given region.\n",
        "3. Implement **vector text embedding index** (Annoy, HNSW) to serve semantic queries.\n",
        "3. Implement **geocoding** with cache. We will use it to obtain city coordinates.\n",
        "4. Impement search for **double queries: town and location type**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed4c2e3",
      "metadata": {},
      "source": [
        "##  0. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a5311c-46bd-459e-9aaa-68ac3f1a088e",
      "metadata": {
        "id": "29a5311c-46bd-459e-9aaa-68ac3f1a088e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def draw_earth(xlim=(-180, +180), ylim=(-90, +90)):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.xlim(xlim)\n",
        "    plt.ylim(ylim)\n",
        "\n",
        "    # this file also lives in github. Adjust the path if needed.\n",
        "    df = pd.read_csv(\"./world.csv\")\n",
        "\n",
        "    for row in df['geojson']:\n",
        "        js = json.loads(row)\n",
        "        polys = js['coordinates']\n",
        "        for poly in polys:\n",
        "            for pp in poly:\n",
        "                x, y = [v[0] for v in pp], [v[1] for v in pp]\n",
        "                plt.plot(x, y, color='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a095e09a-bde2-4854-9b4c-774980032d46",
      "metadata": {
        "id": "a095e09a-bde2-4854-9b4c-774980032d46"
      },
      "source": [
        "Reading the dataset and storing coordinates in `GEO` matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a70144-746d-443d-9a94-2ea16f7d2209",
      "metadata": {
        "id": "50a70144-746d-443d-9a94-2ea16f7d2209",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# replace filename if you want to use another data file\n",
        "# be careful! 2M points is still a big number and can eat significant amout of memory\n",
        "with open(\"poi_sample01.pickle\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "# let's sample 20000 of points to draw\n",
        "step = len(dataset) // 20000\n",
        "\n",
        "# pure coordinated in compressed representation, 2B per number -> 8MB per array\n",
        "GEO = np.array([v[0] for v in dataset], dtype=np.float16)\n",
        "N = len(dataset)\n",
        "# free the memory\n",
        "dataset = None\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553b4471-6023-4baa-9922-6427371ce00d",
      "metadata": {
        "id": "553b4471-6023-4baa-9922-6427371ce00d"
      },
      "source": [
        "Showing approximate dataset data distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34accde-8889-4c48-8979-90801bc33b94",
      "metadata": {
        "id": "e34accde-8889-4c48-8979-90801bc33b94",
        "outputId": "e5fff392-6d13-4327-be88-e5ef72216f2f"
      },
      "outputs": [],
      "source": [
        "draw_earth(ylim=(-75, 75))\n",
        "plt.xlabel('longitude')\n",
        "plt.ylabel('latitude')\n",
        "plt.grid()\n",
        "plt.scatter(GEO[::step, 0], GEO[::step, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df552af0-04b6-486a-9e76-b19bfe957d46",
      "metadata": {
        "id": "df552af0-04b6-486a-9e76-b19bfe957d46"
      },
      "source": [
        "## 0.1. Read the data from the hard drive\n",
        "\n",
        "We will prepare id-based shards (data will be distributed into equal files with ranges `[0..capacity-1], [capacity..2*capacity-1], ...`. Each shard will store `capacity` elements. Your task is to complete the implementation with `iterate_dataset` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4186d0a3-2783-4d0c-889a-a9760461bcdf",
      "metadata": {
        "id": "4186d0a3-2783-4d0c-889a-a9760461bcdf"
      },
      "outputs": [],
      "source": [
        "def split_shards(file, folder='shard', capacity=20000):\n",
        "    import pickle, os, math, gc\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "    with open(file, \"rb\") as f:\n",
        "        dataset = pickle.load(f)\n",
        "    nshards = len(dataset) // capacity\n",
        "    if nshards * capacity < len(dataset):\n",
        "        nshards += 1\n",
        "\n",
        "    for i in range(nshards):\n",
        "        with open(f\"{folder}/{i}\", 'wb') as f:\n",
        "            part = dataset[i * capacity:(i+1)*capacity]\n",
        "            pickle.dump(part, f)\n",
        "    dataset = None\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def dataset_get(indices, folder='shard', capacity=20000) -> list:\n",
        "    result = []\n",
        "    groups = {}\n",
        "    for i in indices:\n",
        "        x = i // capacity\n",
        "        if x not in groups:\n",
        "            groups[x] = []\n",
        "        groups[x].append(i)\n",
        "    for x in groups:\n",
        "        with open(f\"{folder}/{x}\", \"rb\") as f:\n",
        "            sha = pickle.load(f)\n",
        "            for i in groups[x]:\n",
        "                row = sha[i % capacity]\n",
        "                result.append(row)\n",
        "    return result\n",
        "\n",
        "\n",
        "# should return iterator, which goes through all elements, consequently opening files\n",
        "# use ``yield`` operator to simplify your code\n",
        "def iterate_dataset(items, folder=\"shard\", capacity=20000):\n",
        "    # TODO write your code instead\n",
        "    yield ([0.0, 0.0], \"stub\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c2b7c1-2a37-48bc-9a59-f8aa2a12fb4c",
      "metadata": {
        "id": "e6c2b7c1-2a37-48bc-9a59-f8aa2a12fb4c"
      },
      "outputs": [],
      "source": [
        "split_shards(\"poi_sample01.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "453c72c8-ec98-4d69-a04c-a921bcdc9f3d",
      "metadata": {
        "id": "453c72c8-ec98-4d69-a04c-a921bcdc9f3d"
      },
      "source": [
        "### 0.1.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb8e8ee-ee4e-4e51-b1e8-0d6dc55c71b0",
      "metadata": {
        "id": "edb8e8ee-ee4e-4e51-b1e8-0d6dc55c71b0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in [137, 40000, 600000]:\n",
        "    assert np.allclose(GEO[i,:], dataset_get([i])[0][0], atol=5*1e-2), \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2397c665-55fd-4fbe-9557-c2fb13339f37",
      "metadata": {
        "id": "2397c665-55fd-4fbe-9557-c2fb13339f37",
        "outputId": "0495b425-9e73-4623-aef5-617d89638667"
      },
      "outputs": [],
      "source": [
        "dataset_get([1, 10, 1000234, N-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac21fa4-dc16-41b9-8c4f-7ea535b0dd75",
      "metadata": {
        "id": "8ac21fa4-dc16-41b9-8c4f-7ea535b0dd75"
      },
      "source": [
        "# 1. Create spacial index for POI (points of interest)\n",
        "\n",
        "We will store dataset rows numbers as values, and coordinates as keys. Please use [KDtree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) or [BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) from sklearn.\n",
        "\n",
        "## 1.1. Build the index and return it\n",
        "\n",
        "Implement the following functions:\n",
        "- `build_geospacial_index` should build and return a search tree object: KDTree or BallTree.\n",
        "- `kNN` accepts a 2D-point, `k` neighbours parameter, and returns **approximate** `k` neighbours (they can be different from the real neighbours).\n",
        "- `inRadius` accepts a 2D-point, L<sub>2</sub> `radius`, and returns points inside the radius. Clarification: for simplicity **radius is given in units of coordinates (degrees)**, not kilometers or meters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7df3d77",
      "metadata": {},
      "source": [
        "\n",
        "### BallTree\n",
        "BallTree is a data structure used for efficient nearest neighbor searches. It partitions the data space using hyperspheres (balls) to create a tree structure. Each node in the tree represents a ball that contains a subset of the data points. This method is particularly effective for high-dimensional data and can handle various distance metrics.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:700/1*bdt854gdCrd_kKQECAELPA.png)\n",
        "\n",
        "### KDTree\n",
        "KDTree (K-Dimensional Tree) is another data structure for efficient nearest neighbor searches. It partitions the data space using hyperplanes, creating a binary tree where each node represents a split along one of the data dimensions. KDTree is well-suited for low to moderate-dimensional data and typically uses Euclidean distance for its operations.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:700/1*6LuJXjGifBSm7lEEO5sftQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e216ac-fd46-475e-8082-158ba32c4661",
      "metadata": {
        "id": "33e216ac-fd46-475e-8082-158ba32c4661",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "def build_geospacial_index(points, leaf_size=5) -> BallTree: # or KDTree here\n",
        "    #TODO write your code here\n",
        "    tree = None\n",
        "    return tree\n",
        "\n",
        "\n",
        "def kNN(query_point: list, k: int, index: BallTree) -> list:\n",
        "    # TODO your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def inRadius(query_point: list, r: float, index: BallTree) -> list:\n",
        "    # TODO your code here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d82f9fe-efdd-48ca-a859-a91339828693",
      "metadata": {
        "id": "7d82f9fe-efdd-48ca-a859-a91339828693"
      },
      "outputs": [],
      "source": [
        "spaidx = build_geospacial_index(GEO)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ba99a38",
      "metadata": {},
      "source": [
        "### 1.1.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffff3d8c-81a5-4e4d-9b26-d76b67a98a24",
      "metadata": {
        "id": "ffff3d8c-81a5-4e4d-9b26-d76b67a98a24",
        "outputId": "cf306764-0201-4130-a6f6-f44de027a8b0"
      },
      "outputs": [],
      "source": [
        "test_id = 13\n",
        "\n",
        "idx = kNN(GEO[test_id], 10, spaidx)\n",
        "print(sorted(idx))\n",
        "assert test_id in idx, \"Point itself should be in results\"\n",
        "\n",
        "idx = inRadius(GEO[test_id], 0.0625, spaidx)\n",
        "print(sorted(idx))\n",
        "assert test_id in idx, \"Point itself should be in results\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ae0ac1-d762-4dcc-bde5-1a67e84f884b",
      "metadata": {
        "id": "46ae0ac1-d762-4dcc-bde5-1a67e84f884b"
      },
      "source": [
        "## 1.2. Tricky assert\n",
        "\n",
        "Some keys (coordinates) in the dataset (surprise!) are duplicates. Unfortunately search trees (in basic implemenation) cannot support duplicates. Thus you can follow one of the strategies:\n",
        "- a key (coordinateS) corresponds to multiple values. This may require additional data strictures.\n",
        "- improve the data (coordinates) to avoid collisions (e.g. make sure they never coinside by adding insignificant noise)\n",
        "\n",
        "You need to modify `kNN` function from previous step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4adffe8",
      "metadata": {},
      "source": [
        "### 1.2.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3933c8b6-0904-4724-a349-9e1375610553",
      "metadata": {
        "id": "3933c8b6-0904-4724-a349-9e1375610553"
      },
      "outputs": [],
      "source": [
        "points = [1966663, 1480877, 2126566]\n",
        "for p in points:\n",
        "    x = GEO[p, :]\n",
        "    r = kNN(x, 1000, spaidx)\n",
        "    assert (p in r), \"Query did not return itself\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85991d1-b730-4405-a33f-8812e562145a",
      "metadata": {
        "id": "c85991d1-b730-4405-a33f-8812e562145a"
      },
      "source": [
        "## 1.3. How leaf size influences build and search speed?\n",
        "\n",
        "Let us look at how parameter of leaf size affects speed of search and construction.\n",
        "We will test on sizes: 1, 10, 20 and 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc09c3b-cb03-44ea-8c15-ec3787fa090d",
      "metadata": {
        "id": "9bc09c3b-cb03-44ea-8c15-ec3787fa090d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "queries = random.sample(range(N), 1000)\n",
        "leaf_sizes = [1, 10, 20, 50]\n",
        "\n",
        "build_times = []\n",
        "query_times = []\n",
        "for ls in tqdm.tqdm(leaf_sizes):\n",
        "    start = time.time()\n",
        "    idx = build_geospacial_index(GEO, ls)\n",
        "    build_times.append(time.time() - start)\n",
        "\n",
        "    start = time.time()\n",
        "    for q in queries:\n",
        "        d, r = spaidx.query([GEO[q]], 10000, sort_results=False, breadth_first=True)\n",
        "    query_times.append(time.time() - start)\n",
        "    idx = None\n",
        "    gc.collect()\n",
        "\n",
        "plt.xlabel(\"build time, s\")\n",
        "plt.ylabel(\"query time, s\")\n",
        "plt.scatter(build_times, query_times)\n",
        "for i, ls in enumerate(leaf_sizes):\n",
        "    plt.annotate(str(ls), (build_times[i], query_times[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545a1830-2bd2-4e29-898e-300f43ca6dfc",
      "metadata": {
        "id": "545a1830-2bd2-4e29-898e-300f43ca6dfc"
      },
      "source": [
        "## 1.4. Range queries?\n",
        "\n",
        "Ok, you have a **radius query**, but what about **rectangual ranges**? Using the functions you already wrote, please, implement the range query given `north-east` and `south-west` corners. Pass the asserts to get points.\n",
        "\n",
        "Hint: The north-east and south-west corners are boundaries of your rectangular range.\n",
        "Iterate through your dataset and check if each point lies within the defined rectangular boundaries. This involves checking if the point's latitude and longitude fall between the corresponding coordinates of the north-east and south-west corners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518149b1-21e3-4f8c-93a9-4d955f7c9751",
      "metadata": {
        "id": "518149b1-21e3-4f8c-93a9-4d955f7c9751"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# should return ids of the rows in this range\n",
        "def get_in_range(ne, sw, spacial_index, GEO) -> list:\n",
        "    # TODO\n",
        "    return [-1, 0, 23]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add4c18d-e4d1-4fa5-a947-a61ff2c007c4",
      "metadata": {
        "id": "add4c18d-e4d1-4fa5-a947-a61ff2c007c4"
      },
      "outputs": [],
      "source": [
        "def print_starbucks(ids):\n",
        "    for row in dataset_get(ids):\n",
        "        if 'Starbucks' in row[1]:\n",
        "            print(row[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d95a52-1667-4381-acce-686f99e11a8c",
      "metadata": {
        "id": "f2d95a52-1667-4381-acce-686f99e11a8c"
      },
      "source": [
        "### 1.4.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "699927cb-ac44-498a-9dec-1d76a34df67e",
      "metadata": {
        "id": "699927cb-ac44-498a-9dec-1d76a34df67e",
        "outputId": "ce3fc6e0-f350-4d0f-ddd9-33fccf5c61d8"
      },
      "outputs": [],
      "source": [
        "ids = get_in_range([-73.97, 40.75], [-74.03, 40.70], spaidx, GEO)\n",
        "\n",
        "assert any(map(\n",
        "            lambda x: 'Manhattan, 80 Delancey St' in x[1],\n",
        "            dataset_get(ids))), \"This Starbucks should be in place!\"\n",
        "\n",
        "print_starbucks(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17837e0b-7018-4e58-9624-19d0097a46c7",
      "metadata": {
        "id": "17837e0b-7018-4e58-9624-19d0097a46c7"
      },
      "source": [
        "# 2. Geocoding\n",
        "\n",
        "In this block we will learn, how to convert text place names into coordinate rectangles.\n",
        "\n",
        "**Geocoding** is the process of converting addresses or place names into geographic coordinates, such as latitude and longitude. This is essential for mapping applications and spatial analysis, as it allows us to place locations on a map and perform geographic queries. Geocoding services, like the Google Geocoding API, provide this functionality by taking a textual location description and returning the corresponding coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4e38bc",
      "metadata": {},
      "source": [
        "## 2.1. Implement geocoding\n",
        "Complete `get_town_range_coordinates` function which returns north-eastern and south-western points of the place. \n",
        "\n",
        "You can use Google Gecoding API or Yandex Geocoder API. Google required credit card validation, so I reccomend to use Yandex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75cee150-55a9-4501-a31c-fd7a4796d7e2",
      "metadata": {
        "id": "75cee150-55a9-4501-a31c-fd7a4796d7e2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# this function returns a pair of tuples: NE and SW corners.\n",
        "def get_town_range_coordinates(town: str, api_key: str) -> tuple:\n",
        "    api = \"https://geocode-maps.yandex.ru/1.x?geocode={}&apikey={}&format=json\"\n",
        "    # TODO your code here.\n",
        "    # implement the function that will return the NE and SW corners of the town\n",
        "    return NE, SW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6da1e0-a57e-47d5-acd7-3d0ed2b5d60f",
      "metadata": {
        "id": "8e6da1e0-a57e-47d5-acd7-3d0ed2b5d60f"
      },
      "source": [
        "If needed, request your key here: https://yandex.ru/dev/maps/geocoder/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d9c42a-6057-4408-a79f-689298dff17d",
      "metadata": {
        "id": "97d9c42a-6057-4408-a79f-689298dff17d"
      },
      "outputs": [],
      "source": [
        "my_yandex_geocoder_api_key = open('yandex.key', 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3848f5bb-9acd-44a2-ba25-dae0d847f1cb",
      "metadata": {
        "id": "3848f5bb-9acd-44a2-ba25-dae0d847f1cb"
      },
      "source": [
        "### 2.1.1 Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee42826b-91e0-470f-8f9e-c8986894c20c",
      "metadata": {
        "id": "ee42826b-91e0-470f-8f9e-c8986894c20c",
        "outputId": "d606752b-834d-4f33-c635-b35fd1e447c6"
      },
      "outputs": [],
      "source": [
        "p = get_town_range_coordinates('Pittsburgh downtown', my_yandex_geocoder_api_key)\n",
        "print(p)\n",
        "assert p[1][0] <= -80. <= p[0][0] and p[1][1] <= 40.44 <= p[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536a8de0-7528-490a-855c-1cd22497accb",
      "metadata": {
        "id": "536a8de0-7528-490a-855c-1cd22497accb"
      },
      "source": [
        "## 2.2. Town queries\n",
        "\n",
        "Now, having a range query and geocoding, we can implement town-queries! Pass the assert to get the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "664a7f60-5bd3-430b-86d1-43be9a12345f",
      "metadata": {
        "id": "664a7f60-5bd3-430b-86d1-43be9a12345f"
      },
      "outputs": [],
      "source": [
        "# should return dataset indices\n",
        "def get_in_town(town, index, GEO, maps_api_key) -> list:\n",
        "    # TODO your code here!\n",
        "    return [0, 1, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e500f522",
      "metadata": {},
      "source": [
        "### 2.2.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416b1f42-f5e7-45a0-a939-c8b7daef7a4d",
      "metadata": {
        "id": "416b1f42-f5e7-45a0-a939-c8b7daef7a4d",
        "outputId": "22bc4ce2-64ad-4147-e054-58fe2dddf761"
      },
      "outputs": [],
      "source": [
        "ids = get_in_town('Pittsburgh downtown', spaidx, GEO, my_yandex_geocoder_api_key)\n",
        "\n",
        "assert any(map(\n",
        "            lambda x: 'US, Pittsburgh, 810 River Ave' in x[1],\n",
        "            dataset_get(ids))), \"This Starbucks should be in place!\"\n",
        "\n",
        "print_starbucks(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "212c1670-3314-4b81-95e8-278a5b8822db",
      "metadata": {
        "id": "212c1670-3314-4b81-95e8-278a5b8822db"
      },
      "source": [
        "## 2.3. Caching\n",
        "\n",
        "Why should you pay for every geocaching request, if you can cache them? Implement a cached version on geocoding. The second query does not use internet."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41c2685",
      "metadata": {},
      "source": [
        "**Cache** is a high-speed data storage layer that stores a subset of data, typically transient in nature, so that future requests for that data are served up faster than accessing the primary storage location. Caching allows for the reuse of previously retrieved or computed data, reducing the time and resources needed to access frequently used information. It is commonly used in computing to improve the performance and efficiency of systems by minimizing the need to repeatedly fetch or compute the same data.\n",
        "\n",
        "In this case Cache could be a dict where key - town name and value - coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3848261-07c1-47ef-bcea-1b0a72c70038",
      "metadata": {
        "id": "a3848261-07c1-47ef-bcea-1b0a72c70038"
      },
      "outputs": [],
      "source": [
        "global GEO_CACHE\n",
        "\n",
        "def get_town_range_coordinates_cached(town: str, maps_key: str) -> tuple:\n",
        "    global GEO_CACHE\n",
        "    # TODO your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_in_town_cached(town: str, index, GEO, maps_key: str) -> list:\n",
        "    # TODO your code here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcf7790-ba8f-472a-9c67-133ffa8bc10d",
      "metadata": {
        "id": "cdcf7790-ba8f-472a-9c67-133ffa8bc10d",
        "outputId": "79b23d54-1fad-41e0-a689-30dba024dec0"
      },
      "outputs": [],
      "source": [
        "ids = get_in_town_cached('Boulder, CO', spaidx, GEO, my_google_maps_api_key)\n",
        "print_starbucks(ids)\n",
        "ids = get_in_town_cached('Boulder, CO', spaidx, GEO, my_google_maps_api_key)\n",
        "print_starbucks(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f7cb00-c98a-493b-a3bd-f290fcfc83a4",
      "metadata": {
        "id": "59f7cb00-c98a-493b-a3bd-f290fcfc83a4"
      },
      "source": [
        "# 3. Text search\n",
        "\n",
        "We are done with geography, but we have no clear method to search for categories. What if we prepare vector index of location names?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e5e36e-3cc0-4b0e-9cc5-be1ffec44768",
      "metadata": {
        "id": "27e5e36e-3cc0-4b0e-9cc5-be1ffec44768"
      },
      "outputs": [],
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea12030-de9c-4200-a411-0138e43c05db",
      "metadata": {
        "id": "8ea12030-de9c-4200-a411-0138e43c05db"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "names = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07dca13a-5641-4330-b1e2-895f6a81e220",
      "metadata": {
        "id": "07dca13a-5641-4330-b1e2-895f6a81e220"
      },
      "source": [
        "## 3.1. Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beeea15a",
      "metadata": {},
      "source": [
        "\n",
        "**Embeddings** are dense vector representations of text or other data types that capture semantic meaning and relationships between items. They are commonly used in NLP and ML to transform high-dimensional data into a lower-dimensional space for efficient processing and analysis. In our case, embeddings help us represent addresses and place names as vectors for efficient similarity search and retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4ee5af",
      "metadata": {},
      "source": [
        "\n",
        "Here is the trick. If you use any embedding model \"as it is\", it may take some hours to prepare 2M embeddings. It's ok if you can wait, but...\n",
        "\n",
        "Please think, how you can speed up the process with embedding to less than 5 minutes?\n",
        "\n",
        "HINT: spacy model `nlp` has a [dictionary for word embeddings](https://spacy.io/api/vocab). You can access `nlp.vocab[word].vector` to get word embedding, `nlp.vocab.strings` map stores integer indices. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43836204-638f-4f77-b4b2-7348fb0241dd",
      "metadata": {
        "id": "43836204-638f-4f77-b4b2-7348fb0241dd",
        "outputId": "d9ab5330-6afe-4c9c-9098-e866346355ce"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "embeddings = np.zeros((N, 300), dtype=np.float16)\n",
        "\n",
        "for i, item in enumerate(tqdm(iterate_dataset(N), total=N)):\n",
        "    name = item[1].split('.')[0]\n",
        "    emb = embed(name, nlp)\n",
        "    if emb is not None:\n",
        "        embeddings[i, :] = emb\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73177781-3e59-494b-afc6-215f1a926f24",
      "metadata": {
        "id": "73177781-3e59-494b-afc6-215f1a926f24"
      },
      "source": [
        "## 3.2. Vector index\n",
        "\n",
        "Here you build vector index for our embeddings. I want to warn Windows users, that they can observe problems with installing Faiss and HNSWlib (please refer to the corresponding lab). Still this is not the reason not to try :)\n",
        "Choose **one of the libraries** and fulfill the requirements to get full points:\n",
        "1. If you choose [FAISS](https://faiss.ai/). Get started with [installation](https://faiss.ai/#install) and this [tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started). To get full points your index must use [Product Quantization](https://github.com/facebookresearch/faiss/wiki/Lower-memory-footprint): 50 subvectors, 8 bits (1 byte) each. Use custom `nprobe` parameter equal to 23. Number or Voronoi cells is `65536`. Refer to [this document](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index#if-1m---10m-ivf65536_hnsw32) to understand recommendations.\n",
        "2. If you use [HNSWlib](https://github.com/nmslib/hnswlib) (or [nmslib](https://github.com/nmslib/nmslib)) then follow these requirements. Use `cosine` metric for index construction, maximum number of outgoing connections (max outdegree) in the graph is 16, `ef` parameter at construction time should be `250`. Some useful information is given [here](https://github.com/nmslib/nmslib/blob/master/manual/methods.md).\n",
        "3. For [Annoy](https://github.com/spotify/annoy) you should use cosine distance for the space (if vectors are normed, you can use dot product intead), use all CPU cores at construction time. Build the index right on the disk, then load. Your index should consist of 37 trees.\n",
        "\n",
        "**NB** If you run on not-very-modern hardware (e.g. your RAM is less then 8GB), then you'd better reduce dataset size (e.g. take a specific region only like US east cost). You can also reduce other parameters only for the sake of RAM efficiency, but please specify and justify your decisions.\n",
        "\n",
        "e.g.\n",
        "```    \n",
        "roi = set(get_in_range([-68.645945, 43.163175], [-80.461502, 37.097044], spaidx, GEO))\n",
        "```\n",
        "\n",
        "\n",
        "**HINT** You can remove `embeddings` array and call `gc.collect()` before loading index to RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07f72e7-533b-43b7-beb1-dd50f4580ee5",
      "metadata": {
        "id": "d07f72e7-533b-43b7-beb1-dd50f4580ee5"
      },
      "outputs": [],
      "source": [
        "embedding_index = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1a228f-4b29-4bee-af78-79fa0df6eee9",
      "metadata": {
        "id": "7b1a228f-4b29-4bee-af78-79fa0df6eee9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_vector_index():\n",
        "     # TODO returns index object\n",
        "    # Write your code here\n",
        "    pass\n",
        "    \n",
        "\n",
        "\n",
        "def get_kNN_embeddings(embedding, k, index):\n",
        "    # TODO write your kNN queries here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4c7c11-1fee-42b2-b8f8-c600928bd6b6",
      "metadata": {
        "id": "3a4c7c11-1fee-42b2-b8f8-c600928bd6b6",
        "outputId": "01abdc8e-f7db-4800-d42f-97a84278e235"
      },
      "outputs": [],
      "source": [
        "embedding_index = get_vector_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93b8cd7",
      "metadata": {},
      "source": [
        "### 3.2.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5600f9a9-2713-4e43-832c-55dc4b4219be",
      "metadata": {
        "id": "5600f9a9-2713-4e43-832c-55dc4b4219be",
        "tags": []
      },
      "outputs": [],
      "source": [
        "result = get_kNN_embeddings(embed('pharmacy', nlp), 1000, embedding_index)\n",
        "assert len(result) == 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5e690d-a6f8-4c9a-8a89-eb02aabbc690",
      "metadata": {
        "id": "4a5e690d-a6f8-4c9a-8a89-eb02aabbc690"
      },
      "source": [
        "# 4. And now we want to have this together!\n",
        "\n",
        "Implement `find` function, which takes a town and a location which you want to find there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7dd5aab-f429-4d51-8691-2fd497c23d60",
      "metadata": {
        "id": "e7dd5aab-f429-4d51-8691-2fd497c23d60"
      },
      "outputs": [],
      "source": [
        "def find(town, query) -> list:\n",
        "    # TODO should return list of data items, with coordinates and texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce6d2a8-afb3-4ebd-8669-5d49a977808b",
      "metadata": {
        "id": "0ce6d2a8-afb3-4ebd-8669-5d49a977808b",
        "outputId": "b6b5eafc-cd6c-4355-8b35-b5278c0aa283"
      },
      "outputs": [],
      "source": [
        "items = find('Manhattan downtown', 'hospital')\n",
        "print(items[:20])\n",
        "xy = np.array([row[0] for row in items])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505b66ed-e15a-4003-809f-67f6bc585b02",
      "metadata": {
        "id": "505b66ed-e15a-4003-809f-67f6bc585b02",
        "outputId": "685bca05-ccc0-40db-e939-1419ad174e49"
      },
      "outputs": [],
      "source": [
        "NE, SW = get_town_range_coordinates_cached('Manhattan downtown', my_yandex_geocoder_api_key)\n",
        "draw_earth(xlim=(SW[0] - 5, NE[0] + 5), ylim=(SW[1] - 5, NE[1] + 5))\n",
        "plt.scatter(xy[:, 0], xy[:, 1])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information Retrieval\n",
        "## Lesson 1: Comprehensive Guide to Web Crawling and Document Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "gTqJyzdyCoSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Retrieval (IR) is the process of obtaining relevant information from a large repository, such as a database or the internet, in response to a specific query. The primary goal of IR is to provide users with accurate and useful results based on their search criteria. This field encompasses a range of activities, from simple keyword searches to complex algorithms that understand and interpret user intent, context, and semantics.\n",
        "\n",
        "### Key Components of Information Retrieval:\n",
        "\n",
        "1. **Indexing**: The process of organizing data to enable efficient retrieval. This often involves creating an index that maps keywords to their locations in documents.\n",
        "\n",
        "2. **Query Processing**: Interpreting and refining user queries to match relevant documents. This may include techniques such as stemming, stop-word removal, and query expansion.\n",
        "\n",
        "3. **Ranking**: Ordering the search results based on their relevance to the query. Common ranking algorithms include TF-IDF, PageRank, and various machine learning models.\n",
        "\n",
        "4. **Evaluation**: Measuring the effectiveness of the retrieval system using metrics such as precision, recall, and F1-score.\n",
        "\n",
        "Information Retrieval is a critical component of many applications, including search engines, digital libraries, and recommendation systems, helping users find the information they need quickly and accurately.\n",
        "\n",
        "In this lesson, we will mainly deal with Web Crawling and Document Processing"
      ],
      "metadata": {
        "id": "8hqTlhKCELk2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ifkuaEeSSPm"
      },
      "source": [
        "# 1. Crawler\n",
        "\n",
        "A crawler is a computer program that automatically searches documents on the Web. Crawlers are primarily programmed for repetitive actions so that browsing is automated. Search engines use crawlers most frequently to browse the internet and build an index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNy6gYnUSSPn"
      },
      "source": [
        "### 1.0.1. How to parse a page?\n",
        "Parsing a webpage involves retrieving and analyzing its HTML content to extract specific data or information. This process typically includes:\n",
        "\n",
        "1. **Fetching the Page**: Using a tool or library to download the HTML content of a webpage.\n",
        "2. **Processing the HTML**: Utilizing libraries like BeautifulSoup (Python), or similar to navigate and manipulate the HTML structure.\n",
        "3. **Extracting Data**: Identifying and isolating the required elements, such as text, images, links, or other data, based on tags, classes, or attributes.\n",
        "\n",
        "\n",
        "If you build a crawler, you might follow one of the approaches:\n",
        "1. search for URLs in the page, assuming this is just a text.\n",
        "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
        "\n",
        "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
        "\n",
        "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uF2ZauLSSPn"
      },
      "source": [
        "## 1.1. Download and persist #\n",
        "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
        "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
        "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
        "- `load()` method loads data from hard drive. Returns `True` for success.\n",
        "\n",
        "Tests checks that your code somehow works.\n",
        "\n",
        "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
        "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
        "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLgs-CLSSSPn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "from urllib.parse import quote\n",
        "\n",
        "class Document:\n",
        "\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def get(self):\n",
        "        if not self.load():\n",
        "            if not self.download():\n",
        "                raise FileNotFoundError(self.url)\n",
        "            else:\n",
        "                self.persist()\n",
        "\n",
        "    def download(self):\n",
        "        #TODO download self.url content, store it in self.content and return True in case of success\n",
        "        return False\n",
        "\n",
        "    def persist(self):\n",
        "        #TODO write document content to hard drive\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        #TODO load content from hard drive, store it in self.content and return True in case of success\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmTJ7zg2SSPo"
      },
      "source": [
        "### 1.1.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npRbhOcOSSPo"
      },
      "outputs": [],
      "source": [
        "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
        "\n",
        "doc.get()\n",
        "assert doc.content, \"Document download failed\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
        "\n",
        "doc.get()\n",
        "assert doc.load(), \"Load should return true for saved document\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujkl2ZIpSSPo"
      },
      "source": [
        "## 1.2. Parse HTML\n",
        "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
        "1. `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
        "2. `self.images` list of images met in a document. Again, links can be relative to current page.\n",
        "3. `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
        "\n",
        "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCtwaEidSSPo"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import urllib.parse\n",
        "\n",
        "\n",
        "class HtmlDocument(Document):\n",
        "\n",
        "    def parse(self):\n",
        "        #TODO extract plain text, images and links from the document\n",
        "        self.anchors = [(\"fake link text\", \"http://fake.url/\")]\n",
        "        self.images = [\"http://image.com/fake.jpg\"]\n",
        "        self.text = \"fake text and some other text\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuGerrC9SSPo"
      },
      "source": [
        "### 1.2.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1LmX8FwSSPo"
      },
      "outputs": [],
      "source": [
        "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
        "doc.get()\n",
        "doc.parse()\n",
        "\n",
        "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
        "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
        "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfABrWXmSSPo"
      },
      "source": [
        "## 1.3. Document analysis ##\n",
        "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose).\n",
        "\n",
        "**Criteria to succeed in the task**:\n",
        "1. Your `get_word_stats()` method should return `Counter` object.\n",
        "2. Don't forget to lowercase your words for counting.\n",
        "3. Sentences should be obtained from inside `<body>` tag only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktXZjyyCSSPo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class HtmlDocumentTextData:\n",
        "\n",
        "    def __init__(self, url):\n",
        "        self.doc = HtmlDocument(url)\n",
        "        self.doc.get()\n",
        "        self.doc.parse()\n",
        "\n",
        "    def get_sentences(self):\n",
        "        #TODO implement sentence parser\n",
        "        result = []\n",
        "        return result\n",
        "\n",
        "    def get_word_stats(self):\n",
        "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
        "        return Counter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c74xbq9SSPo"
      },
      "source": [
        "### 1.3.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmxTqZeOSSPo"
      },
      "outputs": [],
      "source": [
        "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
        "\n",
        "print(doc.get_word_stats().most_common(10))\n",
        "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4mw0OI5SSPo"
      },
      "source": [
        "## 1.4. Crawling ##\n",
        "\n",
        "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U6VHEGwSSPo"
      },
      "outputs": [],
      "source": [
        "from queue import Queue\n",
        "\n",
        "class Crawler:\n",
        "\n",
        "    def crawl_generator(self, source, depth=1):\n",
        "        #TODO return real crawling results. Don't forget to process failures,\n",
        "        # exceptions, 3**, 4** codes\n",
        "        for i in range(3):\n",
        "            yield HtmlDocumentTextData(source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5bFjpAtSSPo"
      },
      "source": [
        "### 1.4.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrN9Xb9pSSPo"
      },
      "outputs": [],
      "source": [
        "crawler = Crawler()\n",
        "counter = Counter()\n",
        "\n",
        "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
        "    print(c.doc.url)\n",
        "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
        "        print(\"Skipping\", c.doc.url)\n",
        "        continue\n",
        "    counter.update(c.get_word_stats())\n",
        "    print(len(counter), \"distinct word(s) so far\")\n",
        "\n",
        "print(\"Done\")\n",
        "\n",
        "print(counter.most_common(20))\n",
        "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er2B8ewISSPp"
      },
      "source": [
        "\n",
        "## 1.5. Account the caching policy\n",
        "\n",
        "Sometimes remote documents (especially when we speak about static content like `js` or `gif`) can swear that they will not change for some time. This is done by setting [Cache-Control response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfGt0mSHSSPp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from urllib.parse import quote\n",
        "\n",
        "class Document:\n",
        "\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.content = None\n",
        "        self.cache_file = quote(url, safe='')\n",
        "\n",
        "    def get(self):\n",
        "        if not self.load():\n",
        "            if not self.download():\n",
        "                raise FileNotFoundError(self.url)\n",
        "            else:\n",
        "                self.persist()\n",
        "\n",
        "    def download(self):\n",
        "        try:\n",
        "            response = requests.get(self.url, allow_redirects=True)\n",
        "            if response.status_code == 200:\n",
        "                self.content = response.content\n",
        "                self.headers = response.headers\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Failed to download {self.url}: {response.status_code}\")\n",
        "                return False\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def persist(self):\n",
        "        try:\n",
        "            with open(self.cache_file, 'wb') as f:\n",
        "                f.write(self.content)\n",
        "            with open(self.cache_file + '.meta', 'w') as f:\n",
        "                f.write(self.headers.get('Cache-Control', ''))\n",
        "            print(f\"File saved as {self.cache_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to persist {self.url}: {e}\")\n",
        "\n",
        "    def load(self):\n",
        "        if not os.path.exists(self.cache_file):\n",
        "            return False\n",
        "        try:\n",
        "            with open(self.cache_file, 'rb') as f:\n",
        "                self.content = f.read()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {self.url}: {e}\")\n",
        "            return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDc9u7YISSPp"
      },
      "source": [
        "We have implemented a descendant to a `Document` class, which will refresh the document in case of expired cache even if the file is already on the hard drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C3xt39QSSPp"
      },
      "outputs": [],
      "source": [
        "class CachedDocument(Document):\n",
        "    def is_cache_expired(self):\n",
        "        if not os.path.exists(self.cache_file + '.meta'):\n",
        "            return True\n",
        "\n",
        "        with open(self.cache_file + '.meta', 'r') as f:\n",
        "            cache_control = f.read()\n",
        "\n",
        "        if 'max-age' in cache_control:\n",
        "            max_age = int(cache_control.split('max-age=')[1].split(',')[0])\n",
        "            file_age = time.time() - os.path.getmtime(self.cache_file)\n",
        "            return file_age > max_age\n",
        "        return True\n",
        "\n",
        "    def get(self):\n",
        "        if not os.path.exists(self.cache_file) or self.is_cache_expired():\n",
        "            print(f\"Cache expired or not found for {self.url}\")\n",
        "            if not self.download():\n",
        "                raise FileNotFoundError(self.url)\n",
        "            else:\n",
        "                self.persist()\n",
        "        else:\n",
        "            print(f\"Loading from cache for {self.url}\")\n",
        "            self.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GotKBFnvSSPp"
      },
      "source": [
        "### Run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnniF62iSSPp"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "doc = CachedDocument('https://yandex.ru/')\n",
        "doc.get()\n",
        "time.sleep(2)\n",
        "doc.get()\n",
        "time.sleep(2)\n",
        "doc.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS9qa-d1SSPq"
      },
      "source": [
        "## 1.6. Languages\n",
        "Maybe you heard, that there are multiple languages in the world. European languages, like Russian and English, use similar puctuation, but even in this family there is ¡Spanish!\n",
        "\n",
        "Other languages can use different punctiation rules, like **Arabic or [Thai](http://www.thai-language.com/ref/breaking-words)**.\n",
        "\n",
        "Your task is to support (at least) three languages (English, Arabic, and Thai) tokenization in your `HtmlDocumentTextData` class descendant.\n",
        "\n",
        "What should you do (acceptance criteria):\n",
        "1. Use any language dection techniques, e.g. [langdetect](https://pypi.org/project/langdetect/).\n",
        "2. Use language-specific tokenization tools, e.g. for [Thai](https://pythainlp.github.io/tutorials/notebooks/pythainlp_get_started.html#Tokenization-and-Segmentation) and [Arabic](https://github.com/CAMeL-Lab/camel_tools).\n",
        "3. Use these pages to test your code: [1](https://www.bangkokair.com/tha/baggage-allowance) and [2](https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82%D8%A8%D8%A9-%D8%A8%D9%88%D8%AA%D9%8A%D9%86).\n",
        "4. Pass the tests."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "9Y4VTG85S259"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "id": "VSWGaUGpTB5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4.element import Comment\n",
        "import urllib.parse\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from langdetect import detect\n",
        "import pythainlp\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "4TfLAkUaXA1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5pZpM-zSSPq"
      },
      "outputs": [],
      "source": [
        "class HtmlDocument(Document):\n",
        "\n",
        "    def parse(self):\n",
        "        soup = BeautifulSoup(self.content, 'html.parser')\n",
        "\n",
        "        self.anchors = [(a.get_text(), urllib.parse.urljoin(self.url, a.get('href', '')))\n",
        "                        for a in soup.find_all('a', href=True)]\n",
        "        self.images = [urllib.parse.urljoin(self.url, img.get('src', ''))\n",
        "                       for img in soup.find_all('img', src=True)]\n",
        "        self.text = ' '.join(soup.stripped_strings)\n",
        "\n",
        "class HtmlDocumentTextData:\n",
        "    def __init__(self, url):\n",
        "        self.doc = HtmlDocument(url)\n",
        "        self.doc.get()\n",
        "        self.doc.parse()\n",
        "\n",
        "    def get_sentences(self):\n",
        "        return sent_tokenize(self.doc.text)\n",
        "\n",
        "    def get_word_stats(self):\n",
        "        words = word_tokenize(self.doc.text)\n",
        "        return Counter(word.lower() for word in words if word.isalpha())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E-Ad2ywSSPq"
      },
      "outputs": [],
      "source": [
        "class MultilingualHtmlDocumentTextData(HtmlDocumentTextData):\n",
        "    def __init__(self, url):\n",
        "        super().__init__(url)\n",
        "        self.language = self.detect_language()\n",
        "\n",
        "    def detect_language(self):\n",
        "        try:\n",
        "            return detect(self.doc.text)\n",
        "        except:\n",
        "            return 'en'  # Default to English if detection fails\n",
        "\n",
        "    def tokenize_text(self):\n",
        "        if self.language == 'ar':\n",
        "            tokens = ...  # Use arabic-specific tokenization here\n",
        "        elif self.language == 'th':\n",
        "            tokens = ...  # Use Thai-specific tokenization here\n",
        "        else:  # Assuming English or other languages\n",
        "            tokens = word_tokenize(self.doc.text)\n",
        "        return tokens\n",
        "\n",
        "    def get_sentences(self):\n",
        "        if self.language == 'ar':\n",
        "            # Use Arabic-specific sentence tokenizer here if available\n",
        "            pass\n",
        "        elif self.language == 'th':\n",
        "            # Use Thai-specific sentence tokenizer here if available\n",
        "            pass\n",
        "        return super().get_sentences()\n",
        "\n",
        "    def get_word_stats(self):\n",
        "        tokens = self.tokenize_text()\n",
        "        return Counter(token.lower() for token in tokens if token.isalpha())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQOmLxxtSSPq"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeafe5Z0SSPq"
      },
      "outputs": [],
      "source": [
        "doc = MultilingualHtmlDocumentTextData(\"https://www.bangkokair.com/tha/baggage-allowance\")\n",
        "print(doc.get_word_stats().most_common(10))\n",
        "\n",
        "doc = MultilingualHtmlDocumentTextData(\"https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82\")\n",
        "print(doc.get_word_stats().most_common(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
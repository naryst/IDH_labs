{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Practical Machine Learning and Deep Learning\n",
        "# Lesson 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_M9lxLmwt6_"
      },
      "source": [
        "# Fine-tuning a model on a translation task\n",
        "Today we will be finetunning T5(Text-To-Text Transfer Transformer) [model](https://github.com/google-research/t5x) on translation task. For this purpose we will be using [HuggingFace transformers](https://huggingface.co/docs/transformers/index) and [WMT16](https://huggingface.co/datasets/wmt16) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# installing huggingface libraries for dataset, models and metrics\n",
        "!pip install datasets transformers[sentencepiece] sacrebleu\n",
        "\n",
        "!pip install numpy==1.24.3\n",
        "\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:01.766013Z",
          "iopub.status.busy": "2023-09-24T15:10:01.765366Z",
          "iopub.status.idle": "2023-09-24T15:10:01.772400Z",
          "shell.execute_reply": "2023-09-24T15:10:01.771384Z",
          "shell.execute_reply.started": "2023-09-24T15:10:01.765977Z"
        },
        "trusted": true,
        "id": "pK9LcP0kwt7I"
      },
      "outputs": [],
      "source": [
        "# Necessary inputs\n",
        "import warnings\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "import transformers\n",
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEfX36R6wt7J"
      },
      "source": [
        "## Selecting the model\n",
        "For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:01.774778Z",
          "iopub.status.busy": "2023-09-24T15:10:01.773897Z",
          "iopub.status.idle": "2023-09-24T15:10:01.784448Z",
          "shell.execute_reply": "2023-09-24T15:10:01.783216Z",
          "shell.execute_reply.started": "2023-09-24T15:10:01.774744Z"
        },
        "trusted": true,
        "id": "5rqnKW-hwt7L"
      },
      "outputs": [],
      "source": [
        "# selecting model checkpoint\n",
        "model_checkpoint = \"t5-small\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:01.787917Z",
          "iopub.status.busy": "2023-09-24T15:10:01.787594Z",
          "iopub.status.idle": "2023-09-24T15:10:03.219266Z",
          "shell.execute_reply": "2023-09-24T15:10:03.218277Z",
          "shell.execute_reply.started": "2023-09-24T15:10:01.787893Z"
        },
        "id": "IreSlFmlIrIm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# setting random seed for transformers library\n",
        "transformers.set_seed(42)\n",
        "\n",
        "# Load the WMT16 dataset\n",
        "raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n",
        "\n",
        "# Load the BLUE metric\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTClld_Swt7N"
      },
      "source": [
        "## Dataset\n",
        "Downloaded from HuggingFace dataset is a `DatasetDict`.\n",
        "\n",
        "In the Hugging Face datasets library, DatasetDict is a class used to manage multiple datasets, usually representing different splits of a dataset, such as `train`, `test`, and `validation`. It allows users to work with these splits in a structured and intuitive way.\n",
        "\n",
        "### Why use `DatasetDict`?\n",
        "\n",
        "1.   **Dictionary-Like Structure:** DatasetDict is essentially a dictionary where keys are split names (e.g., `train`, `test`, `validation`) and values are Dataset objects.\n",
        "2.   **Convenient Access:** Provides easy access to different splits of a dataset using dictionary-style lookups.\n",
        "3.   **Integrated Operations:** Facilitates applying transformations or preprocessing steps across all dataset splits simultaneously.\n",
        "4.   **Seamless Integration:** Works well with other components of the Hugging Face ecosystem, including tokenizers and models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.220970Z",
          "iopub.status.busy": "2023-09-24T15:10:03.220604Z",
          "iopub.status.idle": "2023-09-24T15:10:03.229080Z",
          "shell.execute_reply": "2023-09-24T15:10:03.228070Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.220925Z"
        },
        "id": "GWiVUF0jIrIv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:35:02.763581Z",
          "iopub.status.busy": "2023-09-24T15:35:02.762411Z",
          "iopub.status.idle": "2023-09-24T15:35:02.772979Z",
          "shell.execute_reply": "2023-09-24T15:35:02.771223Z",
          "shell.execute_reply.started": "2023-09-24T15:35:02.763514Z"
        },
        "id": "X6HrpprwIrIz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# samples from train dataset\n",
        "raw_datasets[\"train\"][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XNL0DKAwt7R"
      },
      "source": [
        "## Metric\n",
        "[Sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu) computes:\n",
        "- `score`: BLEU score\n",
        "- `counts`: list of counts of correct n-grams\n",
        "- `totals`: list of counts of total n-grams\n",
        "- `precisions`: list of precisions\n",
        "- `bp`: Brevity penalty\n",
        "- `sys_len`: cumulative sysem length\n",
        "- `ref_len`: cumulative reference length\n",
        "\n",
        "The main metric is [BLEU score](https://en.wikipedia.org/wiki/BLEU). BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score measures the similarity of the machine-translated text to a set of high quality reference translations. It was originally designed for machine translation tasks but has since been adapted and widely used in other natural language generation tasks.\n",
        "\n",
        "\n",
        "Hereâ€™s a step-by-step overview of how BLEU score is calculated:\n",
        "\n",
        "1. **N-gram Precision**:\n",
        "   - BLEU score calculates precision for [n-grams](https://en.wikipedia.org/wiki/N-gram) (contiguous sequences of n items) of varying lengths (typically 1 to 4). The precision for each n-gram length is computed as follows:\n",
        "     - **Modified Precision**: For each n-gram length n, count the maximum number of times any single n-gram appears in any single reference translation, denoted as\n",
        "$$ \\text{max}_n $$\n",
        "\n",
        "     - **Candidate n-gram Count**: Count the total number of n-grams in the candidate (machine-generated) translation, denoted as $\\( \\text{count}_n \\)$.\n",
        "     - **Clip Count**: Clip $\\( \\text{count}_n \\)$ by $\\( \\text{max}_n \\)$, denoted as $\\( \\text{clip}_n \\)$. This prevents artificially inflating the score by penalizing over-generation of n-grams not found in the references.\n",
        "     - **Precision Calculation**: Calculate the precision for each n-gram length as $\\( \\frac{\\sum \\text{clip}_n}{\\sum \\text{count}_n} \\)$.\n",
        "\n",
        "2. **Brevity Penalty**:\n",
        "   - BLEU includes a penalty term to account for shorter candidate translations compared to the reference translations. This is to discourage generating overly short translations that may inflate precision scores unfairly.\n",
        "   - **Brevity Penalty Calculation**: Compute the brevity penalty $\\text{BP}$ as $ \\exp(1 - \\frac{\\text{reference length}}{\\text{candidate length}}) $, where $ \\text{reference length} $ is the total length of all reference translations and $ \\text{candidate length} $ is the length of the candidate translation.\n",
        "\n",
        "3. **BLEU Score Calculation**:\n",
        "   - Combine the n-gram precisions using a weighted geometric mean, adjusted by the brevity penalty:\n",
        "     - $\\( \\text{BLEU} = \\text{BP} \\times \\exp \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\log p_i \\right) \\)$, where $\\( p_i \\)$ is the precision for n-gram length $\\( i \\)$.\n",
        "\n",
        "4. **Interpretation**:\n",
        "   - BLEU score ranges typically from 0 to 1, with 1 indicating a perfect match between candidate and reference translations. Higher BLEU scores indicate better translation quality, but the interpretation can vary depending on the task and the specific context.\n",
        "\n",
        "5. **Implementation**:\n",
        "   - BLEU score computation is often implemented in libraries such as NLTK or directly in evaluation scripts provided by machine translation frameworks. These implementations handle the counting of n-grams, calculation of precisions, and application of the brevity penalty.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:38:54.393583Z",
          "iopub.status.busy": "2023-09-24T15:38:54.393131Z",
          "iopub.status.idle": "2023-09-24T15:38:54.403154Z",
          "shell.execute_reply": "2023-09-24T15:38:54.401952Z",
          "shell.execute_reply.started": "2023-09-24T15:38:54.393549Z"
        },
        "id": "5o4rUteaIrI_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# how to use sacrebleu and its purpose\n",
        "metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:05:06.585010Z",
          "iopub.status.busy": "2023-09-24T16:05:06.584619Z",
          "iopub.status.idle": "2023-09-24T16:05:06.601506Z",
          "shell.execute_reply": "2023-09-24T16:05:06.600239Z",
          "shell.execute_reply.started": "2023-09-24T16:05:06.584982Z"
        },
        "id": "6XN1Rq0aIrJC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fake_preds = [\"hello there\", \"general kenobi\", \"Can I get an A\"]\n",
        "fake_labels = [[\"hello there\"], [\"general kenobi\"], ['Can I get a C']]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data\n",
        "As usual we will need to preprocess data and tokenize it before passing to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.310034Z",
          "iopub.status.busy": "2023-09-24T15:10:03.309664Z",
          "iopub.status.idle": "2023-09-24T15:10:03.505289Z",
          "shell.execute_reply": "2023-09-24T15:10:03.504208Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.310001Z"
        },
        "id": "eXNLu_-nIrJI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# we will use autotokenizer for this purpose\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.507243Z",
          "iopub.status.busy": "2023-09-24T15:10:03.506858Z",
          "iopub.status.idle": "2023-09-24T15:10:03.514992Z",
          "shell.execute_reply": "2023-09-24T15:10:03.513936Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.507207Z"
        },
        "id": "a5hBlsrHIrJL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer(\"Hello, this one sentence!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.517405Z",
          "iopub.status.busy": "2023-09-24T15:10:03.516686Z",
          "iopub.status.idle": "2023-09-24T15:10:03.529526Z",
          "shell.execute_reply": "2023-09-24T15:10:03.528540Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.517370Z"
        },
        "trusted": true,
        "id": "GVn7DLURwt7T"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.533132Z",
          "iopub.status.busy": "2023-09-24T15:10:03.532860Z",
          "iopub.status.idle": "2023-09-24T15:10:03.539292Z",
          "shell.execute_reply": "2023-09-24T15:10:03.538145Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.533109Z"
        },
        "trusted": true,
        "id": "g2smXDdZwt7T"
      },
      "outputs": [],
      "source": [
        "# prefix for model input\n",
        "prefix = \"translate English to Deutsch:\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Constants and Language Specifications\n",
        "\n",
        "The constants `max_input_length` and `max_target_length` will define the maximum lengths for the input and target sequences, respectively. The `source_lang` and `target_lang` variables will specify the source language (English) and the target language (German).\n",
        "\n",
        "## Preprocess Function\n",
        "Let's define preprocessing function that will prepare text data for a sequence-to-sequence model. This function will handle the conversion of raw text into tokenized sequences suitable for input into a model.\n",
        "\n",
        "\n",
        "1. Input and Target Sentence Preparation\n",
        "\n",
        "    Inside the function, `inputs` will be generated by concatenating a prefix with the source language sentences extracted from `examples[\"translation\"]`. Similarly, `targets` will be a list of target language sentences from the same dictionary.\n",
        "\n",
        "2. Tokenize Inputs\n",
        "\n",
        "    The function will then use a `tokenizer` to convert these sentences into token IDs. For the inputs, the `tokenizer` will be applied with a `max_length` parameter set to `max_input_length` and truncation to ensure that sequences longer than the specified length are truncated.\n",
        "\n",
        "3. Tokenize Targets\n",
        "\n",
        "    Next, the function will tokenize the target sentences, again using the `tokenizer` with a `max_length` parameter set to `max_target_length` and truncation.\n",
        "\n",
        "4. Add Tokenized Targets to Model Inputs\n",
        "\n",
        "    The resulting tokenized target sequences will be added to the `model_inputs` dictionary under the key `\"labels\"`.\n",
        "\n",
        "5. Return Model Inputs\n",
        "\n",
        "    Finally, return the `model_inputs` dictionary, which will now include both the tokenized input sequences and their corresponding tokenized target sequences (labels).\n"
      ],
      "metadata": {
        "id": "xzakbuB876NL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.541473Z",
          "iopub.status.busy": "2023-09-24T15:10:03.540621Z",
          "iopub.status.idle": "2023-09-24T15:10:03.549770Z",
          "shell.execute_reply": "2023-09-24T15:10:03.548874Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.541440Z"
        },
        "id": "vc0BSBLIIrJQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "source_lang = \"en\"\n",
        "target_lang = \"de\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.551620Z",
          "iopub.status.busy": "2023-09-24T15:10:03.551106Z",
          "iopub.status.idle": "2023-09-24T15:10:03.571019Z",
          "shell.execute_reply": "2023-09-24T15:10:03.569879Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.551587Z"
        },
        "id": "-b70jh26IrJS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# example of preprocessing\n",
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:09:35.662567Z",
          "iopub.status.busy": "2023-09-24T16:09:35.662099Z",
          "iopub.status.idle": "2023-09-24T16:09:37.545512Z",
          "shell.execute_reply": "2023-09-24T16:09:37.544516Z",
          "shell.execute_reply.started": "2023-09-24T16:09:35.662533Z"
        },
        "id": "DDtsaJeVIrJT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# for the example purpose we will crop the dataset and select first 5000 for train\n",
        "# and 500 for validation and test\n",
        "cropped_datasets = raw_datasets\n",
        "cropped_datasets['train'] = raw_datasets['train'].select(range(5000))\n",
        "cropped_datasets['validation'] = raw_datasets['validation'].select(range(500))\n",
        "cropped_datasets['test'] = raw_datasets['test'].select(range(500))\n",
        "tokenized_datasets = cropped_datasets.map(preprocess_function, batched=True)\n",
        "tokenized_datasets['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.605060Z",
          "iopub.status.busy": "2023-09-24T15:10:03.604730Z",
          "iopub.status.idle": "2023-09-24T15:10:04.671893Z",
          "shell.execute_reply": "2023-09-24T15:10:04.670859Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.605029Z"
        },
        "id": "TlqNaB8jIrJW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "# create a model for the pretrained model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "About the training arguments:\n",
        "\n",
        "  - An output directory will be created for saving model checkpoints and logs, named based on the model name, source language, and target language.\n",
        "  - The model will be set to evaluate at the end of each epoch.\n",
        "  - The learning rate will be set to 2e-5 for the optimizer.\n",
        "  - Both training and evaluation will use a per-device batch size of 32.\n",
        "  - A weight decay of 0.01 will be applied to the model for regularization.\n",
        "  - A maximum of 3 checkpoints will be saved during training.\n",
        "  - The model will be trained for 10 epochs.\n",
        "  - During evaluation, the model will predict with generation capabilities (e.g., for tasks like translation or summarization).\n",
        "  - Mixed precision training will be enabled to speed up training and reduce memory usage.\n",
        "  - Training metrics will be reported to TensorBoard for visualization"
      ],
      "metadata": {
        "id": "M_qrcaSbBJuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.674163Z",
          "iopub.status.busy": "2023-09-24T15:10:04.673474Z",
          "iopub.status.idle": "2023-09-24T15:10:04.681771Z",
          "shell.execute_reply": "2023-09-24T15:10:04.680562Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.674126Z"
        },
        "id": "Bliy8zgjIrJY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# defining the parameters for training\n",
        "batch_size = 32\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    report_to='tensorboard',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of writing `collate_fn` function we will use `DataCollatorForSeq2Seq`.\n",
        " Simliarly it implements the batch creation for training."
      ],
      "metadata": {
        "id": "OKYI2MTUCS0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.684376Z",
          "iopub.status.busy": "2023-09-24T15:10:04.683883Z",
          "iopub.status.idle": "2023-09-24T15:10:04.693774Z",
          "shell.execute_reply": "2023-09-24T15:10:04.692863Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.684341Z"
        },
        "trusted": true,
        "id": "RL26YDrtwt87"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `postprocess_text` mainly iterates through each prediction (preds) and labels, and strips leading and trailing whitespace.\n",
        "\n",
        "However, for `labels`, it would create a nested list structure where each label is stripped of whitespace and wrapped in another list.\n",
        "\n",
        "Here is how the metrics would be computed:\n",
        "\n",
        "\n",
        "1. **Decoding Predictions and Labels**:\n",
        "   - `preds` would be decoded using `tokenizer.batch_decode(preds, skip_special_tokens=True)`. This will convert the model's token IDs back into readable text, skipping any special tokens.\n",
        "   - `labels` would be decoded similarly, but with special handling for `-100` values. These would be replaced with `tokenizer.pad_token_id` because `-100` indicates tokens that were ignored during training (e.g., padding tokens).\n",
        "\n",
        "2. **Post-processing**:\n",
        "   - After decoding, both `decoded_preds` and `decoded_labels` would undergo post-processing using the `postprocess_text` function. This function would remove leading and trailing whitespace from each prediction (`preds`) and label (`labels`).\n",
        "\n",
        "3. **Metric Calculation**:\n",
        "   - The `metric.compute` function would compute evaluation metrics (like BLEU score) based on the decoded predictions (`decoded_preds`) and decoded labels (`decoded_labels`).\n",
        "\n",
        "4. **Additional Metrics**:\n",
        "   - The function would calculate the average length of the generated predictions (`prediction_lens`) excluding padding tokens.\n",
        "   - The results would be formatted into a dictionary (`result`), where `\"bleu\"` would contain the computed BLEU score, and `\"gen_len\"` would contain the average length of the generated predictions.\n",
        "\n",
        "5. **Output**:\n",
        "   - Finally, the function would return `result`, a dictionary containing `\"bleu\"` and `\"gen_len\"` metrics rounded to four decimal places.\n"
      ],
      "metadata": {
        "id": "pbjLCKGpCqrj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.695838Z",
          "iopub.status.busy": "2023-09-24T15:10:04.695457Z",
          "iopub.status.idle": "2023-09-24T15:10:04.707222Z",
          "shell.execute_reply": "2023-09-24T15:10:04.706315Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.695806Z"
        },
        "id": "UmvbnJ9JIrJd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# simple postprocessing for text\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "# compute metrics function to pass to trainer\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.709249Z",
          "iopub.status.busy": "2023-09-24T15:10:04.708526Z",
          "iopub.status.idle": "2023-09-24T15:10:04.806768Z",
          "shell.execute_reply": "2023-09-24T15:10:04.805816Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.709216Z"
        },
        "id": "imY1oC3SIrJf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# instead of writing trSeq2SeqTrainerain loop we will use Seq2SeqTrainer\n",
        "trainer = (\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model, and save it when it has been trained completely"
      ],
      "metadata": {
        "id": "3L1b_2zVKRKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.808506Z",
          "iopub.status.busy": "2023-09-24T15:10:04.808050Z",
          "iopub.status.idle": "2023-09-24T15:18:44.110261Z",
          "shell.execute_reply": "2023-09-24T15:18:44.109150Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.808459Z"
        },
        "id": "uNx5pyRlIrJh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:19:29.403450Z",
          "iopub.status.busy": "2023-09-24T15:19:29.403061Z",
          "iopub.status.idle": "2023-09-24T15:19:30.003295Z",
          "shell.execute_reply": "2023-09-24T15:19:30.002182Z",
          "shell.execute_reply.started": "2023-09-24T15:19:29.403420Z"
        },
        "trusted": true,
        "id": "be-j2Z9Iwt87"
      },
      "outputs": [],
      "source": [
        "# saving model\n",
        "trainer.save_model('best')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will load the trained model and use it to perform some translations."
      ],
      "metadata": {
        "id": "BB9r6kLmK1xZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:19:30.753608Z",
          "iopub.status.busy": "2023-09-24T15:19:30.753167Z",
          "iopub.status.idle": "2023-09-24T15:19:31.676057Z",
          "shell.execute_reply": "2023-09-24T15:19:31.675005Z",
          "shell.execute_reply.started": "2023-09-24T15:19:30.753575Z"
        },
        "trusted": true,
        "id": "KyYJnIPWwt87"
      },
      "outputs": [],
      "source": [
        "# loading the model and run inference for it\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('best')\n",
        "model.eval()\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:19:31.744595Z",
          "iopub.status.busy": "2023-09-24T15:19:31.744257Z",
          "iopub.status.idle": "2023-09-24T15:19:31.749935Z",
          "shell.execute_reply": "2023-09-24T15:19:31.748926Z",
          "shell.execute_reply.started": "2023-09-24T15:19:31.744568Z"
        },
        "trusted": true,
        "id": "XTbCIVKWwt87"
      },
      "outputs": [],
      "source": [
        "def translate(model, inference_request, tokenizer=tokenizer):\n",
        "    input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids=input_ids)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True,temperature=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:13:02.462385Z",
          "iopub.status.busy": "2023-09-24T16:13:02.461389Z",
          "iopub.status.idle": "2023-09-24T16:13:02.943140Z",
          "shell.execute_reply": "2023-09-24T16:13:02.941981Z",
          "shell.execute_reply.started": "2023-09-24T16:13:02.462348Z"
        },
        "trusted": true,
        "id": "TKYkZeprwt88"
      },
      "outputs": [],
      "source": [
        "inference_request = prefix + 'Why did it take so long to train the model?'\n",
        "translate(model, inference_request,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:13:10.223355Z",
          "iopub.status.busy": "2023-09-24T16:13:10.222968Z",
          "iopub.status.idle": "2023-09-24T16:13:10.549431Z",
          "shell.execute_reply": "2023-09-24T16:13:10.548426Z",
          "shell.execute_reply.started": "2023-09-24T16:13:10.223326Z"
        },
        "trusted": true,
        "id": "uJmnbxuPwt88"
      },
      "outputs": [],
      "source": [
        "inference_request = prefix +\"My name is Wolfgang and I live in Berlin\"\n",
        "translate(model, inference_request,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:17:24.580846Z",
          "iopub.status.busy": "2023-09-24T16:17:24.580402Z",
          "iopub.status.idle": "2023-09-24T16:17:24.852369Z",
          "shell.execute_reply": "2023-09-24T16:17:24.851186Z",
          "shell.execute_reply.started": "2023-09-24T16:17:24.580812Z"
        },
        "trusted": true,
        "id": "FJMMDq_Fwt88"
      },
      "outputs": [],
      "source": [
        "inference_request = prefix + \"Your assignment is hard. Start it today\"\n",
        "translate(model, inference_request,tokenizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
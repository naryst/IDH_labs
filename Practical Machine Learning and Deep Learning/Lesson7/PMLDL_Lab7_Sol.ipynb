{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed04ebc7",
      "metadata": {
        "id": "ed04ebc7"
      },
      "source": [
        "# Practical Machine Learning and Deep Learning\n",
        "\n",
        "# Lesson 7\n",
        "\n",
        "# Semantic Segmentation\n",
        "\n",
        "Semantic segmentation in machine learning is a process where each pixel in an image is classified into a predefined category. Unlike image classification, which assigns a single label to an entire image, semantic segmentation provides a pixel-level understanding, segmenting different objects and regions within the image.\n",
        "\n",
        "### EMaterial Segmentation Dataset\n",
        "\n",
        "Referring to the material segmentation dataset comprising 3817 images gathered from the Virginia Department of Transportation (VDOT) Bridge Inspection Reports, semantic segmentation would involve:\n",
        "\n",
        "1. **Pixel-Level Annotation**:\n",
        "   - Each pixel in the bridge inspection images would be labeled with a specific material category, such as concrete, steel, asphalt, or other materials found in bridge structures.\n",
        "\n",
        "2. **Training a Model**:\n",
        "   - A neural network, often a Convolutional Neural Network (CNN) architecture designed for segmentation tasks (e.g., U-Net, SegNet, or DeepLab), would be trained on this annotated dataset. The model learns to associate pixel patterns with specific material categories.\n",
        "\n",
        "3. **Inference**:\n",
        "   - After training, the model can take a new image as input and output a segmentation map where each pixel is assigned a material category label. This allows for detailed analysis of the materials present in the bridge structure.\n",
        "\n",
        "### Key Steps in Semantic Segmentation\n",
        "\n",
        "1. **Dataset Preparation**:\n",
        "   - The dataset (in this case, the 3817 VDOT images) needs to be annotated at the pixel level, where each pixel is labeled according to the material it represents.\n",
        "\n",
        "2. **Model Architecture**:\n",
        "   - Choose a suitable segmentation model architecture. Popular choices include U-Net, which is effective for biomedical image segmentation but also applicable to other fields, and DeepLab, known for its ability to capture multi-scale contextual information.\n",
        "\n",
        "3. **Training**:\n",
        "   - Train the chosen model on the annotated dataset. This involves feeding the images and their corresponding pixel-level labels into the model, allowing it to learn the patterns associated with each material category.\n",
        "\n",
        "4. **Evaluation**:\n",
        "   - Evaluate the modelâ€™s performance using metrics such as Intersection over Union (IoU), pixel accuracy, and mean average precision to ensure it accurately segments materials in new images.\n",
        "\n",
        "5. **Application**:\n",
        "   - Once trained, the model can be used for automated inspection and analysis of bridge materials in new images, assisting in tasks like detecting material defects, monitoring wear and tear, and planning maintenance activities.\n",
        "\n",
        "### Importance in Real-World Applications\n",
        "\n",
        "Semantic segmentation in the context of the VDOT Bridge Inspection Reports can significantly enhance the efficiency and accuracy of bridge maintenance and inspection processes. By automating the material classification task, engineers can quickly identify and analyze the materials used in bridge construction and their condition, leading to better-informed decisions regarding repairs and maintenance, ultimately improving infrastructure safety and longevity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Dataset"
      ],
      "metadata": {
        "id": "aUQ05C_LZaVN"
      },
      "id": "aUQ05C_LZaVN"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.lib.vt.edu/ndownloader/articles/16624648/versions/1"
      ],
      "metadata": {
        "id": "1wqmVTAFSLlN"
      },
      "id": "1wqmVTAFSLlN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"1\", 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "46bk6Qn2SyIb"
      },
      "id": "46bk6Qn2SyIb",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(\"Material Detection.zip\", 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "ZcYfDSX5S_NI"
      },
      "id": "ZcYfDSX5S_NI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a6a7261c",
      "metadata": {
        "id": "a6a7261c"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "886b3110",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-10-07T13:27:29.214183Z",
          "iopub.status.busy": "2023-10-07T13:27:29.213878Z",
          "iopub.status.idle": "2023-10-07T13:27:35.042355Z",
          "shell.execute_reply": "2023-10-07T13:27:35.041431Z"
        },
        "papermill": {
          "duration": 5.837416,
          "end_time": "2023-10-07T13:27:35.044441",
          "exception": false,
          "start_time": "2023-10-07T13:27:29.207025",
          "status": "completed"
        },
        "tags": [],
        "id": "886b3110"
      },
      "outputs": [],
      "source": [
        "# necessary imports\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Mappings and Constants"
      ],
      "metadata": {
        "id": "SOgIVHaKg6Ql"
      },
      "id": "SOgIVHaKg6Ql"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ce9f6505",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:27:35.055271Z",
          "iopub.status.busy": "2023-10-07T13:27:35.054891Z",
          "iopub.status.idle": "2023-10-07T13:27:35.060476Z",
          "shell.execute_reply": "2023-10-07T13:27:35.059552Z"
        },
        "papermill": {
          "duration": 0.012731,
          "end_time": "2023-10-07T13:27:35.062165",
          "exception": false,
          "start_time": "2023-10-07T13:27:35.049434",
          "status": "completed"
        },
        "tags": [],
        "id": "ce9f6505"
      },
      "outputs": [],
      "source": [
        "# necessary constants\n",
        "CLASS_MAPPING = {\n",
        "    0: \"background\",\n",
        "    1: \"steel\",\n",
        "    2: \"concrete\",  # segment concrete\n",
        "    3: \"metal deck\",\n",
        "}\n",
        "COLOR_MAPPING = {\n",
        "    0: (0, 0, 0),\n",
        "    1: (0, 0, 128),\n",
        "    2: (0, 128, 0),\n",
        "    3: (0, 128, 128),\n",
        "}\n",
        "\n",
        "\n",
        "color2label = {v: k for k, v in COLOR_MAPPING.items()}\n",
        "IMG_SIZE = 256\n",
        "MAX_PIXEL_VALUE = 255\n",
        "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZATION_STD = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a385abb",
      "metadata": {
        "id": "2a385abb"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1d6caa7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:27:35.081193Z",
          "iopub.status.busy": "2023-10-07T13:27:35.080660Z",
          "iopub.status.idle": "2023-10-07T13:27:35.084633Z",
          "shell.execute_reply": "2023-10-07T13:27:35.083824Z"
        },
        "papermill": {
          "duration": 0.010826,
          "end_time": "2023-10-07T13:27:35.086249",
          "exception": false,
          "start_time": "2023-10-07T13:27:35.075423",
          "status": "completed"
        },
        "tags": [],
        "id": "f1d6caa7"
      },
      "outputs": [],
      "source": [
        "train_dir = \"Material Detection/original/Train\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "155dd7da",
      "metadata": {
        "id": "155dd7da"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "For the following lab we will use [Albumentations](https://albumentations.ai/) for the data transforms. Albumentations allows image and mask transformation at the same time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0004184d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:27:35.096399Z",
          "iopub.status.busy": "2023-10-07T13:27:35.096112Z",
          "iopub.status.idle": "2023-10-07T13:27:35.100739Z",
          "shell.execute_reply": "2023-10-07T13:27:35.099760Z"
        },
        "papermill": {
          "duration": 0.01164,
          "end_time": "2023-10-07T13:27:35.102422",
          "exception": false,
          "start_time": "2023-10-07T13:27:35.090782",
          "status": "completed"
        },
        "tags": [],
        "id": "0004184d"
      },
      "outputs": [],
      "source": [
        "transforms = A.Compose(\n",
        "    [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ToFloat(max_value=MAX_PIXEL_VALUE),\n",
        "        A.Normalize(\n",
        "            mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD, max_pixel_value=1.0\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55420452",
      "metadata": {
        "id": "55420452"
      },
      "source": [
        "## Segmentation Dataset Class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SegmentationDataset class handles loading and preprocessing of image and mask data from a specified directory structure for a material segmentation task. It ensures that both images and masks are correctly paired, loaded into memory, and transformed appropriately for training a machine learning model."
      ],
      "metadata": {
        "id": "XXuoTBg8hn2m"
      },
      "id": "XXuoTBg8hn2m"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc79db42",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:27:35.113196Z",
          "iopub.status.busy": "2023-10-07T13:27:35.112683Z",
          "iopub.status.idle": "2023-10-07T13:27:35.122756Z",
          "shell.execute_reply": "2023-10-07T13:27:35.121968Z"
        },
        "papermill": {
          "duration": 0.017614,
          "end_time": "2023-10-07T13:27:35.124404",
          "exception": false,
          "start_time": "2023-10-07T13:27:35.106790",
          "status": "completed"
        },
        "tags": [],
        "id": "fc79db42"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_path, transform):\n",
        "        \"\"\"\n",
        "        Material segmentation dataset\n",
        "\n",
        "        :param root_path: path to train split, which contains images and masks\n",
        "        :param transform: transforms for dataset\n",
        "\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        if not root_path.exists():\n",
        "            raise FileNotFoundError(f\"No root path {root_path} was found\")\n",
        "        self.img_path = root_path / \"images\"\n",
        "        self.mask_path = root_path / \"masks\"\n",
        "\n",
        "        if not self.img_path.exists():\n",
        "            raise FileNotFoundError(\"No images was found\")\n",
        "\n",
        "        if not self.mask_path.exists():\n",
        "            raise FileNotFoundError(\"No masks was found\")\n",
        "\n",
        "        # create list of images and masks\n",
        "        self.img_list = sorted(self._get_filenames(self.img_path))\n",
        "        self.mask_list = sorted(self._get_filenames(self.mask_path))\n",
        "        missing_files = set([f.stem for f in self.img_list]).symmetric_difference(\n",
        "            set([f.stem for f in self.mask_list])\n",
        "        )\n",
        "        if len(missing_files) != 0:\n",
        "            raise FileNotFoundError(f\"Missing files: {missing_files}\")\n",
        "\n",
        "        # load images and masks into memory\n",
        "        self._read_imgs()\n",
        "        self._read_masks()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        mask = self.masks[idx]\n",
        "        transformed = self.transform(image=img, mask=mask)\n",
        "        return transformed[\"image\"].float(), transformed[\"mask\"].long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def _get_filenames(self, path):\n",
        "        return [f for f in path.iterdir() if f.is_file()]\n",
        "\n",
        "    def _read_imgs(self):\n",
        "        \"\"\"\n",
        "        Load images into memory\n",
        "        \"\"\"\n",
        "        self.images = []\n",
        "        for f in tqdm(self.img_list):\n",
        "            img = cv2.imread(f.as_posix())\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)).astype(np.uint8)\n",
        "            self.images.append(img)\n",
        "\n",
        "    def _read_masks(self):\n",
        "        \"\"\"\n",
        "        Load masks into memory and convert multiclass mask\n",
        "        into binary masks for 'concrete' class\n",
        "        \"\"\"\n",
        "        self.masks = []\n",
        "        for f in tqdm(self.mask_list):\n",
        "            mask = cv2.imread(f.as_posix())\n",
        "            mask = (\n",
        "                cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "                / 128\n",
        "            )\n",
        "            binary_mask = mask[..., 1] * 2 + mask[..., 0]\n",
        "            binary_mask[binary_mask != 2] = 0\n",
        "            binary_mask /= 2\n",
        "            self.masks.append(binary_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec223d8a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:27:35.134350Z",
          "iopub.status.busy": "2023-10-07T13:27:35.134086Z",
          "iopub.status.idle": "2023-10-07T13:31:22.160713Z",
          "shell.execute_reply": "2023-10-07T13:31:22.159616Z"
        },
        "papermill": {
          "duration": 227.033733,
          "end_time": "2023-10-07T13:31:22.162614",
          "exception": false,
          "start_time": "2023-10-07T13:27:35.128881",
          "status": "completed"
        },
        "tags": [],
        "id": "ec223d8a"
      },
      "outputs": [],
      "source": [
        "dataset = SegmentationDataset(Path(train_dir), transform=transforms)\n",
        "\n",
        "# splitting dataset into train and validation\n",
        "split_proportion = 0.9\n",
        "size = int(len(dataset) * split_proportion)\n",
        "train_dataset, val_dataset = random_split(dataset, [size, len(dataset) - size])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataloaders\n",
        "\n",
        "We have discussed Dataloaders briefly in previous lesson"
      ],
      "metadata": {
        "id": "j1tDK7GKh3eB"
      },
      "id": "j1tDK7GKh3eB"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ccb37489",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:22.280471Z",
          "iopub.status.busy": "2023-10-07T13:31:22.280130Z",
          "iopub.status.idle": "2023-10-07T13:31:22.285041Z",
          "shell.execute_reply": "2023-10-07T13:31:22.284099Z"
        },
        "papermill": {
          "duration": 0.066689,
          "end_time": "2023-10-07T13:31:22.286693",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.220004",
          "status": "completed"
        },
        "tags": [],
        "id": "ccb37489"
      },
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "batch_size = 8\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e67fa582",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:22.404405Z",
          "iopub.status.busy": "2023-10-07T13:31:22.403697Z",
          "iopub.status.idle": "2023-10-07T13:31:22.410306Z",
          "shell.execute_reply": "2023-10-07T13:31:22.409469Z"
        },
        "papermill": {
          "duration": 0.067957,
          "end_time": "2023-10-07T13:31:22.411920",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.343963",
          "status": "completed"
        },
        "tags": [],
        "id": "e67fa582"
      },
      "outputs": [],
      "source": [
        "# check the sizes of train and validation splits\n",
        "len(train_dataset), len(val_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4741be0e",
      "metadata": {
        "papermill": {
          "duration": 0.057458,
          "end_time": "2023-10-07T13:31:22.526247",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.468789",
          "status": "completed"
        },
        "tags": [],
        "id": "4741be0e"
      },
      "source": [
        "## Model\n",
        "\n",
        "U-Net is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. Every step in the expansive path consists of an upsampling of the feature map, a concatenation with the correspondingly cropped feature map from the contracting path, and convolutions.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "![Alt text](https://media.geeksforgeeks.org/wp-content/uploads/20220614121231/Group14.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ecf1663d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:22.642826Z",
          "iopub.status.busy": "2023-10-07T13:31:22.642485Z",
          "iopub.status.idle": "2023-10-07T13:31:22.654263Z",
          "shell.execute_reply": "2023-10-07T13:31:22.653449Z"
        },
        "papermill": {
          "duration": 0.073333,
          "end_time": "2023-10-07T13:31:22.656033",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.582700",
          "status": "completed"
        },
        "tags": [],
        "id": "ecf1663d"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Block with two convolutional blocks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        \"\"\"\n",
        "        Double convolution\n",
        "\n",
        "        :param in_channels: number of in channels for first conv layer\n",
        "        :param out_channels: number of out channels for last conv layer\n",
        "        :param mid_channels: number of out channels for first conv layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "\n",
        "        # write model that contains 2 conv layer with batch normalization and relu activation function\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"\n",
        "    Block for down path\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Down block\n",
        "\n",
        "        :param in_channels: number of in channels for double conv block\n",
        "        :param out_channels: number of out channels for double conv block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # write model which contains pooling and double conv block\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"\n",
        "    Block for up path\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Down block\n",
        "\n",
        "        :param in_channels: number of in channels for transpose convolution\n",
        "        :param out_channels: number of out channels for double conv block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(\n",
        "            in_channels, in_channels // 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Final convolution block\n",
        "\n",
        "        :param in_channels: number of in channels for conv layer\n",
        "        :param out_channels: number of out channels for conv layer\n",
        "        \"\"\"\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ff61cb3d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:22.772420Z",
          "iopub.status.busy": "2023-10-07T13:31:22.772059Z",
          "iopub.status.idle": "2023-10-07T13:31:22.779557Z",
          "shell.execute_reply": "2023-10-07T13:31:22.778584Z"
        },
        "papermill": {
          "duration": 0.066714,
          "end_time": "2023-10-07T13:31:22.781223",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.714509",
          "status": "completed"
        },
        "tags": [],
        "id": "ff61cb3d"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8955c6d2",
      "metadata": {
        "papermill": {
          "duration": 0.057194,
          "end_time": "2023-10-07T13:31:22.895712",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.838518",
          "status": "completed"
        },
        "tags": [],
        "id": "8955c6d2"
      },
      "source": [
        "## Loss\n",
        "\n",
        "As the loss we will use combination of Cross Entropy Loss and Dice Loss\n",
        "\n",
        "### Dice loss\n",
        "\n",
        "Dice loss is based on [SÃ¸rensen-Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient). It measures the overlap between the predicted and target segmentation masks. Dice loss provides a differentiable and smooth measure of segmentation accuracy.\n",
        "\n",
        "$$\n",
        "DiceLoss\\left( y, \\overline{p} \\right) = 1 - \\dfrac{\\left(  2y\\overline{p} + 1 \\right)} {\\left( y+\\overline{p } + 1 \\right)}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4ac95081",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:23.012904Z",
          "iopub.status.busy": "2023-10-07T13:31:23.012000Z",
          "iopub.status.idle": "2023-10-07T13:31:23.018409Z",
          "shell.execute_reply": "2023-10-07T13:31:23.017558Z"
        },
        "papermill": {
          "duration": 0.067488,
          "end_time": "2023-10-07T13:31:23.020091",
          "exception": false,
          "start_time": "2023-10-07T13:31:22.952603",
          "status": "completed"
        },
        "tags": [],
        "id": "4ac95081"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, eps=1e-6):\n",
        "        \"\"\"\n",
        "        Calculation of dice loss\n",
        "\n",
        "        :param inputs: model predictions\n",
        "        :param targets: target values\n",
        "        :param eps: stability factor, defaults to 1e-6\n",
        "        :return: loss value\n",
        "        \"\"\"\n",
        "        # implement dice loss\n",
        "        inputs = F.softmax(inputs, dim=1)\n",
        "\n",
        "        target_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(\n",
        "            0, 3, 1, 2\n",
        "        )\n",
        "\n",
        "        dims = (1, 2, 3)\n",
        "        intersection = torch.sum(inputs * target_one_hot, dims)\n",
        "        cardinality = torch.sum(inputs + target_one_hot, dims)\n",
        "\n",
        "        dice_score = 2.0 * intersection / (cardinality + eps)\n",
        "\n",
        "        return torch.mean(1.0 - dice_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2422224f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:23.136231Z",
          "iopub.status.busy": "2023-10-07T13:31:23.135585Z",
          "iopub.status.idle": "2023-10-07T13:31:23.467229Z",
          "shell.execute_reply": "2023-10-07T13:31:23.466297Z"
        },
        "papermill": {
          "duration": 0.392312,
          "end_time": "2023-10-07T13:31:23.469276",
          "exception": false,
          "start_time": "2023-10-07T13:31:23.076964",
          "status": "completed"
        },
        "tags": [],
        "id": "2422224f"
      },
      "outputs": [],
      "source": [
        "model = UNet(n_channels=3, n_classes=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "df8dd8f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:23.765147Z",
          "iopub.status.busy": "2023-10-07T13:31:23.764829Z",
          "iopub.status.idle": "2023-10-07T13:31:23.770034Z",
          "shell.execute_reply": "2023-10-07T13:31:23.769079Z"
        },
        "papermill": {
          "duration": 0.06535,
          "end_time": "2023-10-07T13:31:23.771784",
          "exception": false,
          "start_time": "2023-10-07T13:31:23.706434",
          "status": "completed"
        },
        "tags": [],
        "id": "df8dd8f6"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        ")\n",
        "criterion1 = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "criterion2 = DiceLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0430b524",
      "metadata": {
        "id": "0430b524"
      },
      "source": [
        "## Training\n",
        "The function `train_model` will train our segmentation model and validate its performance over a specified number of epochs (default is 10). During each epoch, the model will be set to training mode, and it will process batches of images and corresponding masks from the training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7ba86758",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:23.887041Z",
          "iopub.status.busy": "2023-10-07T13:31:23.886719Z",
          "iopub.status.idle": "2023-10-07T13:31:23.895396Z",
          "shell.execute_reply": "2023-10-07T13:31:23.894426Z"
        },
        "papermill": {
          "duration": 0.068273,
          "end_time": "2023-10-07T13:31:23.897118",
          "exception": false,
          "start_time": "2023-10-07T13:31:23.828845",
          "status": "completed"
        },
        "tags": [],
        "id": "7ba86758"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, optimizer, epochs=10):\n",
        "    \"\"\"\n",
        "    Train a segmentation model and validate its performance.\n",
        "\n",
        "    :param model: The model to be trained.\n",
        "    :param train_loader: DataLoader for the training dataset.\n",
        "    :param val_loader: DataLoader for the validation dataset.\n",
        "    :param device: Device to run the training on (CPU or GPU).\n",
        "    :param optimizer: Optimizer to use for training.\n",
        "    :param epochs: Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        with tqdm(\n",
        "            total=len(train_dataset), desc=f\"Epoch {epoch}/{epochs}\", unit=\"img\"\n",
        "        ) as pbar:\n",
        "            for batch in train_loader:\n",
        "                images, true_masks = batch\n",
        "                images, true_masks = images.to(device), true_masks.to(device)\n",
        "\n",
        "                masks_pred = model(images)\n",
        "                loss = criterion1(masks_pred, true_masks) + criterion2(\n",
        "                    masks_pred, true_masks\n",
        "                )\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                pbar.update(images.shape[0])\n",
        "                epoch_loss += loss.item()\n",
        "                pbar.set_postfix(**{\"loss (batch)\": loss.item()})\n",
        "        model.eval()\n",
        "        with tqdm(total=len(val_dataset), desc=f\"Validation\", unit=\"img\") as pbar:\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    images, true_masks = batch\n",
        "\n",
        "                    images, true_masks = images.to(device), true_masks.to(device)\n",
        "\n",
        "                    masks_pred = model(images)\n",
        "                    loss = criterion1(masks_pred, true_masks) + criterion2(\n",
        "                        masks_pred, true_masks\n",
        "                    )\n",
        "                    pbar.update(images.shape[0])\n",
        "                    epoch_loss += loss.item()\n",
        "                    pbar.set_postfix(**{\"loss (batch)\": loss.item()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06081954",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:31:24.012547Z",
          "iopub.status.busy": "2023-10-07T13:31:24.011741Z",
          "iopub.status.idle": "2023-10-07T13:46:47.314070Z",
          "shell.execute_reply": "2023-10-07T13:46:47.313119Z"
        },
        "papermill": {
          "duration": 923.362327,
          "end_time": "2023-10-07T13:46:47.315915",
          "exception": false,
          "start_time": "2023-10-07T13:31:23.953588",
          "status": "completed"
        },
        "tags": [],
        "id": "06081954"
      },
      "outputs": [],
      "source": [
        "train_model(model, train_loader, val_loader, \"cuda\", optimizer, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's save the model"
      ],
      "metadata": {
        "id": "fdWoy24DkMyz"
      },
      "id": "fdWoy24DkMyz"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3a4f0996",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:48.179084Z",
          "iopub.status.busy": "2023-10-07T13:46:48.178749Z",
          "iopub.status.idle": "2023-10-07T13:46:48.418017Z",
          "shell.execute_reply": "2023-10-07T13:46:48.417002Z"
        },
        "papermill": {
          "duration": 0.673409,
          "end_time": "2023-10-07T13:46:48.420307",
          "exception": false,
          "start_time": "2023-10-07T13:46:47.746898",
          "status": "completed"
        },
        "tags": [],
        "id": "3a4f0996"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"best.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025a9d39",
      "metadata": {
        "id": "025a9d39"
      },
      "source": [
        "## Predict\n",
        "\n",
        "For prediction, we will first follow some transformation steps that we performed earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "25bd7dbd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:49.324026Z",
          "iopub.status.busy": "2023-10-07T13:46:49.323673Z",
          "iopub.status.idle": "2023-10-07T13:46:49.329256Z",
          "shell.execute_reply": "2023-10-07T13:46:49.328224Z"
        },
        "papermill": {
          "duration": 0.428466,
          "end_time": "2023-10-07T13:46:49.330985",
          "exception": false,
          "start_time": "2023-10-07T13:46:48.902519",
          "status": "completed"
        },
        "tags": [],
        "id": "25bd7dbd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_mask(mask, color_mapping=COLOR_MAPPING):\n",
        "    color_mask = np.zeros((*mask.shape[::-1], 3), dtype=np.uint8)\n",
        "    for i in range(mask.shape[1]):\n",
        "        for j in range(mask.shape[0]):\n",
        "            color_mask[i, j] = color_mapping[mask[j, i]]\n",
        "    color_mask = cv2.cvtColor(color_mask, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(color_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c6753bc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:50.947902Z",
          "iopub.status.busy": "2023-10-07T13:46:50.947492Z",
          "iopub.status.idle": "2023-10-07T13:46:50.952929Z",
          "shell.execute_reply": "2023-10-07T13:46:50.952119Z"
        },
        "papermill": {
          "duration": 0.923852,
          "end_time": "2023-10-07T13:46:50.956965",
          "exception": false,
          "start_time": "2023-10-07T13:46:50.033113",
          "status": "completed"
        },
        "tags": [],
        "id": "c6753bc0"
      },
      "outputs": [],
      "source": [
        "test_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=0),\n",
        "        A.ToFloat(max_value=MAX_PIXEL_VALUE),\n",
        "        A.Normalize(\n",
        "            mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD, max_pixel_value=1.0\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "917ddca5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:51.881164Z",
          "iopub.status.busy": "2023-10-07T13:46:51.880843Z",
          "iopub.status.idle": "2023-10-07T13:46:51.886505Z",
          "shell.execute_reply": "2023-10-07T13:46:51.885542Z"
        },
        "papermill": {
          "duration": 0.480101,
          "end_time": "2023-10-07T13:46:51.888203",
          "exception": false,
          "start_time": "2023-10-07T13:46:51.408102",
          "status": "completed"
        },
        "tags": [],
        "id": "917ddca5"
      },
      "outputs": [],
      "source": [
        "RESULTS_SHAPE = (64, 64)\n",
        "\n",
        "\n",
        "def predict(model, img, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Model inference on image\n",
        "\n",
        "    :param model: model\n",
        "    :param img: image\n",
        "    :param device: device for computation, defaults to \"cpu\"\n",
        "    :return: mask\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tensor_img = test_transforms(image=img)[\"image\"]\n",
        "    tensor_img = tensor_img.to(device=device, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor_img)\n",
        "        mask = output.argmax(dim=1)\n",
        "    mask = mask.detach().cpu().numpy()[0].astype(np.uint8)\n",
        "    mask = cv2.resize(mask, RESULTS_SHAPE, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a86db70",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:52.726832Z",
          "iopub.status.busy": "2023-10-07T13:46:52.726496Z",
          "iopub.status.idle": "2023-10-07T13:46:54.609005Z",
          "shell.execute_reply": "2023-10-07T13:46:54.608094Z"
        },
        "papermill": {
          "duration": 2.3107,
          "end_time": "2023-10-07T13:46:54.611124",
          "exception": false,
          "start_time": "2023-10-07T13:46:52.300424",
          "status": "completed"
        },
        "tags": [],
        "id": "6a86db70"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\n",
        "    \"Material Detection/original/Test/images/0.jpeg\"\n",
        ")\n",
        "mask = predict(model, img)\n",
        "plot_mask(mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efeb8ede",
      "metadata": {
        "id": "efeb8ede"
      },
      "source": [
        "## Results\n",
        "\n",
        "Run-Length encoding (RLE)\n",
        "\n",
        "The Run-Length encoding function performs run-length encoding on a binary mask array by first identifying the indices of the foreground pixels. It then iterates through these indices, grouping consecutive pixels into runs. For each run, it records the start position and the length of the run, resulting in a list of start positions and lengths. This method efficiently compresses the mask data by only storing information about the runs of foreground pixels, rather than every individual pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a462d68f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:55.504500Z",
          "iopub.status.busy": "2023-10-07T13:46:55.504077Z",
          "iopub.status.idle": "2023-10-07T13:46:55.510952Z",
          "shell.execute_reply": "2023-10-07T13:46:55.509860Z"
        },
        "papermill": {
          "duration": 0.427417,
          "end_time": "2023-10-07T13:46:55.512683",
          "exception": false,
          "start_time": "2023-10-07T13:46:55.085266",
          "status": "completed"
        },
        "tags": [],
        "id": "a462d68f"
      },
      "outputs": [],
      "source": [
        "def rle_encode(x, fg_val=1):\n",
        "    dots = np.where(x.T.flatten() == fg_val)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def list_to_string(x):\n",
        "    if x:  # non-empty list\n",
        "        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n",
        "    else:\n",
        "        s = \"-\"\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a55d89",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:56.403393Z",
          "iopub.status.busy": "2023-10-07T13:46:56.402858Z",
          "iopub.status.idle": "2023-10-07T13:46:56.794714Z",
          "shell.execute_reply": "2023-10-07T13:46:56.793745Z"
        },
        "papermill": {
          "duration": 0.863982,
          "end_time": "2023-10-07T13:46:56.796465",
          "exception": false,
          "start_time": "2023-10-07T13:46:55.932483",
          "status": "completed"
        },
        "tags": [],
        "id": "49a55d89"
      },
      "outputs": [],
      "source": [
        "model = UNet(n_channels=3, n_classes=2)\n",
        "model.load_state_dict(torch.load(\"best.pt\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c593e54",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T13:46:57.635240Z",
          "iopub.status.busy": "2023-10-07T13:46:57.634874Z",
          "iopub.status.idle": "2023-10-07T13:47:17.501173Z",
          "shell.execute_reply": "2023-10-07T13:47:17.500236Z"
        },
        "papermill": {
          "duration": 20.292345,
          "end_time": "2023-10-07T13:47:17.502955",
          "exception": false,
          "start_time": "2023-10-07T13:46:57.210610",
          "status": "completed"
        },
        "tags": [],
        "id": "6c593e54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(columns=[\"id\", \"pixels\"])\n",
        "test_dir = \"/content/Material Detection/original/Test/images/\"\n",
        "for i, f in tqdm(enumerate(os.listdir(test_dir)), total=len(os.listdir(test_dir))):\n",
        "    img = cv2.imread(test_dir + f)\n",
        "    mask = predict(model, img, device=\"cuda\")\n",
        "    pred = list_to_string(rle_encode(mask))\n",
        "    df.loc[i] = [f[:-5], pred]\n",
        "df.to_csv(\"results.csv\", index=None)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1195.259307,
      "end_time": "2023-10-07T13:47:21.458888",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-07T13:27:26.199581",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
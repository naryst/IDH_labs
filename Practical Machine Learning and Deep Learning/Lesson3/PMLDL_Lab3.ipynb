{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPaLQ2ls1Ufg"
      },
      "source": [
        "# Practical Machine Learning and Deep Learning\n",
        "# Lesson 3: Deep Learning in Natural Language Processing\n",
        "\n",
        "\n",
        "## About the Data\n",
        "The data is taken from Amazon Product Review\n",
        "#### Files\n",
        "train.csv - training 40k Amazon product reviews\n",
        "\n",
        "test.csv - 10k reviews for test\n",
        "#### Categories\n",
        "There are 6 target categories: health personal care, toys games, beauty, pet supplies, baby products, and grocery gourmet food.\n",
        "\n",
        "#### Columns\n",
        "1. Title\n",
        "Format is string\n",
        "2. Helpfulness - Assessment of review from other users\n",
        "Format is int/int. 4/5 means that from 5 assessments 4 are helpful and 1 is not helpful\n",
        "3. Score - Score assigned by the user\n",
        "Format is float. 1.0 is minimum, 5.0 is maximum.\n",
        "4. Text - Text of the review\n",
        "Format is string\n",
        "5. Category - one of the 6 target categories\n",
        "Format is string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data\n",
        "\n",
        "We have training data and testing data available seperately. So instead of reading complete data and then splitting into train and test, we will read them separately. However, later in this lab we will split the test data further."
      ],
      "metadata": {
        "id": "8BAuaPYH7_yJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:47.753751Z",
          "iopub.status.busy": "2023-09-10T16:15:47.753403Z",
          "iopub.status.idle": "2023-09-10T16:15:48.039971Z",
          "shell.execute_reply": "2023-09-10T16:15:48.038883Z",
          "shell.execute_reply.started": "2023-09-10T16:15:47.753722Z"
        },
        "trusted": true,
        "id": "eLN1LxdK1Ufx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_dataframe = pd.read_csv('train.csv')\n",
        "test_dataframe = pd.read_csv('test.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_IQljLt1Ufu"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Text preprocessing in Natural Language Processing (NLP) involves a series of steps to clean, normalize, and prepare raw text data for further analysis or modeling. It is a crucial step because it transforms messy, unstructured text data into a structured form that machine learning models can understand and work with more effectively.\n",
        "\n",
        "The various text preprocessing steps are:\n",
        "\n",
        "* Tokenization\n",
        "* Lower casing\n",
        "* Stop words removal\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "\n",
        "These various text preprocessing steps are widely used for dimensionality reduction.\n",
        "\n",
        "But before that, let's look at the data that we have:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe.head()"
      ],
      "metadata": {
        "id": "eK6FjhvCLeDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od5TdYgo1Uf2"
      },
      "source": [
        "In the training data we have `4` features (`Title`, `Helpfulness`, `Score` and `Text`) with target category (`Category`). For the test features are the same, except for target column.\n",
        "\n",
        "\n",
        "## Normalize the values\n",
        "\n",
        "First, let's write functions for preprocessing helpfulness and score feature in case we needed them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.042376Z",
          "iopub.status.busy": "2023-09-10T16:15:48.041992Z",
          "iopub.status.idle": "2023-09-10T16:15:48.049601Z",
          "shell.execute_reply": "2023-09-10T16:15:48.048121Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.042346Z"
        },
        "trusted": true,
        "id": "Zif-goQR1Uf2"
      },
      "outputs": [],
      "source": [
        "def preprocess_score_inplace(df):\n",
        "    \"\"\"\n",
        "    Normalizes score to make it from 0 to 1.\n",
        "\n",
        "    For now it is from 1.0 to 5.0, so natural choice\n",
        "    is to normalize by (f - 1.0)/4.0\n",
        "    \"\"\"\n",
        "    # Subtract 1.0 from each score, changing the range from [1.0, 5.0] to [0.0, 4.0].\n",
        "    # Divide by 4.0, normalizing the range from [0.0, 4.0] to [0.0, 1.0].\n",
        "    df['Score'] = (df['Score'] - 1.0) / 4.0\n",
        "    return df\n",
        "\n",
        "def preprocess_helpfulness_inplace(df):\n",
        "    \"\"\"\n",
        "    Splits feature by '/' and normalize helpfulness to make it from 0 to 1\n",
        "\n",
        "    The total number of assessments can be 0, so let's substitute it\n",
        "    with 1. The resulting helpfulness still will be zero but we\n",
        "    remove the possibility of division by zero exception.\n",
        "    \"\"\"\n",
        "    # Split the 'Helpfulness' column by the '/' delimiter, creating two new columns: _helpful and _total.\n",
        "    _splitted = df['Helpfulness'].str.split('/', expand=True)\n",
        "    _helpful, _total = _splitted[0], _splitted[1]\n",
        "    # Replace all instances of \"0\" in _total with \"1\" to prevent division by zero.\n",
        "    _total.replace(\"0\", \"1\", inplace=True)\n",
        "    # Convert _helpful and _total to integers and computes the ratio, normalizing the 'Helpfulness' values to a range from 0 to 1.\n",
        "    df['Helpfulness'] = _helpful.astype(int) / _total.astype(int)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hKZXe4d1Uf3"
      },
      "source": [
        "## Concat the textual data\n",
        "\n",
        "The two other features are both text. For simplicity, let's remove concatenate them so that we will have one full text feature. The resulting code is also a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.051984Z",
          "iopub.status.busy": "2023-09-10T16:15:48.051605Z",
          "iopub.status.idle": "2023-09-10T16:15:48.070372Z",
          "shell.execute_reply": "2023-09-10T16:15:48.068944Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.051953Z"
        },
        "trusted": true,
        "id": "jF-gwXIT1Uf3"
      },
      "outputs": [],
      "source": [
        "def concat_title_text_inplace(df):\n",
        "    \"\"\"\n",
        "    Concatenates Title and Text columns together\n",
        "    \"\"\"\n",
        "    df['Text'] = df['Title'] + \" \" + df['Text']\n",
        "    df.drop('Title', axis=1, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUL2UT3y1Uf3"
      },
      "source": [
        "## Encode the Target Values\n",
        "\n",
        "Also, encode the target categories, so that the output is become an index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.074412Z",
          "iopub.status.busy": "2023-09-10T16:15:48.073975Z",
          "iopub.status.idle": "2023-09-10T16:15:48.083943Z",
          "shell.execute_reply": "2023-09-10T16:15:48.082650Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.074375Z"
        },
        "trusted": true,
        "id": "UDa5dNhI1Uf3"
      },
      "outputs": [],
      "source": [
        "# define categories indices\n",
        "cat2idx = {\n",
        "    'toys games': 0,\n",
        "    'health personal care': 1,\n",
        "    'beauty': 2,\n",
        "    'baby products': 3,\n",
        "    'pet supplies': 4,\n",
        "    'grocery gourmet food': 5,\n",
        "}\n",
        "# define reverse mapping\n",
        "idx2cat = {\n",
        "    v:k for k,v in cat2idx.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.086171Z",
          "iopub.status.busy": "2023-09-10T16:15:48.085714Z",
          "iopub.status.idle": "2023-09-10T16:15:48.102570Z",
          "shell.execute_reply": "2023-09-10T16:15:48.101327Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.086136Z"
        },
        "trusted": true,
        "id": "19Et28mH1Uf4"
      },
      "outputs": [],
      "source": [
        "def encode_categories(df):\n",
        "    df['Category'] = df['Category'].apply(lambda x: cat2idx[x])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSRSq15V1Uf4"
      },
      "source": [
        "Let's visualize our first stage of preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.104645Z",
          "iopub.status.busy": "2023-09-10T16:15:48.104266Z",
          "iopub.status.idle": "2023-09-10T16:15:48.130026Z",
          "shell.execute_reply": "2023-09-10T16:15:48.128681Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.104612Z"
        },
        "trusted": true,
        "id": "mZjCP4FN1Uf6"
      },
      "outputs": [],
      "source": [
        "train_copy = train_dataframe.head().copy()\n",
        "\n",
        "encode_categories(preprocess_score_inplace(preprocess_helpfulness_inplace(concat_title_text_inplace(train_copy))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl0rKzaN1Uf8"
      },
      "source": [
        "### Text cleaning\n",
        "\n",
        "For text cleaning, you can use lower casting, punctuation removal, numbers removal, tokenization, stop words removal, stemming. This will get a perfectly cleaned text without any garbage information."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## EXERCISE 1:\n",
        "\n",
        "\n",
        "  1. Lower text - In python you can use .lower() or .upper() functions to change the case of string\n",
        "\n",
        "  For example:\n",
        "\n",
        "  `\"PmLDl\".upper()`\n",
        "  would result in\n",
        "  `PMLDL`\n",
        "\n",
        "\n",
        "  2. Remove numbers - Use [Regex](https://docs.python.org/3/library/re.html) to manipulate string to remove numbers efficiently and replace with a space\n",
        "\n",
        "  For example:\n",
        "\n",
        "  \"123abc456def789\" ==> \" abc def \"\n",
        "\n",
        "\n",
        "  3. Remove Punctuation - Remove the Punction from string like the step above and and replace with a space\n",
        "\n",
        "  For example:\n",
        "\n",
        "  \"hello, what's up??\" ==> \"hello  what s up \"\n",
        "\n",
        "\n",
        "  4. Remove Multiple Spaces - You can use any approach for this and replace with a space\n",
        "\n",
        "  For example:\n",
        "\n",
        "  \"hi  how are you?\" ==> \"hi how are you?\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AElOrzp-Z6B-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.132208Z",
          "iopub.status.busy": "2023-09-10T16:15:48.131538Z",
          "iopub.status.idle": "2023-09-10T16:15:48.139413Z",
          "shell.execute_reply": "2023-09-10T16:15:48.138057Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.132177Z"
        },
        "trusted": true,
        "id": "CDy7fw-J1Uf9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def lower_text(text: str):\n",
        "    ...\n",
        "\n",
        "def remove_numbers(text: str):\n",
        "    \"\"\"\n",
        "    Substitute all punctuations with space in case of\n",
        "    \"there is5dogs\".\n",
        "\n",
        "    If subs with '' -> \"there is5dogs\"\n",
        "    With ' ' -> there is dogs\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "def remove_punctuation(text: str):\n",
        "    \"\"\"\n",
        "    Substitute all punctiations with space in case of\n",
        "    \"hello!nice to meet you\"\n",
        "\n",
        "    If subs with '' -> \"hellonice to meet you\"\n",
        "    With ' ' -> \"hello nice to meet you\"\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "def remove_multiple_spaces(text: str):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_gT1puS1Uf9"
      },
      "source": [
        "This will give us clean text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert lower_text(\"MiXeD CaSe\") == \"mixed case\"\n",
        "assert remove_numbers(\"123abc456def789\") == \" abc def \"\n",
        "assert remove_punctuation(\"hello, what's up??\") == \"hello  what s up \"\n",
        "assert remove_multiple_spaces(\"hi  how are you?\") == \"hi how are you?\"\n",
        "\n",
        "print(\"Passed all test cases\")"
      ],
      "metadata": {
        "id": "iKJqkmhtdbB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.141589Z",
          "iopub.status.busy": "2023-09-10T16:15:48.141224Z",
          "iopub.status.idle": "2023-09-10T16:15:48.161404Z",
          "shell.execute_reply": "2023-09-10T16:15:48.159832Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.141559Z"
        },
        "trusted": true,
        "id": "a2c8x4iP1Uf_"
      },
      "outputs": [],
      "source": [
        "sample_text = train_copy['Text'][4]\n",
        "\n",
        "_lowered = lower_text(sample_text)\n",
        "_without_numbers = remove_numbers(_lowered)\n",
        "_without_punct = remove_punctuation(_without_numbers)\n",
        "_single_spaced = remove_multiple_spaces(_without_punct)\n",
        "\n",
        "print(sample_text)\n",
        "print('-'*10)\n",
        "print(_lowered)\n",
        "print('-'*10)\n",
        "print(_without_numbers)\n",
        "print('-'*10)\n",
        "print(_without_punct)\n",
        "print('-'*10)\n",
        "print(_single_spaced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7gFqQi1Uf_"
      },
      "source": [
        "Now, harder preprocessing: tokenization, stop words removal and stemming.\n",
        "\n",
        "### Tokenization\n",
        "Tokenization is the process of breaking down a text into smaller units called tokens. Tokens can be words, subwords, or characters.\n",
        "Tokenization helps in converting raw text into a structured format that can be easily analyzed. It is the first step in many NLP tasks.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Input: \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "\n",
        "### Stop Words Removal\n",
        "Stop words are common words that usually do not carry significant meaning and are often removed from text data. Examples include \"the\", \"is\", \"in\", \"and\", etc.\n",
        "Removing stop words helps in reducing the size of the dataset and focusing on the more meaningful words, which can improve the performance of NLP models.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Input: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "\n",
        "Output: [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"]\n",
        "\n",
        "### Stemming\n",
        "Stemming is the process of reducing words to their root or base form. This is done by removing suffixes and prefixes. The resulting root form may not be a valid word.\n",
        "Stemming helps in normalizing words to their base form, which reduces the number of unique words in the text and helps in identifying related words.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Input: [\"running\", \"jumps\", \"easily\", \"faster\"]\n",
        "\n",
        "Output: [\"run\", \"jump\", \"easili\", \"fast\"]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## EXERCISE 2:\n",
        "\n",
        "  Implement the functions to work like shown in examples above\n",
        "\n",
        "  ### Tools\n",
        "\n",
        "  For that you can use several packages, but we encourage you to use `nltk` - Natural Language ToolKit as well as `torchtext`.\n",
        "\n",
        "\n",
        "  Take a look at:\n",
        "  * `nltk.tokenize.word_tokenize` or `torchtext.data.utils.get_tokenizer` for tokenization\n",
        "  * `nltk.corpus.stopwords` for stop words removal\n",
        "  * `nltk.stem.PorterStemmer` for stemming\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.165884Z",
          "iopub.status.busy": "2023-09-10T16:15:48.165507Z",
          "iopub.status.idle": "2023-09-10T16:15:48.788344Z",
          "shell.execute_reply": "2023-09-10T16:15:48.787453Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.165854Z"
        },
        "trusted": true,
        "id": "Ar65Mn-T1Uf_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize_text(text: str) -> list[str]:\n",
        "  ...\n",
        "\n",
        "def remove_stop_words(tokenized_text: list[str]) -> list[str]:\n",
        "  ...\n",
        "\n",
        "def stem_words(tokenized_text: list[str]) -> list[str]:\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenize_text():\n",
        "    text = \"This is a sample sentence.\"\n",
        "    expected_tokens = [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \".\"]\n",
        "    assert tokenize_text(text) == expected_tokens\n",
        "    print(\"tokenize_text test passed.\")\n",
        "\n",
        "test_tokenize_text()\n",
        "\n",
        "def test_remove_stop_words():\n",
        "    tokenized_text = [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \".\"]\n",
        "    expected_output = [\"sample\", \"sentence\", \".\"]\n",
        "    assert remove_stop_words(tokenized_text) == expected_output\n",
        "    print(\"remove_stop_words test passed.\")\n",
        "\n",
        "test_remove_stop_words()\n",
        "\n",
        "def test_stem_words():\n",
        "    tokenized_text = [\"running\", \"quickly\", \"cats\"]\n",
        "    expected_output = [\"run\", \"quickli\", \"cat\"]\n",
        "    assert stem_words(tokenized_text) == expected_output\n",
        "    print(\"stem_words test passed.\")\n",
        "\n",
        "test_stem_words()\n",
        "\n"
      ],
      "metadata": {
        "id": "rQpYA5Ldmrbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.790311Z",
          "iopub.status.busy": "2023-09-10T16:15:48.789608Z",
          "iopub.status.idle": "2023-09-10T16:15:48.808359Z",
          "shell.execute_reply": "2023-09-10T16:15:48.806882Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.790278Z"
        },
        "trusted": true,
        "id": "zLIhqSsZ1Uf_"
      },
      "outputs": [],
      "source": [
        "_tokenized = tokenize_text(_single_spaced)\n",
        "_without_sw = remove_stop_words(_tokenized)\n",
        "_stemmed = stem_words(_without_sw)\n",
        "\n",
        "print(_single_spaced)\n",
        "print('-'*10)\n",
        "print(_tokenized)\n",
        "print('-'*10)\n",
        "print(_without_sw)\n",
        "print('-'*10)\n",
        "print(_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfZyIoTk1Uf_"
      },
      "source": [
        "As you can see, there is a lot of words removed as well as the unnecessary language rules. Now we are able to construct full cleaning preprocessing stage.\n",
        "\n",
        "---\n",
        "\n",
        "## EXERCISE 3:\n",
        "\n",
        "Create the function preprocessing_stage to apply the processing steps to a piece of string in following order:\n",
        "\n",
        "\n",
        "  This function applies the following preprocessing steps in order:\n",
        "\n",
        "1.   Convert text to lowercase.\n",
        "\n",
        "2.   Remove numbers from the text.\n",
        "\n",
        "3. Remove punctuation from the text.\n",
        "\n",
        "4. Replace multiple spaces with a single space.\n",
        "\n",
        "5. Tokenize the text.\n",
        "\n",
        "6. Remove stop words from the tokenized text.\n",
        "\n",
        "7. Apply stemming to the remaining tokens.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.810386Z",
          "iopub.status.busy": "2023-09-10T16:15:48.809971Z",
          "iopub.status.idle": "2023-09-10T16:15:48.818839Z",
          "shell.execute_reply": "2023-09-10T16:15:48.817502Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.810343Z"
        },
        "trusted": true,
        "id": "pCOjPHB11Uf_"
      },
      "outputs": [],
      "source": [
        "def preprocessing_stage(text):\n",
        "    \"\"\"\n",
        "    Performs a series of preprocessing steps on the input text.\n",
        "\n",
        "    This function applies the following preprocessing steps in order:\n",
        "    1. Converts text to lowercase.\n",
        "    2. Removes numbers from the text.\n",
        "    3. Removes punctuation from the text.\n",
        "    4. Replaces multiple spaces with a single space.\n",
        "    5. Tokenizes the text.\n",
        "    6. Removes stop words from the tokenized text.\n",
        "    7. Applies stemming to the remaining tokens.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of preprocessed and stemmed tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    ...\n",
        "\n",
        "\n",
        "def clean_text_inplace(df):\n",
        "    \"\"\"\n",
        "    Applies the preprocessing_stage function to the 'Text' column of the DataFrame.\n",
        "\n",
        "    This function modifies the DataFrame in place by applying the preprocessing_stage\n",
        "    function to each entry in the 'Text' column.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The DataFrame with preprocessed 'Text' column.\n",
        "    \"\"\"\n",
        "\n",
        "    df['Text'] = df['Text'].apply(preprocessing_stage)\n",
        "    return df\n",
        "\n",
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    Applies a series of preprocessing steps to the DataFrame.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Fills any missing values in the DataFrame with a space.\n",
        "    2. Normalizes the 'Score' column.\n",
        "    3. Normalizes the 'Helpfulness' column.\n",
        "    4. Concatenates the 'Title' and 'Text' columns.\n",
        "    5. Encodes the 'Category' column if it exists.\n",
        "    6. Applies text cleaning to the 'Text' column.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The input DataFrame to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The preprocessed DataFrame.\n",
        "    \"\"\"\n",
        "    df.fillna(\" \", inplace=True)\n",
        "    _preprocess_score = preprocess_score_inplace(df)\n",
        "    _preprocess_helpfulness = preprocess_helpfulness_inplace(_preprocess_score)\n",
        "    _concatted = concat_title_text_inplace(_preprocess_helpfulness)\n",
        "\n",
        "    if 'Category' in df.columns:\n",
        "        _encoded = encode_categories(_concatted)\n",
        "        _cleaned = clean_text_inplace(_encoded)\n",
        "    else:\n",
        "        _cleaned = clean_text_inplace(_concatted)\n",
        "    return _cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocessing_stage(\"Hi, this is a sample text :)\")== ['hi', 'sampl', 'text']\n",
        "print(\"Preprocessing stage Passed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouMQxILQlmd1",
        "outputId": "d2aa6c85-f8f5-4a8d-fd95-ebe829c745f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing stage Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFn36b3W1Uf_"
      },
      "source": [
        "And now let's apply it on our train and test dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:15:48.820167Z",
          "iopub.status.busy": "2023-09-10T16:15:48.819847Z",
          "iopub.status.idle": "2023-09-10T16:17:03.123896Z",
          "shell.execute_reply": "2023-09-10T16:17:03.122724Z",
          "shell.execute_reply.started": "2023-09-10T16:15:48.820143Z"
        },
        "trusted": true,
        "id": "pXATCucq1UgA"
      },
      "outputs": [],
      "source": [
        "train_preprocessed = preprocess(train_dataframe)\n",
        "test_preprocessed = preprocess(test_dataframe)\n",
        "\n",
        "train_preprocessed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGG8RqC1UgA"
      },
      "source": [
        "Now, let's split our original train dataset into train and val sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:17:03.127989Z",
          "iopub.status.busy": "2023-09-10T16:17:03.127443Z",
          "iopub.status.idle": "2023-09-10T16:17:03.152760Z",
          "shell.execute_reply": "2023-09-10T16:17:03.151812Z",
          "shell.execute_reply.started": "2023-09-10T16:17:03.127953Z"
        },
        "trusted": true,
        "id": "DYcQ7S0h1UgA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ratio = 0.2\n",
        "train, val = train_test_split(\n",
        "    train_preprocessed, stratify=train_preprocessed['Category'], test_size=0.2, random_state=420\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTLTIvKd1UgA"
      },
      "source": [
        "And now, for the best result, lets get rid of pandas so that nothing is stopping us from working with torchtext. For that let's create an iterator that is going to yield samples for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVPpShlA1UgA"
      },
      "source": [
        "# Creating dataloaders\n",
        "\n",
        "First, you should generate our vocab from the train set.\n",
        "\n",
        "For that, use `torchtext.vocab.build_vocab_from_iterator`.\n",
        "\n",
        "\n",
        "The function, `yield_tokens`, will take a DataFrame (df) as input and iterate over each row using iterrows(). For each row (sample), it will convert the row to a list and yield a list of tokens. The function effectively serves as a generator that will be used to provide tokens to build the vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:32:44.269868Z",
          "iopub.status.busy": "2023-09-10T16:32:44.269463Z",
          "iopub.status.idle": "2023-09-10T16:32:47.174358Z",
          "shell.execute_reply": "2023-09-10T16:32:47.173092Z",
          "shell.execute_reply.started": "2023-09-10T16:32:44.269831Z"
        },
        "trusted": true,
        "id": "PcjnGtts1UgA"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(df):\n",
        "    for _, sample in train.iterrows():\n",
        "        yield sample.to_list()[2]\n",
        "\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train), specials=special_symbols)\n",
        "vocab.set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6h0dC6T1UgA"
      },
      "source": [
        "And then use our vocab to encode the tokenized sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:33:09.988862Z",
          "iopub.status.busy": "2023-09-10T16:33:09.988458Z",
          "iopub.status.idle": "2023-09-10T16:33:09.997309Z",
          "shell.execute_reply": "2023-09-10T16:33:09.996137Z",
          "shell.execute_reply.started": "2023-09-10T16:33:09.988828Z"
        },
        "trusted": true,
        "id": "Uch6YlM21UgA"
      },
      "outputs": [],
      "source": [
        "sample = train['Text'][2]\n",
        "print(sample)\n",
        "encoded = vocab(sample)\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ep9Tvc1UgA"
      },
      "source": [
        "Now we can define our collate function and create dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## EXERCISE 4:\n",
        "\n",
        "Write a `collate_batch` function that will be designed to take a batch of data and process it into a format suitable for input into a neural network. Here’s what this function will do:\n",
        "\n",
        "1. Initialize Lists:\n",
        "\n",
        "  `label_list, text_list, score_list` and `helpfulness_list` to store respective data from each sample in the batch.\n",
        "\n",
        "  `offsets` to store the starting index of each sequence in the concatenated batch (useful for models like RNNs that require this information).\n",
        "\n",
        "2. Iterate Over Batch:\n",
        "\n",
        "  For each item in the batch, extract `_helpfulnes, _score, _text, and _label`.\n",
        "  Append `_label` to `label_list`.\n",
        "\n",
        "  Convert `_text` using text_pipeline, wrap it in a tensor, and append to `text_list`.\n",
        "\n",
        "  Append `_score` to `score_list`.\n",
        "\n",
        "  Append `_helpfulnes` to `helpfulness_list`.\n",
        "\n",
        "  Calculate the cumulative length of sequences for offsets.\n",
        "\n",
        "3. Pad Sequences:\n",
        "\n",
        "  Use `pad_sequence` to pad the text sequences so that all sequences in the batch have the same length. The padding_value of 1 is used here.\n",
        "\n",
        "4. Convert Lists to Tensors:\n",
        "\n",
        "  Convert `label_list`, `score_list`, `helpfulness_list`, and `offsets` to tensors.\n",
        "\n",
        "  Move these tensors to the specified device (CPU or GPU).\n",
        "\n",
        "5. Return Tensors:\n",
        "\n",
        "  Return the batched tensors ready for input to the model.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "tqK6HgCiRNMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T16:35:37.307444Z",
          "iopub.status.busy": "2023-09-10T16:35:37.307110Z",
          "iopub.status.idle": "2023-09-10T16:35:37.324888Z",
          "shell.execute_reply": "2023-09-10T16:35:37.323537Z",
          "shell.execute_reply.started": "2023-09-10T16:35:37.307400Z"
        },
        "trusted": true,
        "id": "RF7V89AJ1UgA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as Fun\n",
        "\n",
        "torch.manual_seed(420)\n",
        "\n",
        "text_pipeline = lambda x: vocab(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    ...\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train.to_numpy(), batch_size=128, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Xh7DUr1UgB"
      },
      "source": [
        "# Defining Network\n",
        "\n",
        "\n",
        "For writing a network we can use `torch.nn.Embedding` or `torch.nn.EmbeddingBag`. This will allow your netorwk to learn embedding vector for your tokens.\n",
        "\n",
        "As for the other modules in your network, we can consider these options:\n",
        "* Simple Linear layers, activations, basic stuff that goes into the network\n",
        "* There is a possible of not using the offsets (indices of sequences) in the formard, put use predefined sequence length (maximum length, some value, etc.). If this is an option for you, change the `collate_batch` function according to your architecture.\n",
        "* You could use all this recurrent stuff (RNN, GRU, LSTM, even Transformer, all up to you), but remembder about the dimentions and hidden states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:00:10.267625Z",
          "iopub.status.busy": "2023-09-10T17:00:10.267160Z",
          "iopub.status.idle": "2023-09-10T17:00:10.276201Z",
          "shell.execute_reply": "2023-09-10T17:00:10.274651Z",
          "shell.execute_reply.started": "2023-09-10T17:00:10.267583Z"
        },
        "trusted": true,
        "id": "ld7al5SY1UgB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        num_words = len(vocab.get_itos())\n",
        "        embed_dim = 1024\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_words, embedding_dim=embed_dim)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        # a simple bidirectional lstm with an hidden_dim of 128\n",
        "        self.lstm = nn.LSTM(embed_dim, 128, bidirectional=True, batch_first=True, num_layers=2, dropout=0.5)\n",
        "        # output layer is a layer which has only one output\n",
        "        # input(512) = 128+128 for mean and same for max pooling\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(512, num_classes),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, text):\n",
        "        x = self.embedding(text)\n",
        "        x = self.dropout(x)\n",
        "        # move the embedding output to lstm\n",
        "        x,_ = self.lstm(x)\n",
        "        # apply mean and max pooling on lstm output\n",
        "        avg_pool = torch.mean(x,1)\n",
        "        max_pool, _ = torch.max(x,1)\n",
        "        # concatenate mean and max pooling this is why 512\n",
        "        # 128 for each direction = 256\n",
        "        # avg_pool = 256, max_pool = 256\n",
        "        out = torch.cat((avg_pool,max_pool), 1)\n",
        "        # pass through the output layer and return the output\n",
        "        out = self.out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:01:07.059754Z",
          "iopub.status.busy": "2023-09-10T17:01:07.059384Z",
          "iopub.status.idle": "2023-09-10T17:01:07.070400Z",
          "shell.execute_reply": "2023-09-10T17:01:07.069450Z",
          "shell.execute_reply.started": "2023-09-10T17:01:07.059730Z"
        },
        "trusted": true,
        "id": "3scxfg881UgB"
      },
      "outputs": [],
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    epoch_num=-1\n",
        "):\n",
        "    loop = tqdm(\n",
        "        enumerate(loader, 1),\n",
        "        total=len(loader),\n",
        "        desc=f\"Epoch {epoch_num}: train\",\n",
        "        leave=True,\n",
        "    )\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for i, batch in loop:\n",
        "        labels, texts, offsets, scores, helpfulness = batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(texts)\n",
        "        # loss calculation\n",
        "#         loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer run\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loop.set_postfix({\"loss\": train_loss/(i * len(labels))})\n",
        "\n",
        "def val_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    loss_fn,\n",
        "    epoch_num=-1,\n",
        "    best_so_far=0.0,\n",
        "    ckpt_path='best.pt'\n",
        "):\n",
        "\n",
        "    loop = tqdm(\n",
        "        enumerate(loader, 1),\n",
        "        total=len(loader),\n",
        "        desc=f\"Epoch {epoch_num}: val\",\n",
        "        leave=True,\n",
        "    )\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # evaluation mode\n",
        "        for i, batch in loop:\n",
        "            labels, texts, offsets, scores, helpfulness = batch\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(texts)\n",
        "            # loss calculation\n",
        "#             loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            _, predicted = outputs.data.max(1, keepdim=True)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels.data.view_as(predicted)).sum()\n",
        "\n",
        "            val_loss += loss\n",
        "            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total})\n",
        "\n",
        "        if correct / total > best_so_far:\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            return correct / total\n",
        "\n",
        "    return best_so_far"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:15:00.046131Z",
          "iopub.status.busy": "2023-09-10T17:15:00.045790Z",
          "iopub.status.idle": "2023-09-10T17:15:00.065114Z",
          "shell.execute_reply": "2023-09-10T17:15:00.063305Z",
          "shell.execute_reply.started": "2023-09-10T17:15:00.046106Z"
        },
        "trusted": true,
        "id": "8ZTJ8kkY1UgC"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "model = TextClassificationModel(len(train_preprocessed['Category'].unique())).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:15:00.512516Z",
          "iopub.status.busy": "2023-09-10T17:15:00.512175Z",
          "iopub.status.idle": "2023-09-10T17:15:48.081406Z",
          "shell.execute_reply": "2023-09-10T17:15:48.079846Z",
          "shell.execute_reply.started": "2023-09-10T17:15:00.512492Z"
        },
        "trusted": true,
        "id": "-cBY1ggf1UgC"
      },
      "outputs": [],
      "source": [
        "best = -float('inf')\n",
        "for epoch in range(epochs):\n",
        "    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n",
        "    best = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noIi3K1X1UgC"
      },
      "source": [
        "# Predictions\n",
        "\n",
        "---\n",
        "\n",
        "## EXERCISE 5:\n",
        "\n",
        "The `collate_batch` function would be used to process batches of data into the suitable format. Here's how you can complete it:\n",
        "\n",
        "1. Initialization:\n",
        "\n",
        "  Lists: Four lists will be initialized to store text sequences (`text_list`), scores (`score_list`), helpfulness scores (`helpfulness_list`), and offsets (`offsets`).\n",
        "  Offsets: The `offsets` list will be initialized with [0].\n",
        "\n",
        "2. Iterate Over Batch:\n",
        "\n",
        "  The function will iterate through each item in the batch. For each item, it will extract `_helpfulness, _score, _text, and id` (though id will not be used further).\n",
        "\n",
        "  - Text Processing:\n",
        "  The `_text` will be processed using `text_pipeline`, which typically converts the text to a list of token indices.\n",
        "  The processed text will then be converted to a tensor and appended to text_list.\n",
        "\n",
        "  - Scores and Helpfulness:\n",
        "  `_score` will be appended to `score_list`.\n",
        "  `_helpfulness` will be appended to `helpfulness_list`.\n",
        "\n",
        "  - Offsets:\n",
        "  Offsets are intended to store the starting index of each sequence in the concatenated batch, but in this function, offsets will not be correctly updated beyond initialization.\n",
        "\n",
        "3. Padding Sequences:\n",
        "\n",
        "  `pad_sequence` will be used to pad the sequences in `text_list` to ensure they all have the same length. This function will pad sequences to the right, using 1 as the padding value.\n",
        "\n",
        "4. Convert to Tensors:\n",
        "\n",
        "  The lists `text_list`, `score_list`, and `helpfulness_list` will be converted to tensors.\n",
        "  The offsets list will also be converted to a tensor, but since offsets will not be updated correctly, it will remain as [0].\n",
        "\n",
        "5. Return Tensors:\n",
        "\n",
        "  The function will return the padded text sequences, offsets, scores, and helpfulness scores as tensors. These tensors will be moved to the specified device (CPU or GPU).\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    ...\n",
        "\n",
        "# Create DataLoader for test data\n",
        "test_dataloader = DataLoader(\n",
        "    test_preprocessed.to_numpy(),  # Convert preprocessed test data to NumPy array\n",
        "    batch_size=128,                # Set batch size to 128\n",
        "    shuffle=False,                 # Do not shuffle data for predictions\n",
        "    collate_fn=collate_batch       # Use collate_batch function to process batches\n",
        ")"
      ],
      "metadata": {
        "id": "dEZkcXQXTkOT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:15:54.262980Z",
          "iopub.status.busy": "2023-09-10T17:15:54.262614Z",
          "iopub.status.idle": "2023-09-10T17:15:54.270985Z",
          "shell.execute_reply": "2023-09-10T17:15:54.269671Z",
          "shell.execute_reply.started": "2023-09-10T17:15:54.262951Z"
        },
        "trusted": true,
        "id": "mGtoqN7P1Ugd"
      },
      "outputs": [],
      "source": [
        "# Define predict function to make predictions\n",
        "def predict(\n",
        "    model,\n",
        "    loader,\n",
        "):\n",
        "    \"\"\"\n",
        "    Predicts labels for data batches using the provided model and DataLoader.\n",
        "\n",
        "    This function iterates over batches in the DataLoader, performs a forward pass\n",
        "    through the model to obtain predicted outputs, and collects these predictions.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): The trained PyTorch model for prediction.\n",
        "    loader (torch.utils.data.DataLoader): DataLoader containing batches of data.\n",
        "\n",
        "    Returns:\n",
        "    list: List of predicted labels for all batches in DataLoader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create tqdm progress bar for prediction loop\n",
        "    loop = tqdm(\n",
        "        enumerate(loader, 1),       # Enumerate over DataLoader with start index 1\n",
        "        total=len(loader),          # Set total number of batches for tqdm\n",
        "        desc=\"Predictions:\",        # Description for tqdm progress bar\n",
        "        leave=True,                 # Leave tqdm progress bar after completion\n",
        "    )\n",
        "    predictions = []                # Initialize empty list to store predictions\n",
        "    with torch.no_grad():\n",
        "        model.eval()                # Set model to evaluation mode (no gradient computation)\n",
        "\n",
        "        # Iterate over batches in DataLoader\n",
        "        for i, batch in loop:\n",
        "            texts, offsets, scores, helpfulness = batch  # Unpack batch data\n",
        "\n",
        "            # Forward pass: compute model outputs\n",
        "            outputs = model(texts, offsets)\n",
        "\n",
        "            # Get predicted labels: argmax along the second dimension (class dimension)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Convert predicted tensor to list and append to predictions\n",
        "            predictions += predicted.detach().cpu().tolist()\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define predict function to make predictions\n",
        "def predict(\n",
        "    model,\n",
        "    loader,\n",
        "):\n",
        "    \"\"\"\n",
        "    Predicts labels for data batches using the provided model and DataLoader.\n",
        "\n",
        "    This function iterates over batches in the DataLoader, performs a forward pass\n",
        "    through the model to obtain predicted outputs, and collects these predictions.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): The trained PyTorch model for prediction.\n",
        "    loader (torch.utils.data.DataLoader): DataLoader containing batches of data.\n",
        "\n",
        "    Returns:\n",
        "    list: List of predicted labels for all batches in DataLoader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create tqdm progress bar for prediction loop\n",
        "    loop = tqdm(\n",
        "        enumerate(loader, 1),       # Enumerate over DataLoader with start index 1\n",
        "        total=len(loader),          # Set total number of batches for tqdm\n",
        "        desc=\"Predictions:\",        # Description for tqdm progress bar\n",
        "        leave=True,                 # Leave tqdm progress bar after completion\n",
        "    )\n",
        "    predictions = []                # Initialize empty list to store predictions\n",
        "    with torch.no_grad():\n",
        "        model.eval()                # Set model to evaluation mode (no gradient computation)\n",
        "\n",
        "        # Iterate over batches in DataLoader\n",
        "        for i, batch in loop:\n",
        "            texts, offsets, scores, helpfulness = batch  # Unpack batch data\n",
        "            # Combine texts and offsets into a single input if your model expects it\n",
        "            input_data = (texts, offsets) # Pack texts and offsets together\n",
        "\n",
        "            # Forward pass: compute model outputs\n",
        "            outputs = model(input_data) # Pass the combined input to the model\n",
        "\n",
        "            # Get predicted labels: argmax along the second dimension (class dimension)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Convert predicted tensor to list and append to predictions\n",
        "            predictions += predicted.detach().cpu().tolist()\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "HNKy_ikH2rGw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(\n",
        "    model,\n",
        "    loader,\n",
        "):\n",
        "    loop = tqdm(\n",
        "        enumerate(loader, 1),\n",
        "        total=len(loader),\n",
        "        desc=\"Predictions:\",\n",
        "        leave=True,\n",
        "    )\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # evaluation mode\n",
        "        for i, batch in loop:\n",
        "            texts, offsets, scores, helpfulness = batch\n",
        "\n",
        "            # forward pass and loss calculation\n",
        "            outputs = model(texts)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predictions += predicted.detach().cpu().tolist()\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "aztGpxay3dTh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:15:54.846205Z",
          "iopub.status.busy": "2023-09-10T17:15:54.845815Z",
          "iopub.status.idle": "2023-09-10T17:15:55.137130Z",
          "shell.execute_reply": "2023-09-10T17:15:55.136138Z",
          "shell.execute_reply.started": "2023-09-10T17:15:54.846179Z"
        },
        "trusted": true,
        "id": "DthaeAYv1Ugd"
      },
      "outputs": [],
      "source": [
        "# Load the best model checkpoint\n",
        "# Load checkpoint from \"best.pt\" file\n",
        "ckpt = torch.load(\"best.pt\")\n",
        "# Update model with loaded state_dict\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "# Make predictions using the model and test DataLoader\n",
        "predictions = predict(model, test_dataloader)  # Get predictions for test data\n",
        "predictions[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-10T17:15:56.288564Z",
          "iopub.status.busy": "2023-09-10T17:15:56.287913Z",
          "iopub.status.idle": "2023-09-10T17:15:56.311894Z",
          "shell.execute_reply": "2023-09-10T17:15:56.310598Z",
          "shell.execute_reply.started": "2023-09-10T17:15:56.288514Z"
        },
        "trusted": true,
        "id": "xRZGoRhr1Ugd"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to corresponding category labels and save to CSV\n",
        "# Map predictions to category labels using idx2cat\n",
        "results = pd.Series(predictions).apply(lambda x: idx2cat[x])\n",
        "\n",
        "# Save results to CSV with 'id' as index label\n",
        "results.to_csv('result.csv', index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vx1ZC1pmrYhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
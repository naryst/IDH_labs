{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "## Lesson 12 Ensemble Learning\n",
    "## Introduction\n",
    "\n",
    "In this lab work, we will explore various ensemble learning techniques. Ensemble learning combines multiple models to improve the overall performance, robustness, and accuracy of predictions.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Learn how to use:\n",
    "1. Bagging\n",
    "2. Random Forests\n",
    "3. AdaBoost\n",
    "\n",
    "### Bagging\n",
    "\n",
    "Bagging, or Bootstrap Aggregating, involves training multiple models on different subsets of the training data and averaging their predictions. This technique helps in reducing variance and preventing overfitting. You will learn how to implement bagging and understand its benefits.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Random Forests are an extension of bagging that uses multiple decision trees. Each tree is trained on a random subset of the data and a random subset of features. This technique improves accuracy and robustness. You will learn how to build and evaluate random forests.\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost, or Adaptive Boosting, combines multiple weak classifiers to create a strong classifier. It adjusts the weights of incorrectly classified instances, focusing more on difficult cases. You will learn how to implement AdaBoost and analyze its effectiveness.\n",
    "\n",
    "In this lab, you will gain practical experience with these ensemble methods and understand how they can be applied to various machine learning problems.\n",
    "\n",
    "\n",
    "#### Why ensemble learning? How does it help?\n",
    "<span style=\"color:blue\"> By combining the power of multiple models in a single\n",
    "model while overcoming their weaknesses, thus reducing variance and/or bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single tree accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "### Exercise 1\n",
    "#### Bagging\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How?\n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 0th sample\n",
      "66 Trees predicted 3\n",
      "34 Trees predicted 8\n",
      "\n",
      "Predictions for the 1th sample\n",
      "100 Trees predicted 8\n",
      "\n",
      "Predictions for the 2th sample\n",
      "100 Trees predicted 2\n",
      "\n",
      "Predictions for the 3th sample\n",
      "100 Trees predicted 6\n",
      "\n",
      "Predictions for the 4th sample\n",
      "100 Trees predicted 6\n",
      "\n",
      "Bagging accuracy: 0.8787878787878788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Predictions for the {i}th sample')\n",
    "    sample_pred, sample_pred_count = np.unique(base_pred[i], return_counts=True)\n",
    "    for j in range(len(sample_pred)):\n",
    "        print(sample_pred_count[j],'Trees predicted', sample_pred[j])\n",
    "    print()\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now the accuracy grew up to **0.88**. Also, you can see that our classifiers\n",
    "return very similar results.\n",
    "\n",
    "Let's compare our bagging to SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747474747474747"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question:\n",
    "### Why our performance is much lower then sklearn? \n",
    "<span style=\"color:blue\">\n",
    "Because the base classifiers are identical. The only difference might be when\n",
    "the improvement of the criterion is identical for several splits and one split\n",
    "has to be selected at random.\n",
    "</span>\n",
    "\n",
    "---\n",
    "### Exercise 2\n",
    "#### Random forest\n",
    "\n",
    "Compared to the simple bagging we've just implemented, random forest can show\n",
    "better results because base classifiers are much less correlated.\n",
    "\n",
    "At first, let's implement bootstrap sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [ 3,  4,  5],\n",
       "        [ 0,  1,  2]]),\n",
       " array([0, 3, 1, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y):\n",
    "    # generate bootstrap indices and return data according to them\n",
    "    ind = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "    return X[ind,:], y[ind]\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "\n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap\n",
    "sampling from X and $\\sqrt d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    base = DecisionTreeClassifier(max_features=\"sqrt\", random_state=42)\n",
    "    bs_X, bs_y = bootstrap(X_train, y_train)\n",
    "    base.fit(bs_X, bs_y)\n",
    "    classifiers.append(base)\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now we got **0.98** accuracy, which is a significant improvement! Now you\n",
    "can see why it is so important to have diverse classifiers.\n",
    "\n",
    "---\n",
    "### Exercise 3\n",
    "#### Boosting\n",
    "\n",
    "How does boosting work? <span style=\"color:blue\"> Models are built sequentially:\n",
    "each model is built using information from previously built models. Boosting\n",
    "does not involve bootstrap sampling; instead each tree is fit on a modified\n",
    "version of the original data set. </span>\n",
    "\n",
    "For simplicity let's solve a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "data['target'][data['target']==0] = -1  # turn 0s to -1 to make the calculations easier\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_trees = 3\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "# initialize sample weights\n",
    "sample_weights = np.ones(train_samples) / train_samples\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3, random_state=42)\n",
    "    clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train, pred, sample_weight=sample_weights)\n",
    "    # Calculate tree weight\n",
    "    w = 0.5 * np.log(acc / (1 - acc))\n",
    "    tree_weights[i] = w\n",
    "    classifiers.append(clf)\n",
    "    # Update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y[j]:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp(w)\n",
    "        else:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp((-w))\n",
    "    # Normalize the weights\n",
    "    sample_weights = sample_weights / np.sum(sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "# Aggregate the  predictions\n",
    "for t in range(n_trees):\n",
    "    pred += classifiers[t].predict(X_test) * tree_weights[t]\n",
    "for i in range(n_test):\n",
    "    pred[i] = 1 if pred[i] > 0 else -1\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Boosting accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n",
    "Sklearn has many ensemble [methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print('AdaBoost accuracy:', clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "In this lab work, we explored different methods for building ensemble models in machine learning. We started with the Bagging method, which uses bootstrap to create different subsamples of data and aggregate the results, which helps improve the generalizability of the model. \n",
    "\n",
    "We then looked at Random Forests, which combines multiple decision trees to improve the accuracy and robustness of the model. \n",
    "\n",
    "Finally, we examined AdaBoost, which consistently trains weak models on weighted subsets of data, focusing on those samples that present the greatest difficulty for previous models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "6df0ddd77085922c773681b1c23afa6ec355a7eb5a25c833f534ec75c0111436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

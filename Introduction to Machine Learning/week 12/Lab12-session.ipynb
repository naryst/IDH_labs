{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3RN7cPfAQWIh"
      },
      "source": [
        "# Introduction to Machine Learning\n",
        "## Lesson 12 Ensemble Learning\n",
        "## Introduction\n",
        "\n",
        "In this lab work, we will explore various ensemble learning techniques. Ensemble learning combines multiple models to improve the overall performance, robustness, and accuracy of predictions.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "Learn how to use:\n",
        "1. Bagging\n",
        "2. Random Forests\n",
        "3. AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging\n",
        "\n",
        "Bagging, or Bootstrap Aggregating, involves training multiple models on different subsets of the training data and averaging their predictions. This technique helps in reducing variance and preventing overfitting. You will learn how to implement bagging and understand its benefits.\n",
        "\n",
        "#### Theory and Visualization\n",
        "\n",
        "Bagging generates multiple subsets of the training dataset using bootstrapping (random sampling with replacement). Each subset is used to train a separate model, typically a decision tree. The final prediction is made by averaging the predictions (for regression) or by majority voting (for classification).\n",
        "\n",
        "- **Reducing Variance**: By training on different subsets of the data, bagging helps to reduce the variance of the model, which means it reduces the model's sensitivity to the noise in the training data.\n",
        "\n",
        "**Example**: If we train several decision trees on different subsets of the training data and combine their predictions, we can achieve a more robust and accurate model.\n",
        "\n",
        "**Visualization**:"
      ],
      "metadata": {
        "id": "qwN9sliwXrzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Example code to implement Bagging\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "bagging_model = BaggingClassifier(estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "predictions = bagging_model.predict(X_test)\n",
        "print(f'Bagging Accuracy: {accuracy_score(y_test, predictions):.2f}')"
      ],
      "metadata": {
        "id": "W4qxYt7OYCLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests\n",
        "Random Forests are an extension of bagging that uses multiple decision trees. Each tree is trained on a random subset of the data and a random subset of features. This technique improves accuracy and robustness. You will learn how to build and evaluate random forests.\n",
        "\n",
        "#### Theory and Visualization\n",
        "**Random Forests** address both the variance and the bias of the model by introducing randomness in the feature selection process. Each tree in the forest is trained on a bootstrap sample of the data and at each split in the tree, a random subset of features is considered.\n",
        "\n",
        "- **Reducing Overfitting**: By averaging the results of multiple decision trees, random forests reduce the risk of overfitting that is common with individual decision trees.\n",
        "Improving Accuracy: The combination of multiple trees results in a model that is more accurate and robust than individual trees.\n",
        "Example: A random forest can be used for classification or regression tasks, providing reliable and accurate predictions.\n",
        "- **Improving Accuracy**: The combination of multiple trees results in a model that is more accurate and robust than individual trees.\n",
        "\n",
        "**Example**: A random forest can be used for classification or regression tasks, providing reliable and accurate predictions.\n",
        "\n",
        "**Visualization**:"
      ],
      "metadata": {
        "id": "hSenqZLOYEEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Example code to implement Random Forest\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "predictions = random_forest_model.predict(X_test)\n",
        "print(f'Random Forest Accuracy: {accuracy_score(y_test, predictions):.2f}')\n"
      ],
      "metadata": {
        "id": "Eo-JNjhUYGcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost\n",
        "**AdaBoost**, or **Adaptive Boosting**, combines multiple weak classifiers to create a strong classifier. It adjusts the weights of incorrectly classified instances, focusing more on difficult cases. You will learn how to implement AdaBoost and analyze its effectiveness.\n",
        "\n",
        "#### Theory and Visualization\n",
        "**AdaBoost** sequentially applies weak classifiers (like decision stumps) to re-weighted versions of the training data. Misclassified instances get higher weights, forcing subsequent classifiers to focus on the harder cases.\n",
        "\n",
        "- **Boosting Performance**: By focusing on misclassified instances, AdaBoost can significantly improve the performance of weak classifiers.\n",
        "- **Reducing Bias**: By combining the strengths of multiple weak classifiers, AdaBoost helps to reduce the overall bias of the model.\n",
        "\n",
        "**Example**: AdaBoost can be particularly effective for binary classification problems, but it can also be extended to multi-class problems.\n",
        "\n",
        "**Visualization**:"
      ],
      "metadata": {
        "id": "ermh4XfbYIMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Example code to implement AdaBoost\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "adaboost_model = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "predictions = adaboost_model.predict(X_test)\n",
        "print(f'AdaBoost Accuracy: {accuracy_score(y_test, predictions):.2f}')\n"
      ],
      "metadata": {
        "id": "y0UvkEyGYKg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why ensemble learning? How does it help?"
      ],
      "metadata": {
        "id": "4BYKFTp5X6rU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jfMqIWYaQWIj"
      },
      "source": [
        "## Ensemble learning\n",
        "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
        "\n",
        "Let's start from uploading MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fc6bXhn7QWIk"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
        "\n",
        "\n",
        "plt.figure(1, figsize=(3, 3))\n",
        "plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title(f\"label is {y[0]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3tX_DZgOQWIl"
      },
      "source": [
        "### Single decision tree\n",
        "\n",
        "First, we train a single decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DuNZBKtAQWIl"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "pred = tree.predict(X_test)\n",
        "tree_score = accuracy_score(y_test, pred)\n",
        "print(\"Single tree accuracy:\", tree_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "QD5MpAa_QWIl"
      },
      "source": [
        "Note the accuracy - it is around **0.85**.\n",
        "\n",
        "### Exercise 1\n",
        "#### Bagging\n",
        "\n",
        "What is decreased by bagging? Variance or bias? How?\n",
        "\n",
        "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TDvIQ145QWIl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "n_trees = 100\n",
        "\n",
        "classifiers = []\n",
        "for i in range(n_trees):\n",
        "    # train a new classifier and append it to the list\n",
        "    tree = ...\n",
        "    classifiers.append(tree)\n",
        "\n",
        "# here we will store predictions for all samples and all base classifiers\n",
        "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
        "for i in range(n_trees):\n",
        "    # obtain the predictions from each tree\n",
        "    ...\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'Predictions for the {i}th sample')\n",
        "    sample_pred, sample_pred_count = np.unique(base_pred[i], return_counts=True)\n",
        "    for j in range(len(sample_pred)):\n",
        "        print(sample_pred_count[j],'Trees predicted', sample_pred[j])\n",
        "    print()\n",
        "\n",
        "# aggregate predictions by majority voting\n",
        "pred = ...\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(\"Bagging accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b9E8y0_zQWIl"
      },
      "source": [
        "Now the accuracy grew up to **0.88**. Also, you can see that our classifiers\n",
        "return very similar results.\n",
        "\n",
        "Let's compare our bagging to SKlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9LLUJrNFQWIl"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gWAB85jUQWIl"
      },
      "source": [
        "## Question:\n",
        "### Why our performance is much lower then sklearn?\n",
        "---\n",
        "### Exercise 2\n",
        "#### Random forest\n",
        "\n",
        "Compared to the simple bagging we've just implemented, random forest can show\n",
        "better results because base classifiers are much less correlated.\n",
        "\n",
        "At first, let's implement bootstrap sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7SRl3V2vQWIm"
      },
      "outputs": [],
      "source": [
        "def bootstrap(X, y):\n",
        "    # generate bootstrap indices and return data according to them\n",
        "    ind = ...\n",
        "    return X[ind,:], y[ind]\n",
        "\n",
        "\n",
        "# this is a test, will work if you are using np.random.randint() for indices generation\n",
        "np.random.seed(0)\n",
        "a = np.array(range(12)).reshape(4,3)\n",
        "b = np.array(range(4))\n",
        "bootstrap(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gBUxipqHQWIm"
      },
      "source": [
        "You should get\n",
        "\n",
        "(array([[ 0,  1,  2], <br>\n",
        "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
        "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
        "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
        "array([0, 3, 1, 0]))\n",
        "\n",
        "Now let's build a set of decision trees, each of them is trained on a bootstrap\n",
        "sampling from X and $\\sqrt d$ features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SrobG8ocQWIm"
      },
      "outputs": [],
      "source": [
        "classifiers = []\n",
        "for i in range(n_trees):\n",
        "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
        "    base =...\n",
        "    classifiers.append(base)\n",
        "\n",
        "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
        "for i in range(n_trees):\n",
        "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
        "\n",
        "pred = mode(base_pred, axis=0)[0].ravel()\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(\"Random forest accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "GZOu8PA1QWIm"
      },
      "source": [
        "And now we got **0.98** accuracy, which is a significant improvement! Now you\n",
        "can see why it is so important to have diverse classifiers.\n",
        "\n",
        "---\n",
        "### Exercise 3\n",
        "#### Boosting\n",
        "\n",
        "#### Question\n",
        "**How does boosting work?**  \n",
        "\n",
        "---\n",
        "\n",
        "For simplicity let's solve a binary classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eCieO0lkQWIm"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "data['target'][data['target']==0] = -1  # turn 0s to -1 to make the calculations easier\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xIRHV_aGQWIm"
      },
      "source": [
        "Now let's train a boosting model.\n",
        "\n",
        "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
        "\n",
        "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
        "\n",
        "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
        "\n",
        "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cVq88au-QWIm"
      },
      "outputs": [],
      "source": [
        "n_trees = 3\n",
        "tree_weights = np.zeros(n_trees)\n",
        "classifiers = []\n",
        "train_samples = X_train.shape[0]\n",
        "# initialize sample weights\n",
        "sample_weights = np.ones(train_samples) / train_samples\n",
        "for i in range(n_trees):\n",
        "    clf = DecisionTreeClassifier(min_samples_leaf=3, random_state=42)\n",
        "    clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "    pred = clf.predict(X_train)\n",
        "    acc = accuracy_score(y_train, pred, sample_weight=sample_weights)\n",
        "    # Calculate tree weight\n",
        "    w = ...\n",
        "    tree_weights[i] = w\n",
        "    classifiers.append(clf)\n",
        "    # Update sample weights\n",
        "    for j in range(train_samples):\n",
        "        if pred[j] != y[j]:\n",
        "            sample_weights[j] = sample_weights[j] * np.exp(w)\n",
        "        else:\n",
        "            sample_weights[j] = sample_weights[j] * np.exp((-w))\n",
        "    # normalize the weights\n",
        "    sample_weights = sample_weights / np.sum(sample_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jyeTGrm1QWIn"
      },
      "source": [
        "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
        "\n",
        "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1j17iPOvQWIn"
      },
      "outputs": [],
      "source": [
        "n_test = X_test.shape[0]\n",
        "\n",
        "pred = np.zeros(n_test)\n",
        "\n",
        "# Aggregate the  predictions\n",
        "...\n",
        "\n",
        "for i in range(n_test):\n",
        "    pred[i] = 1 if pred[i] > 0 else -1\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(\"Boosting accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "te_N2ZhwQWIn"
      },
      "source": [
        "# [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n",
        "Sklearn has many ensemble [methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d_QTT-c6QWIn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=100\n",
        ")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "print('AdaBoost accuracy:', clf.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P1HuaKNsQWIn"
      },
      "source": [
        "## Conclusion\n",
        "In this lab work, we explored different methods for building ensemble models in machine learning. We started with the Bagging method, which uses bootstrap to create different subsamples of data and aggregate the results, which helps improve the generalizability of the model.\n",
        "\n",
        "We then looked at Random Forests, which combines multiple decision trees to improve the accuracy and robustness of the model.\n",
        "\n",
        "Finally, we examined AdaBoost, which consistently trains weak models on weighted subsets of data, focusing on those samples that present the greatest difficulty for previous models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kClajn3ZQWIn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "6df0ddd77085922c773681b1c23afa6ec355a7eb5a25c833f534ec75c0111436"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
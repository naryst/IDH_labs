{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ84QHlub057"
      },
      "source": [
        "# Introduction to Machine Learning\n",
        "## Lesson 10 Advanced training of Neural Networks in Pytorch\n",
        "## Introduction\n",
        "\n",
        "In this lab work, we will explore advanced techniques for training neural networks using PyTorch. These methods aim to enhance the efficiency, reliability, and speed of training neural networks.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "Learn how to use:\n",
        "1. Data augmentation\n",
        "2. Batch normalization\n",
        "3. Dropout\n",
        "4. Early stopping\n",
        "5. Learning rate scheduling\n",
        "6. TensorBoard for training control\n",
        "\n",
        "### Data Augmentation\n",
        "\n",
        "Data augmentation involves creating new training examples by transforming the existing data, which helps improve the model's generalization. You will learn how to apply various data augmentation techniques to your dataset.\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "Batch normalization standardizes the inputs to a layer for each mini-batch. This technique helps in speeding up the training process and improving the model's performance. You will implement batch normalization in your neural network layers.\n",
        "\n",
        "### Dropout\n",
        "\n",
        "Dropout is a regularization technique that helps prevent overfitting by randomly dropping units during training. You will learn how to apply dropout in your neural network.\n",
        "\n",
        "### Early Stopping\n",
        "\n",
        "Early stopping monitors the model's performance on a validation set and stops training when performance starts to degrade. This technique helps in preventing overfitting. You will implement early stopping in your training process.\n",
        "\n",
        "### Learning Rate Scheduling\n",
        "\n",
        "Learning rate scheduling adjusts the learning rate during training to improve convergence. You will explore different learning rate scheduling strategies and apply them to your training process.\n",
        "\n",
        "### TensorBoard for Training Control\n",
        "\n",
        "TensorBoard provides a suite of visualization tools to monitor and debug your training process. You will learn how to use TensorBoard to track metrics such as loss and accuracy, visualize the computational graph, and more.\n",
        "\n",
        "\n",
        "### Dropout\n",
        "\n",
        "#### Purpose and Idea\n",
        "\n",
        "Dropout is a regularization technique to prevent overfitting in neural networks. During training, dropout randomly sets a fraction of the input units to 0 at each update step. This helps to break the reliance on specific neurons, ensuring that the network learns more robust features.\n",
        "\n",
        "#### Formula\n",
        "\n",
        "Given a layer's output $\\mathbf{h}$, dropout can be applied as follows:\n",
        "1. Generate a random binary mask $\\mathbf{m}$ where each element is 0 with probability $p$ and 1 with probability $1 - p$.\n",
        "2. Apply the mask to the output: $\\mathbf{\\tilde{h}} = \\mathbf{h} \\odot \\mathbf{m}$.\n",
        "\n",
        "During inference, we scale the activations by $(1 - p)$ to maintain the same expected value:\n",
        "$$\n",
        "\\mathbf{\\hat{h}} = (1 - p) \\mathbf{h}\n",
        "$$\n",
        "\n",
        "\n",
        "#### Visualization\n",
        "\n",
        "Imagine a neural network layer with 5 neurons. During training with dropout, some neurons are randomly \"dropped out.\"\n",
        "\n",
        "```plaintext\n",
        "Before Dropout:        After Dropout (p = 0.4):\n",
        "\n",
        " o   o   o   o   o      o   o   o   x   o\n",
        " |   |   |   |   |      |   |   |       |\n",
        " o   o   o   o   o      o   x   o   x   o\n",
        "```\n",
        "\n",
        "#### Gain\n",
        "\n",
        "Dropout helps in:\n",
        "- Reducing overfitting.\n",
        "- Making the network more robust by ensuring it doesn't rely on specific neurons.\n",
        "- Promoting independence among feature detectors.\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "#### Purpose and Idea\n",
        "\n",
        "Batch Normalization (BatchNorm) is a technique to improve the training of deep neural networks by normalizing the inputs of each layer. It helps in speeding up the training and provides some regularization, reducing the need for dropout.\n",
        "\n",
        "#### Formula\n",
        "\n",
        "For a given mini-batch, batch normalization is applied as follows:\n",
        "1. Compute the mean $\\mu_B$ and variance $\\sigma_B^2$ of the mini-batch.\n",
        "2. Normalize the batch: $\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
        "3. Scale and shift: $y_i = \\gamma \\hat{x}_i + \\beta$\n",
        "\n",
        "Here, $\\gamma$ and $\\beta$ are learnable parameters, and $\\epsilon$ is a small constant to avoid division by zero.\n",
        "\n",
        "\n",
        "#### Visualization\n",
        "\n",
        "Consider the activations of a neural network layer before and after applying batch normalization.\n",
        "\n",
        "```plaintext\n",
        "Before BatchNorm:     After BatchNorm:\n",
        "\n",
        " |    |    |    |      |    |    |    |\n",
        " o    o    o    o      o    o    o    o\n",
        " |    |    |    |      |    |    |    |\n",
        " o    o    o    o      o    o    o    o\n",
        "```\n",
        "\n",
        "BatchNorm ensures that the activations are centered and scaled, which helps in stabilizing and accelerating the training process.\n",
        "\n",
        "#### Gain\n",
        "\n",
        "Batch Normalization helps in:\n",
        "- Reducing internal covariate shift, leading to faster training.\n",
        "- Allowing the use of higher learning rates.\n",
        "- Providing regularization, reducing the need for other regularization techniques like dropout.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyAmrz7Hb06D",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 1) Data augmentation\n",
        "We're going to use a new dataset, CIFAR10, as our example for the task of classification. The data set consists of 60000 32x32 color images in 10 classes looking like this:\n",
        "\n",
        "![cifar10.png](attachment:77959831-c494-4dd5-9742-4a305a294031.png)\n",
        "\n",
        "**Question: why might we use data augmentation? What problem does it solve?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF8mdyfGKvdU"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "**1) Write the following augmentation transforms of CIFAR10 dataset:**\n",
        "\n",
        "    - Crop (with size = 32, crop = 2)\n",
        "    - Horizontal flip (with probability 0.5)\n",
        "    - Rotation (with 10 degrees max)\n",
        "    - Random affine (degrees = 0, shear = 10, scale=(0.8,1.2))\n",
        "    \n",
        "**2) Define DataLoader-s of CIFAR-10 with these transforms**\n",
        "\n",
        "**Hint:** refer [ILLUSTRATION OF TRANSFORMS](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py) in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K1MNlrdb06D",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# You can increase these values if you've enough computational power\n",
        "train_batch_size = 128\n",
        "test_batch_size = 128\n",
        "\n",
        "\n",
        "# Put augmentations\n",
        "train_transforms = transforms.Compose([\n",
        "    ...,\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "# Do not modify test transforms, because it will corrupt test data\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "# You should get how to get the training part of datasets.CIFAR10 and\n",
        "# how to apply train_transforms here\n",
        "# Specify path of downloaded set in root, if you've loaded it\n",
        "train_dataset = datasets.CIFAR10(root='cifar10', ...)\n",
        "\n",
        "# Define the loader of train data set: use train_batch_size as batch_size\n",
        "# and don't forget to shuffle\n",
        "train_data_loader = data.DataLoader(...)\n",
        "\n",
        "\n",
        "# Do the same for test part of CIFAR10, apply test_transforms\n",
        "test_dataset = datasets.CIFAR10(root='cifar10', ...)\n",
        "\n",
        "# Define the loader of test data set: use test_batch_size\n",
        "test_data_loader = data.DataLoader(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q60fmrGCKvdU"
      },
      "outputs": [],
      "source": [
        "# Check the results of transformations\n",
        "import matplotlib.pyplot as plt\n",
        "images, _ = next(iter(train_data_loader))\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=4)\n",
        "\n",
        "for i in range(4):\n",
        "    ax = axs[i]\n",
        "    ax.imshow(images[i].numpy().transpose(1,2,0))\n",
        "    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test cases:\n",
        "import unittest\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Определяем функции для создания датасетов и DataLoader-ов\n",
        "def get_train_transforms():\n",
        "    return train_transforms;\n",
        "def get_test_transforms():\n",
        "    return test_transforms;\n",
        "\n",
        "def get_train_dataset():\n",
        "    return train_dataset;\n",
        "\n",
        "def get_test_dataset():\n",
        "    return test_dataset;\n",
        "\n",
        "def get_train_data_loader():\n",
        "    return train_data_loader;\n",
        "\n",
        "def get_test_data_loader():\n",
        "    return test_data_loader;\n",
        "\n",
        "class TestCIFAR10DataLoading(unittest.TestCase):\n",
        "\n",
        "    def test_train_dataset_loads_correctly(self):\n",
        "        train_dataset = get_train_dataset()\n",
        "        self.assertEqual(len(train_dataset), 50000, \"Train dataset should have 50000 samples.\")\n",
        "\n",
        "    def test_test_dataset_loads_correctly(self):\n",
        "        test_dataset = get_test_dataset()\n",
        "        self.assertEqual(len(test_dataset), 10000, \"Test dataset should have 10000 samples.\")\n",
        "\n",
        "    def test_train_batch_size(self):\n",
        "        train_data_loader = get_train_data_loader()\n",
        "        images, labels = next(iter(train_data_loader))\n",
        "        self.assertEqual(images.size(0), 128, \"Train batch size should be 128\")\n",
        "        self.assertEqual(labels.size(0), 128, \"Train batch size should be 128\")\n",
        "\n",
        "    def test_test_batch_size(self):\n",
        "        test_data_loader = get_test_data_loader()\n",
        "        images, labels = next(iter(test_data_loader))\n",
        "        self.assertEqual(images.size(0), 128, \"Test batch size should be 128\")\n",
        "        self.assertEqual(labels.size(0), 128, \"Test batch size should be 128\")\n",
        "\n",
        "    def test_data_types_and_normalization(self):\n",
        "        train_data_loader = get_train_data_loader()\n",
        "        mean = torch.tensor([0.5, 0.5, 0.5])\n",
        "        std = torch.tensor([0.5, 0.5, 0.5])\n",
        "        images, _ = next(iter(train_data_loader))\n",
        "        self.assertEqual(images.dtype, torch.float32, \"Images should be of type torch.float32\")\n",
        "\n",
        "\n",
        "    def test_data_randomization(self):\n",
        "        train_data_loader = get_train_data_loader()\n",
        "        first_iter_images, _ = next(iter(train_data_loader))\n",
        "        second_iter_images, _ = next(iter(train_data_loader))\n",
        "        self.assertFalse(torch.equal(first_iter_images, second_iter_images), \"Data should be randomized across batches\")\n",
        "\n",
        "# Run\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "metadata": {
        "id": "tLtuHDP6Lm2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH8Ovp-Sb06E"
      },
      "source": [
        "# 2) Build a model with dropout and batch normalization\n",
        "\n",
        "Here we're going to build about the same model as we used before, but with two new layers: batch normalization and dropout.\n",
        "\n",
        "#### Question: what's the purpose of these operations? What's the proposed order of their disposition relative to other layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSVJA0kBKvdV"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "**Declare 4 blocks (nn.Sequential) of Custom model with the default parameters (unless otherwise stated):**\n",
        "\n",
        "**1st block, convolutional)**\n",
        "\n",
        "    - Convolution layer with 16 filters, kernel size equal to 3x3 and stride 1x1. Use ReLU as activation;\n",
        "    - Max pool layer with kernel size 2;\n",
        "    - Batch norm layer.\n",
        "    \n",
        "**2nd block, convolutional)**\n",
        "\n",
        "    - Convolution layer with 32 filters, kernel size equal to 3x3 and stride 1x1. Use ReLU as activation;\n",
        "    - Batch norm layer;\n",
        "    - Dropout layer with probability of unit drop equal to 0.25.\n",
        "    \n",
        "**3rd block, convolutional)**\n",
        "\n",
        "    - Convolution layer with 64 filters, kernel size equal to 3x3 and stride 1x1. Use ReLU as activation;\n",
        "    - Batch norm layer;\n",
        "\n",
        "**4th block, linear)**\n",
        "\n",
        "    - Linear layer. If you stated the previous parameters properly, in_features should be 64*11*11. Set out_features as 256 and ReLU as activation;\n",
        "    - Dropout layer with probability of unit drop equal to 0.1;\n",
        "    - Final linear layer with size of output equals 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9fWUDdub06E",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        # Build your model\n",
        "        self.conv1 = nn.Sequential(...)\n",
        "        self.conv2 = nn.Sequential(...)\n",
        "        self.conv3 = nn.Sequential(...)\n",
        "        self.linear1 = nn.Sequential(...)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Propagate x through the network\n",
        "        # Do not forget to flatten after the 3rd block\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = CustomModel().to(device)\n",
        "\n",
        "print(f'Device: {device}')\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM_0vfc9b06F",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 3) Training pipeline upgrades: early stopping and LR scheduler\n",
        "\n",
        "Early Stopping is a form of regularization, used to stop training when a monitored metric has stopped improving.\n",
        "\n",
        "\n",
        "#### Question: what kind of metric can we monitor? What's the benefit of using early stopping?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## for tests\n",
        "!pip install torchsummary"
      ],
      "metadata": {
        "id": "XSzya4lSLuQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "class TestCustomModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.model = CustomModel().to(device)\n",
        "        self.device = device\n",
        "        self.input_shape = (3, 32, 32)  # CIFAR-10 image dimensions\n",
        "\n",
        "    def test_model_initialization(self):\n",
        "        self.assertIsInstance(self.model, CustomModel, \"Model should be an instance of CustomModel\")\n",
        "\n",
        "    def test_model_forward_pass(self):\n",
        "        # Create a random input tensor with batch size 1\n",
        "        input_tensor = torch.randn(1, *self.input_shape).to(self.device)\n",
        "        output = self.model(input_tensor)\n",
        "\n",
        "        # Check if the output shape is correct (batch size, number of classes)\n",
        "        self.assertEqual(output.shape, (1, 10), f\"Output shape should be (1, 10) but got {output.shape}\")\n",
        "\n",
        "    def test_model_summary(self):\n",
        "        try:\n",
        "            summary(self.model, input_size=self.input_shape)\n",
        "        except Exception as e:\n",
        "            self.fail(f\"Model summary failed with exception: {e}\")\n",
        "\n",
        "# Run\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "metadata": {
        "id": "cu6eJVQzLz5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkVdOZgOKvdV"
      },
      "source": [
        "Vanila Pytorch doesn't contain early stopping (check [Pytorch Ignite](https://pytorch.org/ignite/generated/ignite.handlers.early_stopping.EarlyStopping.html) for 'official' solution), so we have to write it it from scratch. Although, sometimes it's useful to have such a custom tool which you can tune to your specific needs.\n",
        "\n",
        "### Exercise 3\n",
        "\n",
        "**Implement EarlyStopping class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SV3TQc7b06F",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Fill this class to stop when a certain value stop improving\n",
        "import operator\n",
        "class EarlyStopping():\n",
        "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
        "        '''\n",
        "        :param tolerance: number of epochs that the metric doesn't improve\n",
        "        :param min_delta: minimum improvement\n",
        "        :param mode: 'min' or 'max' to minimize or maximize the metric\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        You should keep these parameters,\n",
        "        define a counter of __call__ falses and the previous best value of metric\n",
        "        '''\n",
        "        self.early_stop = False\n",
        "        pass\n",
        "\n",
        "    def __call__(self, metric)->bool:\n",
        "        ''' This function should return True if `metric` is not improving for\n",
        "            'tolerance' calls\n",
        "        '''\n",
        "        if ...:\n",
        "            self.early_stop = True\n",
        "        return self.early_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQmDw-iFKvdW"
      },
      "source": [
        "\n",
        "### Let's look how different LR-schedulers work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx4OhhvEKvdW"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Just a toy model\n",
        "class NullModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(1,1)\n",
        "\n",
        "\n",
        "toy_model = NullModule()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def plot_lr(scheduler, name):\n",
        "    # Re-init for each scheduler\n",
        "    optimizer.param_groups[0]['lr'] = 0.01\n",
        "    optimizer.zero_grad()\n",
        "    toy_model.zero_grad()\n",
        "    lrs = []\n",
        "    step = 100\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set(xlabel='Step', ylabel='LR value', title=name)\n",
        "\n",
        "    for i in range(step):\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        if name == \"ReduceLROnPlateau\":\n",
        "            scheduler.step(lr)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "        lrs.append(lr)\n",
        "\n",
        "    ax.plot(lrs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# You can check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "LRs = {\"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3,\n",
        "                                                           patience=10, verbose=True,min_lr=0.001),\n",
        "       \"Step LR\": lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5),\n",
        "       \"Exponent LR\": lr_scheduler.ExponentialLR(optimizer, gamma=0.9),\n",
        "       \"Cyclic LR\":lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.2,\n",
        "                                         cycle_momentum=False, step_size_up=10)}\n",
        "\n",
        "for name, lr in LRs.items():\n",
        "    plot_lr(lr, name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz8gdAFKb06F"
      },
      "source": [
        "# 4) Gather all together in training loops\n",
        "\n",
        "This implementation of training and testing loops for a PyTorch model, designed to streamline the training process and evaluate model performance.\n",
        "\n",
        "The `train` function handles the model's training phase by iterating over the training dataset, computing the loss using a given criterion, performing backpropagation, and updating model parameters with an optimizer. It also tracks and reports the training loss and accuracy for each epoch.\n",
        "\n",
        "The `test` function evaluates the trained model on a separate test dataset, calculating the test loss and accuracy without performing backpropagation. These functions leverage GPU acceleration if available and provide real-time progress updates using the `tqdm` library for better monitoring of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc9nKg7Pb06F",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time()\n",
        "    correct = 0\n",
        "    iteration = 0\n",
        "\n",
        "    bar = tqdm(train_loader)\n",
        "    for data, target in bar:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        # Get the index of the max log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        iteration += 1\n",
        "        bar.set_postfix({\"Loss\": format(epoch_loss/iteration, '.6f')})\n",
        "\n",
        "    acc = 100. * correct / len(train_loader.dataset)\n",
        "    print(f'\\rTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
        "    return epoch_loss, acc\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8LFDl-gKvdW"
      },
      "source": [
        "This code snippet sets up the training configuration for a PyTorch model, including the definition of hyperparameters, loss function, optimizer, learning rate scheduler, and early stopping mechanism.\n",
        "\n",
        "The `epochs` variable specifies the number of training iterations.\n",
        "\n",
        "The `criterion` is defined as CrossEntropyLoss, which is suitable for classification tasks.\n",
        "\n",
        "The optimizer is set to Stochastic Gradient Descent (SGD) with a learning rate of 0.1 and a momentum of 0.9, enhancing convergence speed and stability.\n",
        "\n",
        "The learning rate scheduler, `ReduceLROnPlateau`, reduces the learning rate by a factor of 0.3 if the monitored metric does not improve for 3 consecutive epochs, with a minimum learning rate of 0.001.\n",
        "\n",
        "The early stopping mechanism halts training if the validation loss does not improve for 7 epochs, preventing overfitting. Finally, `best_model_wts` stores a deep copy of the model's initial state, ensuring the best model weights can be restored after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GLmmhQWJKvdW"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from copy import deepcopy\n",
        "\n",
        "# Define hyperparams\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# Choose the LR you like\n",
        "scheduler = ...\n",
        "early_stopping = EarlyStopping(tolerance=7, mode='min')\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TestEarlyStopping(unittest.TestCase):\n",
        "\n",
        "    def test_initialization(self):\n",
        "        early_stopping = EarlyStopping(tolerance=5, min_delta=0, mode='min')\n",
        "        self.assertEqual(early_stopping.tolerance, 5)\n",
        "        self.assertEqual(early_stopping.min_delta, 0)\n",
        "        self.assertEqual(early_stopping.mode, 'min')\n",
        "        self.assertEqual(early_stopping.counter, 0)\n",
        "        self.assertEqual(early_stopping.early_stop, False)\n",
        "        self.assertEqual(early_stopping.prev_metric, np.inf)\n",
        "\n",
        "    def test_call_method_min_mode(self):\n",
        "        early_stopping = EarlyStopping(tolerance=3, min_delta=0, mode='min')\n",
        "        metrics = [1.0, 0.9, 0.8, 0.8, 0.8, 0.8]\n",
        "        results = [early_stopping(metric) for metric in metrics]\n",
        "        self.assertEqual(results, [False, False, False, False, False, False])\n",
        "\n",
        "    def test_call_method_max_mode(self):\n",
        "        early_stopping = EarlyStopping(tolerance=3, min_delta=0, mode='max')\n",
        "        metrics = [1.0, 1.1, 1.2, 1.2, 1.2, 1.2]\n",
        "        results = [early_stopping(metric) for metric in metrics]\n",
        "        self.assertEqual(results, [False, False, False, False, False, False])\n",
        "\n",
        "class TestTrainingFunctions(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = CustomModel().to(self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "        # Create a small dataset for testing\n",
        "        data = torch.randn(100, 3, 32, 32)\n",
        "        targets = torch.randint(0, 10, (100,))\n",
        "        dataset = TensorDataset(data, targets)\n",
        "        self.loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "    def test_train_function(self):\n",
        "        epoch_loss, acc = train(self.model, self.device, self.loader, self.criterion, self.optimizer, epoch=1)\n",
        "        self.assertIsInstance(epoch_loss, float)\n",
        "        self.assertIsInstance(acc, float)\n",
        "\n",
        "    def test_test_function(self):\n",
        "        test_loss, acc = test(self.model, self.device, self.loader, self.criterion)\n",
        "        self.assertIsInstance(test_loss, float)\n",
        "        self.assertIsInstance(acc, float)\n",
        "\n",
        "# Run\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "metadata": {
        "id": "_MF16LefL8x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFXLGk7sKvdW"
      },
      "source": [
        "# 5) Use TensorBoard to check the progress of learning\n",
        "\n",
        "\n",
        "This code defines a `training` function to train and evaluate a PyTorch model. If `writing` is `True`, it logs metrics to TensorBoard. The function iterates over epochs, updating model weights using a specified optimizer and learning rate scheduler. It calculates training and test losses and accuracies. Early stopping halts training if the test loss stops improving. The best model weights are saved if test accuracy improves. Finally, the model's state is saved to disk. This setup ensures efficient model training, evaluation, and optional logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJwFwepob06G",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "def training(writing=False):\n",
        "    if writing:\n",
        "        writer = SummaryWriter(log_dir='runs/model')\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train(model, device, train_data_loader, criterion, optimizer, epoch)\n",
        "        # Update learning rate if needed\n",
        "        scheduler.step(train_loss)\n",
        "\n",
        "        test_loss, test_acc = test(model, device, test_data_loader, criterion)\n",
        "        # Terminate training if loss stopped to decrease\n",
        "        if early_stopping(test_loss):\n",
        "            print('\\nEarly stopping\\n')\n",
        "            break\n",
        "        # Deep copy the weight of model if its accuracy is the best for now\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if writing:\n",
        "            writer.add_scalars('Loss',\n",
        "                            {\n",
        "                                'train': train_loss,\n",
        "                                'test': test_loss\n",
        "                            },\n",
        "                            epoch)\n",
        "\n",
        "            writer.add_scalars('Accuracy',\n",
        "                            {\n",
        "                                'train': train_acc,\n",
        "                                'test': test_acc\n",
        "                            },\n",
        "                            epoch)\n",
        "        else:\n",
        "            print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
        "            print(f\"Training loss {train_loss}, test loss {test_loss}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"model.pt\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    if writing:\n",
        "        writer.close()\n",
        "\n",
        "training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO3nuxFRKvdW"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In this lab, you will implement advanced techniques to improve the training of neural networks. By using data augmentation, batch normalization, dropout, early stopping, learning rate scheduling, and TensorBoard, you will enhance the efficiency, reliability, and performance of your neural network models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoZUi6yXKvdW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Machine Learning\n",
    "## Lesson 4 Naive Bayes Classifier, KNN, Cross-validation, Regularization\n",
    "## Introduction\n",
    "\n",
    "In this lab work, we will be introduced to basic methods for classification and evaluation of machine learning models such as Naive Bayesian classifier (Naive Bayes), nearest neighbor method (KNN), cross-validation and regularization. Learn to apply these methods to solve classification problems on real data.\n",
    "\n",
    "\n",
    "### Objectives:\n",
    "1. Lasso and Ridge\n",
    "2. Naïve Bayes\n",
    "3. KNN\n",
    "4. Cross Validation\n",
    "\n",
    "---\n",
    "## Lasso and Ridge\n",
    "Both models are the regularized forms of the linear regression.\n",
    "Lass with L1 regularization and Ridge with L2 regularization.\n",
    "Both act as a constraint region for the coefficients/weight, where they must reside in.\n",
    "\n",
    "### Issues:\n",
    "1. When to use Lasso?\n",
    "\n",
    "2. When to use Ridge?\n",
    "\n",
    "3. Since it is hard to decide the parameters influence, How we can decide which regularization? and decide the value of lambda?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Pandas](https://pandas.pydata.org/) - For data analysis and manipulation\n",
    "\n",
    "[Numpy](https://numpy.org/) - To deal with matrices\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) - Scikit-learn is one of the most widely used Python packages for Data Science and Machine Learning. It allows you to perform many operations and provides many algorithms. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Boston dataset\n",
    "Preparing data\n",
    "\n",
    "### About dataset:\n",
    " The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    " prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics...', Wiley, 1980.   N.B. Various transformations are used in the table on pages 244-261 of the latter.\n",
    "\n",
    " Variables in order:\n",
    " CRIM   -  per capita crime rate by town\n",
    " ZN    -  proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    " INDUS  -  proportion of non-retail business acres per town\n",
    " CHAS   -  Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    " NOX   -   nitric oxides concentration (parts per 10 million)\n",
    " RM    -   average number of rooms per dwelling\n",
    " AGE   -   proportion of owner-occupied units built prior to 1940\n",
    " DIS   -   weighted distances to five Boston employment centres\n",
    " RAD   -   index of accessibility to radial highways\n",
    " TAX   -   full-value property-tax rate per 10000 dollars\n",
    " PTRATIO - pupil-teacher ratio by town\n",
    " B    -    1000 (Bk - 0.63) ^2 where Bk is the proportion of blacks by town\n",
    " LSTAT  -  % lower status of the population\n",
    " MEDV   -  Median value of owner-occupied homes in $1000s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# This dataset is biased, but we will use it for educational purposes\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/8, random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fitting both Lasso and Ridge\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "Fit two models: Lasso and Ridge - with the default alpha-.\n",
    "Then print their coefficients and notice the difference.\n",
    "\n",
    "use function **fit()**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# write code here\n",
    "lasso = None\n",
    "ridge = None\n",
    "\n",
    "print(\"Lasso Coefficient:\", *lasso.coef_, sep='\\n\\t')\n",
    "print(\"Ridge Coefficient:\", *ridge.coef_, sep='\\n\\t')\n",
    "print('Sum of lasso abs values:', np.sum(np.abs(lasso.coef_)))\n",
    "print('Sum of ridge abs values:', np.sum(np.abs(ridge.coef_)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2:\n",
    "We need to regularize the Lasso and Ridge models by analyzing the effect of different values of the regularization parameter alpha on the mean square error (MSE). As a result, we will determine the optimal values of alpha that minimize the error for each of the models.\n",
    "\n",
    "use **Lasso**, **Ridge**, **fit()**, **predict()**, **mse_squared_error()**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "lasso_alphas = [0.3, 0.5, 1, 1.1, 1.2, 1.3, 1.5, 2, 2.2, 2.5]\n",
    "ridge_alphas = [50, 200, 300, 350, 400, 500, 600, 700, 1000, 1200]\n",
    "lasso_losses = []\n",
    "ridge_losses = []\n",
    "for i in range(len(lasso_alphas)):\n",
    "    # Create a Lasso regressor with the alpha value.\n",
    "    # Fit it to the training set, then get the prediction of the validation set (x_val).\n",
    "    # calculate the mean squared error loss, then append it to the losses array\n",
    "    lasso = None\n",
    "\n",
    "    y_pred = None\n",
    "    mse = None\n",
    "    lasso_losses.append(mse)\n",
    "\n",
    "    ridge = None\n",
    "\n",
    "    y_pred = None\n",
    "    mse = None\n",
    "    ridge_losses.append(mse)\n",
    "\n",
    "plt.suptitle('The effect of changing alpha on MSE for lasso and ridge')\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True)\n",
    "fig.suptitle('Aligning x-axis using sharex')\n",
    "\n",
    "ax1.plot(lasso_alphas, lasso_losses, label='lasso')\n",
    "ax1.legend()\n",
    "ax1.set(xlabel='alpha', ylabel='MSE')\n",
    "\n",
    "ax2.plot(ridge_alphas, ridge_losses, label='ridge')\n",
    "ax2.legend()\n",
    "ax2.set(xlabel='alpha', ylabel='MSE')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "lasso_best_alpha = lasso_alphas[np.argmin(lasso_losses)]\n",
    "ridge_best_alpha = ridge_alphas[np.argmin(ridge_losses)]\n",
    "print(\"Best value of alpha for lasso:\", lasso_best_alpha)\n",
    "print(\"Best value of alpha for ridge:\", ridge_best_alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Next, we do the following operations\n",
    "1) Determines the best alpha values for Lasso and Ridge models based on MSE minimization on a validation dataset.\n",
    "2) Trains Lasso and Ridge models with these best alpha values and evaluates their performance on a test dataset.\n",
    "3) Investigates the effect of different alpha values on the coefficients of the Lasso and Ridge models, displaying the results in graphs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso = Lasso(lasso_best_alpha)\n",
    "lasso.fit(x_train, y_train)\n",
    "y_pred = lasso.predict(x_test)\n",
    "print(\"Lasso MSE on test set:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "ridge = Ridge(ridge_best_alpha)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)\n",
    "print(\"Ridge MSE on test set:\", mean_squared_error(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature_names =\n",
    "\n",
    "lasso_alphas = [1, 1.1, 1.2, 1.3, 1.5, 2, 2.2, 2.5, 3, 5]\n",
    "ridge_alphas = [100, 200, 300, 350, 400, 500, 600, 700, 1000, 1200, 2000, 3000]\n",
    "lasso_coefs_ = np.zeros((len(lasso_alphas), len(X[0])))\n",
    "ridge_coefs_ = np.zeros((len(ridge_alphas), len(X[0])))\n",
    "for i in range(len(lasso_alphas)):\n",
    "    lasso = Lasso(alpha=lasso_alphas[i])\n",
    "    lasso.fit(x_train, y_train)\n",
    "    lasso_coefs_[i] = lasso.coef_\n",
    "\n",
    "for i in range(len(ridge_alphas)):\n",
    "    ridge = Ridge(alpha=ridge_alphas[i])\n",
    "    ridge.fit(x_train, y_train)\n",
    "    ridge_coefs_[i] = ridge.coef_\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle('The effect of changing alpha on the coefficients of lasso and ridge')\n",
    "\n",
    "for idx in range(len(X[0])):\n",
    "    ax1.plot(lasso_alphas, lasso_coefs_[:, idx], label=f'feature {idx}')\n",
    "ax1.legend()\n",
    "ax1.set(xlabel='alpha', ylabel='coefs')\n",
    "\n",
    "for idx in range(len(X[0])):\n",
    "    ax2.plot(ridge_alphas, ridge_coefs_[:, idx], label=f'feature {idx}')\n",
    "ax2.legend()\n",
    "ax2.set(xlabel='alpha', ylabel='coefs')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the iris dataset\n",
    "We split the dataset into training and test samples and prepare the data for further use in training and evaluating the machine learning model. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, load_iris\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# We will show why we didn't split a validation set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naïve Bayes\n",
    "We will use the Gaussian Naïve Bayes, that deals -as a assumption- with the continous features as gaussian variables to compute their probability.\n",
    "\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}exp(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2})$$\n",
    "\n",
    "While $\\mu_y$ and $\\sigma_y^2$ are the mean and the variance of the feature $i$ for class $y$.\n",
    "\n",
    "Note: The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$.\n",
    "\n",
    "___\n",
    "What are the pros and cons of Naive bayes classifier?\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train a naive-bayes model and check the test accuracy.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gauss_nb = GaussianNB()\n",
    "gauss_nb.fit(x_train, y_train)\n",
    "y_pred = gauss_nb.predict(x_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K nearest neighbour classifier\n",
    "1. What are the pros and cons of KNN?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Let's do the same with KNN classifier.\n",
    "\n",
    "\n",
    "Rescale the features first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's tune the hyperparameter $n\\_neighbors$ in the KNN classifier object using the cross-validation.\n",
    "\n",
    "___\n",
    "## Cross Validation\n",
    "Cross validation comes as an alternative for the validation set splitting.\n",
    "\n",
    "Note: that's why we didn't make a validation set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We investigate the effect of the n_neighbors parameter on the classification accuracy of the K-Nearest Neighbors (KNN) model using cross-validation. For each value of K from a given range, the average cross-validation accuracy is calculated, and then a graph of this dependence is plotted. Based on the graph, the optimal value of K that provides the highest accuracy of the model is determined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "Ks = list(range(1, 20))\n",
    "cv_scores = []\n",
    "for K in Ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    scores = cross_val_score(knn, x_train, y_train,\n",
    "                             cv=7, scoring='accuracy')\n",
    "    avg_score = np.mean(scores)\n",
    "    cv_scores.append(avg_score)\n",
    "\n",
    "plt.title('The effect of changing K on accuracy')\n",
    "plt.plot(Ks, cv_scores)\n",
    "plt.xlabel('K')\n",
    "plt.xticks(Ks)\n",
    "plt.ylabel('CV Average accuracy')\n",
    "plt.show()\n",
    "print('Best K:', Ks[np.argmax(cv_scores)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In KNN classifier, there are several hyperparamters to tune, tuning them one\n",
    "by one is exhaustive approach. Let's try a better approach called GridSearchCV.\n",
    "\n",
    "### GridSearchCV\n",
    "In GridSearch Cross-validation, you give different values for each hyperparamter and it will try all combinations for you.\n",
    "At the end, it will return the best combination of hyperparamters that got the best cross-validation score.\n",
    "\n",
    "### Exercise 3:\n",
    "Use gridsearch to tune 3 hyperparameters:\n",
    "\n",
    "1. $n\\_neighbors$: [1, 2, . . ., 10]\n",
    "2. $weights$: ['uniform', 'distance']\n",
    "3. $metric$: ['euclidean', 'manhattan', 'chebyshev', 'cosine']\n",
    "\n",
    "Check this [link](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "for help.\n",
    "\n",
    "Then measure the accuracy on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Modify the next lines to run GridSearchCV with cv=7\n",
    "param_grid = {'n_neighbors':list(range(1, 11)),\n",
    "              'weights':['uniform', 'distance'],\n",
    "              'metric':['euclidean', 'manhattan', 'chebyshev', 'cosine']\n",
    "              }\n",
    "\n",
    "# create a GridSearch cross validation with cv=7,\n",
    "# and accuracy as scoring, and specify param_grid\n",
    "\n",
    "\n",
    "grid_search_clf = None\n",
    "# then train on the train dataset\n",
    "\n",
    "\n",
    "means = grid_search_clf.cv_results_['mean_test_score']\n",
    "stds = grid_search_clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search_clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        % (mean, std * 2, params))\n",
    "print()\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(grid_search_clf.best_params_)\n",
    "\n",
    "y_pred = grid_search_clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "When the hyper-parameter(s) range is big, grid-search becomes exponentially inefficient.\n",
    "What other approaches can we use to solve this problem?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further reading\n",
    "\n",
    "[L1 vs L2](https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning),\n",
    "What if we used something other than the norm? like $\\Sigma \\ln(w_j^2)$\n",
    "\n",
    "[Elastic net](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)\n",
    "\n",
    "[Huber regularization](http://www.stephanmandt.com/papers/ECML_2016.pdf),\n",
    "what if we switch the condition?\n",
    "\n",
    "[Pruned Cross Validation](https://piotrekga.github.io/Pruned-Cross-Validation/)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6df0ddd77085922c773681b1c23afa6ec355a7eb5a25c833f534ec75c0111436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "## Lesson 14 Clustering 2\n",
    "## Introduction\n",
    "\n",
    "In this lab, we will delve deeper into clustering techniques, exploring their nuances and applications beyond basic scenarios. Clustering is an unsupervised learning method that groups similar data points together, making it invaluable for tasks such as image segmentation, customer segmentation, and anomaly detection.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Learn about:\n",
    "\n",
    "- When K-means is not suitable\n",
    "- AgglomerativeClustering (Dendrograms, Hierarchical Clustering)\n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "### When K-means is not a good choice\n",
    "\n",
    "K-means clustering works well when clusters are spherical and of similar size. We will explore scenarios where data violates these assumptions and alternative methods become more appropriate.\n",
    "\n",
    "### AgglomerativeClustering\n",
    "\n",
    "AgglomerativeClustering is a hierarchical clustering technique that builds clusters by recursively merging the nearest clusters. We will visualize clustering structures using dendrograms and understand the principles of hierarchical clustering.\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "DBSCAN is a density-based clustering algorithm that identifies clusters of varying shapes and sizes based on density. We will learn how DBSCAN handles noise and outliers, making it suitable for complex datasets.\n",
    "\n",
    "In this lab, you will gain practical experience with these advanced clustering techniques and understand their strengths and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Kmeans is inefficient (from previous lab)\n",
    "\n",
    "### Exercise 1\n",
    "> Apply Kmean on the following datasets and interpret the results. First, complete the function `k_means_and_plot` to fit Kmeans and to visualize the results. Then, conclude on why Kmean is does work well on these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from copy import deepcopy\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, k=3, centers=None):\n",
    "        self.k = k\n",
    "        self.centers = centers\n",
    "\n",
    "    def _predict(self, X, centers):\n",
    "        distance = cdist(X, centers)\n",
    "        return np.argmin(distance, axis=1)\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self._predict(X, self.centers)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        k = self.k\n",
    "        c = X.shape[1]\n",
    "        if self.centers is None:\n",
    "            mean = np.mean(X, axis = 0)\n",
    "            std = np.std(X, axis = 0)\n",
    "            self.centers = np.random.randn(k, c) * std + mean\n",
    "        centers = self.centers\n",
    "        centers_new = deepcopy(self.centers) # Store new centers\n",
    "\n",
    "        error = -1\n",
    "\n",
    "        # When, after an update, the estimate of that center stays the same, exit loop\n",
    "        while error != 0:\n",
    "            clusters = self._predict(X, centers)\n",
    "            centers_old = deepcopy(centers_new)\n",
    "            for i in range(k):\n",
    "                centers_new[i] = np.mean(X[clusters == i], axis=0)\n",
    "                \n",
    "\n",
    "            error = np.linalg.norm(centers_new - centers_old)\n",
    "            centers = centers_new\n",
    "        self.centers = centers_new\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "def choose(X, prob):\n",
    "    idx = choice(X.shape[0], 1, p=prob)\n",
    "    return X[idx]     \n",
    "\n",
    "def kmeans_pp(X, k):\n",
    "    n = X.shape[0]\n",
    "    weights = np.ones(n) / n\n",
    "    centers = []\n",
    "    while len(centers) < k:\n",
    "        # Choose a centroid with the current weights\n",
    "        # Type your code here\n",
    "\n",
    "        # Calculate the pair-wise distances\n",
    "        # between the datapoints X and the current centers\n",
    "        # get min distance then square it.\n",
    "        # Obtain new probabilities in weights.\n",
    "        # Type your code here\n",
    "\n",
    "    return np.array(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_and_plot(X, y, k=2):\n",
    "    #Write a function k_means_and_plot(X, y, k=2) that:\n",
    "    # - Uses the k-means method to partition the data X labeled y into k clusters.\n",
    "    # - Initializes the initial centers of the clusters using the k-means++ method.\n",
    "    # - Visualizes the clustering results using the plot_decision_regions function, displaying the cluster separation and cluster centers.\n",
    "    # - Adds a legend to the plot to label clusters and centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles \n",
    "n_samples = 1500\n",
    "X, y = make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "plt.scatter(X[:,0], X[:,1], marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_and_plot(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y_aniso = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y_aniso)\n",
    "plt.scatter(X_aniso[:,0], X_aniso[:,1], marker='.', c=y_aniso) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_and_plot(X_aniso, y_aniso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Imagine a scenario in which you are part of a data science team that interfaces with the marketing department. Marketing has been gathering customer shopping data for a while, and they want to understand, based on the collected data, if there are similarities between customers. Those similarities divide customers into groups and having customer groups helps in the targeting of campaigns, promotions, conversions, and building better customer relationships.\n",
    "\n",
    "In this case, our marketing data is fairly small. We have information on only 200 customers. Considering the marketing team, it is important that we can clearly explain to them how the decisions were made based on the number of clusters, therefore explaining to them how the algorithm actually works.\n",
    "\n",
    "Since our data is small and explicability is a major factor, we can leverage Hierarchical Clustering to solve this problem. This process is also known as Hierarchical Clustering Analysis (HCA).\n",
    "\n",
    "> One of the advantages of HCA is that it is interpretable and works well on small datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_name = 'hierarchical-clustering-with-python-and-scikit-learn-shopping-data.csv'\n",
    "\n",
    "customer_data = pd.read_csv(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that marketing has generated a CustomerID, gathered the Genre, Age, Annual Income (in thousands of dollars), and a Spending Score going from 1 to 100 for each of the 200 customers. When asked for clarification, they said that the values in the Spending Score column signify how often a person spends money in a mall on a scale of 1 to 100. In other words, if a customer has a score of 0, this person never spends money, and if the score is 100, we have just spotted the highest spender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['Spending Score (1-100)'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['Genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Variables and Feature Engineering\n",
    "\n",
    "Let's start by dividing the Age into groups that vary in 10, so that we have 20-30, 30-40, 40-50, and so on. Since our youngest customer is 15, we can start at 15 and end at 70, which is the age of the oldest customer in the data. Starting at 15, and ending at 70, we would have 15-20, 20-30, 30-40, 40-50, 50-60, and 60-70 intervals.\n",
    "\n",
    "To group or bin Age values into these intervals, we can use the Pandas cut() method to cut them into bins and then assign the bins to a new Age Groups column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [15, 20, 30, 40, 50, 60, 70]\n",
    "col = customer_data['Age']\n",
    "customer_data['Age Groups'] = pd.cut(x=col, bins=intervals)\n",
    "\n",
    "# To be able to look at the result stored in the variable\n",
    "customer_data['Age Groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.groupby('Age Groups', observed=False)['Age Groups'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to spot that most customers are between 30 and 40 years of age, followed by customers between 20 and 30 and then customers between 40 and 50. This is also good information for the Marketing department.\n",
    "\n",
    "At the moment, we have two categorical variables, Age and Genre, which we need to transform into numbers to be able to use in our model. There are many different ways of making that transformation - we will use the Pandas get_dummies() method that creates a new column for each interval and genre and then fill its values with 0s and 1s- this kind of operation is called one-hot encoding. Let's see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The _oh means one-hot\n",
    "customer_data_oh = pd.get_dummies(customer_data)\n",
    "# Display the one-hot encoded dataframe\n",
    "customer_data_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Each Pair of Data\n",
    "\n",
    "Since plotting 10 dimensions is a bit impossible, we'll plot the combination of the initial features. We can choose two of them for our clustering analysis. One way we can see all of our data pairs combined is with a Seaborn pairplot():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Dropping CustomerID column from data \n",
    "customer_data = customer_data.drop('CustomerID', axis=1)\n",
    "\n",
    "# plot the pair plot that shows the relationship between the features\n",
    "# 1 line\n",
    "\n",
    "sns.pairplot(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, we can spot the scatterplots that seem to have groups of data. One that seems interesting is the scatterplot that combines Annual Income and Spending Score. Notice that there is no clear separation between other variable scatterplots. At the most, we can maybe tell that there are two distinct concentrations of points in the Spending Score vs Age scatterplot.\n",
    "\n",
    "Both scatterplots consisting of Annual Income and Spending Score are essentially the same. We can see it twice because the x and y-axis were exchanged. By taking a look at any of them, we can see what appears to be five different groups. Let's plot just those two features with a Seaborn scatterplot() to take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=customer_data['Annual Income (k$)'],\n",
    "                y=customer_data['Spending Score (1-100)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using theese two features for our clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Hierarchical Structure with Dendrograms\n",
    "\n",
    "So far, we have explored the data, one-hot encoded categorical columns, decided which columns were fit for clustering. The plots indicate we have 5 clusters in our data, but there's also another way to visualize the relationships between our points and help determine the number of clusters - by creating a dendrogram (commonly misspelled as dendogram). Dendro means tree in Latin.\n",
    "\n",
    "The dendrogram is a result of the linking of points in a dataset. It is a visual representation of the hierarchical clustering process. And how does the hierarchical clustering process work? Well... it depends - probably an answer you've already heard a lot in Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Hierarchical Clustering\n",
    "When the Hierarchical Clustering Algorithm (HCA) starts to link the points and find clusters, it can first split points into 2 large groups, and then split each of those two groups into smaller 2 groups, having 4 groups in total, which is the divisive and top-down approach.\n",
    "\n",
    "Alternatively, it can do the opposite - it can look at all the data points, find 2 points that are closer to each other, link them, and then find other points that are the closest ones to those linked points and keep building the 2 groups from the bottom-up. Which is the agglomerative approach we will develop.\n",
    "\n",
    "#### Steps to Perform Agglomerative Hierarchical Clustering\n",
    "\n",
    "To make the agglomerative approach even clear, there are steps of the Agglomerative Hierarchical Clustering (AHC) algorithm:\n",
    "\n",
    "1. At the start, treat each data point as one cluster. Therefore, the number of clusters at the start will be K - while K is an integer representing the number of data points.\n",
    "\n",
    "2. Form a cluster by joining the two closest data points resulting in K-1 clusters.\n",
    "\n",
    "3. Form more clusters by joining the two closest clusters resulting in K-2 clusters.\n",
    "\n",
    "4. Repeat the above three steps until one big cluster is formed.\n",
    "\n",
    "> If you invert the steps of the ACH algorithm, going from 4 to 1 - those would be the steps to *Divisive Hierarchical Clustering (DHC)*.\n",
    "\n",
    "\n",
    "Notice that HCAs can be either divisive and top-down, or agglomerative and bottom-up. The top-down DHC approach works best when you have fewer, but larger clusters, hence it's more computationally expensive. On the other hand, the bottom-up AHC approach is fitted for when you have many smaller clusters. It is computationally simpler, more used, and more available.\n",
    "\n",
    "### Exercise 1.1\n",
    "Let's plot our customer data dendrogram to visualize the hierarchical relationships of the data. This time, we will use the scipy library to create the dendrogram for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Customers Dendrogram\")\n",
    "\n",
    "# Selecting Annual Income and Spending Scores by index\n",
    "selected_data = customer_data_oh.iloc[:, 2:4]\n",
    "\n",
    "# todo: calculate the dendrogram using the selected data, \n",
    "# and the ward method (let the metric be euclidean), and plot it\n",
    "# 3 lines\n",
    "\n",
    "clusters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage Methods\n",
    "There are many other linkage methods, by understanding more about how they work, you will be able to choose the appropriate one for your needs. Besides that, each of them will yield different results when applied. There is not a fixed rule in clustering analysis, if possible, study the nature of the problem to see which fits its best, test different methods, and inspect the results.\n",
    "\n",
    "## Question\n",
    "**What linkage methods do you know?**\n",
    "\n",
    "#### Distance Metrics\n",
    "\n",
    "Besides the linkage, we can also specify some of the most used distance metrics\n",
    "\n",
    "## Question\n",
    "**What distance metrics do you know?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen Ward and Euclidean for the dendrogram because they are the most commonly used method and metric. They usually give good results since Ward links points based on minimizing the errors, and Euclidean works well in lower dimensions.\n",
    "\n",
    "In this example, we are working with two features (columns) of the marketing data, and 200 observations or rows. Since the number of observations is larger than the number of features (200 > 2), we are working in a low-dimensional space.\n",
    "\n",
    "If we were to include more attributes, so we have more than 200 features, the Euclidean distance might not work very well, since it would have difficulty in measuring all the small distances in a very large space that only gets larger. In other words, the Euclidean distance approach has difficulties working with the data sparsity. This is an issue that is called the curse of dimensionality. The distance values would get so small, as if they became \"diluted\" in the larger space, distorted until they became 0.\n",
    "\n",
    "Finding an interesting number of clusters in a dendrogram is the same as finding the largest horizontal space that doesn't have any vertical lines (the space with the longest vertical lines). This means that there's more separation between the clusters.\n",
    "\n",
    "### Exercise 1.2\n",
    "We can draw a horizontal line that passes through that longest distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Customers Dendogram with line\")\n",
    "\n",
    "# todo: calculate the dendrogram using the selected data,\n",
    "# and the ward method (let the metric be euclidean), and plot it\n",
    "# also add the horizontal line (let the y be 125) to the plot that cuts the dendrogram\n",
    "# 3 lines\n",
    "\n",
    "clusters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "**How many clusters do we have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "## Implementing an Agglomerative Hierarchical Clustering\n",
    "### Using Original Data\n",
    "So far we've calculated the suggested number of clusters for our dataset that corroborate with our initial analysis. Now we can create our agglomerative hierarchical clustering model using Scikit-Learn AgglomerativeClustering and find out the labels of marketing points with labels_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# implement the AgglomerativeClustering class with the following parameters:\n",
    "# n_clusters=5, affinity='euclidean', linkage='ward'\n",
    "# then fit the selected data\n",
    "clustering_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have investigated a lot to get to this point. And what does these labels mean? Here, we have each point of our data labeled as a group from 0 to 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = clustering_model.labels_\n",
    "\n",
    "# plot the scatter plot of the selected data, and color it by the data_labels\n",
    "# 1 line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also apply the HCA on the dataset that Kmeans was performing poorly. We will load the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# Set up cluster parameters\n",
    "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how different linkage methods affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 1\n",
    "\n",
    "default_base = {'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (varied, {}),\n",
    "    (aniso, {}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    # todo: create the AgglomerativeClustering object with the parameters:\n",
    "    # n_clusters=params['n_clusters'], linkage - based on the name of the variable\n",
    "    \n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward')\n",
    "    complete = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='complete')\n",
    "    average = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='average')\n",
    "    single = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='single')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('Single Linkage', single),\n",
    "        ('Average Linkage', average),\n",
    "        ('Complete Linkage', complete),\n",
    "        ('Ward Linkage', ward),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=8)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=8,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "### DBSCAN\n",
    "\n",
    "## Question\n",
    "**What's the pros and cons of DBSCAN?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                }\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('KMeans', ms),\n",
    "        ('DBSCAN', dbscan),\n",
    "\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        \n",
    "        # todo: train the appropriate model on the data\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # todo: perform the prediction using the trained model\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right'\n",
    "                )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.subplots_adjust(right=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/\n",
    "- https://towardsdatascience.com/dbscan-clustering-explained-97556a2ad556\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we explored advanced clustering techniques beyond K-means, which is limited to spherical clusters of similar size. Here's a summary:\n",
    "\n",
    "#### When K-means is not a good choice\n",
    "K-means struggles with irregularly shaped clusters or those of varying sizes. For such scenarios, AgglomerativeClustering and DBSCAN offer better alternatives.\n",
    "\n",
    "#### AgglomerativeClustering\n",
    "This hierarchical method merges nearest clusters iteratively and visualizes structures with dendrograms, accommodating diverse cluster shapes and sizes.\n",
    "\n",
    "#### DBSCAN\n",
    "DBSCAN identifies clusters based on data density, making it robust against noise and outliers. It excels in handling datasets with irregular shapes and varying densities.\n",
    "\n",
    "Mastering these techniques enhances your ability to analyze complex data effectively, providing insights crucial for various applications like image segmentation and anomaly detection in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "6df0ddd77085922c773681b1c23afa6ec355a7eb5a25c833f534ec75c0111436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

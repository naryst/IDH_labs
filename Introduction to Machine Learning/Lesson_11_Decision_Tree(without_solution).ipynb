{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkmTNhw2iICO"
      },
      "source": [
        "# Lesson 11: Decision Tree\n",
        "in this lab we are going to study the following :\n",
        "\n",
        "- Advantages / Disadvantages of DT\n",
        "- The dataset types where DT performs better\n",
        "- Implementing DT on a classification problem\n",
        "- Sensitivity of the tree to the training set details\n",
        "- Improving DT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6lZpevhiICR"
      },
      "source": [
        "## Recap\n",
        "\n",
        "1. What are advantages of DT?\n",
        "\n",
        "2. What are disadvantages of DT?\n",
        "\n",
        "3. Question: The difference between Neural networks and DT regarding their approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKr3_h3iICS"
      },
      "source": [
        "## Dataset:\n",
        "\n",
        "- Let's load the IRIS dataset and see how other classifiers, such as ANN perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iHa_qX4iICS"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_text\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']\n",
        "\n",
        "# Scale data to have mean 0 and variance 1\n",
        "# which is importance for convergence of the neural network\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visualizing the Data"
      ],
      "metadata": {
        "id": "UZ8hPEjotkmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwQstv5tiICU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "for target, target_name in enumerate(names):\n",
        "    X_plot = X[y == target]\n",
        "    ax1.plot(X_plot[:, 0], X_plot[:, 1],\n",
        "             linestyle='none',\n",
        "             marker='o',\n",
        "             label=target_name)\n",
        "ax1.set_xlabel(feature_names[0])\n",
        "ax1.set_ylabel(feature_names[1])\n",
        "ax1.axis('equal')\n",
        "ax1.legend()\n",
        "\n",
        "for target, target_name in enumerate(names):\n",
        "    X_plot = X[y == target]\n",
        "    ax2.plot(X_plot[:, 2], X_plot[:, 3],\n",
        "             linestyle='none',\n",
        "             marker='o',\n",
        "             label=target_name)\n",
        "ax2.set_xlabel(feature_names[2])\n",
        "ax2.set_ylabel(feature_names[3])\n",
        "ax2.axis('equal')\n",
        "ax2.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Implementing an Artificial Neural Network (ANN)"
      ],
      "metadata": {
        "id": "Gmj8scbXtrrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqIVq9N9iICU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer2 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training the ANN"
      ],
      "metadata": {
        "id": "fN5XCYaAtvM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB-7tb8NiICV"
      },
      "outputs": [],
      "source": [
        "model     = Model(X_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn   = nn.CrossEntropyLoss()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Bcmp1miICV"
      },
      "source": [
        "Now its time to run the training. In order to track progress more efficiently, we can use tqdm, which is a great and easy to use progress bar for our training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGpy2KVriICW"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "EPOCHS  = 100\n",
        "X_train = Variable(torch.from_numpy(X_train)).float()\n",
        "y_train = Variable(torch.from_numpy(y_train)).long()\n",
        "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
        "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
        "\n",
        "loss_list     = np.zeros((EPOCHS,))\n",
        "accuracy_list = np.zeros((EPOCHS,))\n",
        "\n",
        "for epoch in tqdm.trange(EPOCHS):\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss_list[epoch] = loss.item()\n",
        "\n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "        accuracy_list[epoch] = correct.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMRb2aJSiICW"
      },
      "source": [
        "## Plot Accuracy and Loss from Training\n",
        "Letâ€™s have a look how our models perform. We can clearly see that adding more nodes makes the training perform better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdtdPaW1iICX"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax1.plot(accuracy_list)\n",
        "ax1.set_ylabel(\"validation accuracy\")\n",
        "ax2.plot(loss_list)\n",
        "ax2.set_ylabel(\"validation loss\")\n",
        "ax2.set_xlabel(\"epochs\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbwxkVYFiICY"
      },
      "source": [
        "The ANN model achieved good accuracy on the test set, but we had to spend some effort on the dataset. In reality, we have to spend even more effort on fine-tuning the hyperparameters and prepare the dataset (standartization, normalization). Also, the output from the ANN model that we trained is one-hot vector which should be converted back into category values.\n",
        "\n",
        "What about Decision Tree? We will try to implement decision tree along with data preparation steps and see how simpler is working with DT comparing to other machine learning algorithms, such as ANN."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 1:\n",
        "\n",
        "complete the following code. write one line to train the the decision tree on the train_x train_y train sets"
      ],
      "metadata": {
        "id": "XaIRrlDrl4e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTqjGBxEiICY"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# instantiate the DecisionTreeClassifier model with criterion gini index\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "\n",
        "dt_clf = # your code here\n",
        "\n",
        "\n",
        "score = dt_clf.score(test_x, test_y)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK-_31f9iICY"
      },
      "source": [
        "As you can see, within a few lines we can train the Decision Tree classifier. Also, pay attention that, we did not spend any effort on data preparation (i.e. converting labels into one-hot encoding etc.)\n",
        "We can also have more understanding the performance of our decision tree model by building a text report showing the rules of a decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERlUQBRoiICY"
      },
      "outputs": [],
      "source": [
        "r = export_text(dt_clf, feature_names=iris['feature_names'])\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WABB-QTiICY"
      },
      "source": [
        "Now let's see what kind of trees can we build with Sklearn.\n",
        "\n",
        "If you have some problems with running the following code, make sure that you installed the graphviz (see instructions: https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLlQuABliICY"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "def plot_tree(clf):\n",
        "    dot_data = tree.export_graphviz(clf, out_file=None,\n",
        "                       feature_names=iris.feature_names,\n",
        "                       class_names=iris.target_names,\n",
        "                       filled=True, rounded=True,\n",
        "                       special_characters=True)\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    return graph\n",
        "graph = plot_tree(dt_clf)\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnhjQiT8iICY"
      },
      "source": [
        "### Q/A\n",
        "\n",
        "1.  What do leaves and branches represent in Decision trees?\n",
        "\n",
        "2. When do we stop splitting the tree?\n",
        "\n",
        "3. What is Entropy?\n",
        "\n",
        "4. What is Information Gain?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVICIi1UiICZ"
      },
      "source": [
        "Let's use a smaller Decison Tree and try to recreate some of the values on it.\n",
        "\n",
        "Calculate the gini and entropy of the second branch (Petal width (cm) <= 1.75).\n",
        "you can hover over the branches to read their number.\n",
        "\n",
        "<center>\n",
        "\n",
        "![link text](https://miro.medium.com/v2/resize:fit:640/format:webp/1*x5W_NTWoNeSTexV2PsFICQ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9swK0gIQiICZ"
      },
      "outputs": [],
      "source": [
        "decision_tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
        "decision_tree = decision_tree.fit(X, y)\n",
        "plot_tree(decision_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS30bFqSiICZ"
      },
      "source": [
        "Let's implement the gini and entropy functions ourselves, and see how they are implemented like.\n",
        "\n",
        "##Recall the definitions.\n",
        "The Gini Index is determined by deducting the sum of squared of probabilities of each class from one, mathematically, Gini Index can be expressed as:\n",
        "\n",
        "\n",
        "![link text](https://miro.medium.com/v2/resize:fit:540/format:webp/1*Nnv80e30rA69z_jEJGmLbw.png)\n",
        "\n",
        "\n",
        "Information Gain is applied to quantify which feature provides maximal information about the classification based on the notion of entropy, i.e. by quantifying the size of uncertainty, disorder or impurity, in general, with the intention of decreasing the amount of entropy initiating from the top (root node) to bottom(leaves nodes).\n",
        "\n",
        "The information gain takes the product of probabilities of the class with a log having base 2 of that class probability, the formula for Entropy is given below:\n",
        "\n",
        "\n",
        "\n",
        "![link text](https://miro.medium.com/v2/resize:fit:640/format:webp/1*ySURWwv0BpvsYm7flH4rbw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 2:\n",
        "\n",
        "complete the following code. write the body of this function that calculates the gini from an numpy array and function that calculates the entropy of an numpy array."
      ],
      "metadata": {
        "id": "kQuMVF6-mMkZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS52kJ7WiICZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def gini(value):\n",
        "    # .............your code is here. function that calculates the gini from an numpy array\n",
        "\n",
        "    return # gini\n",
        "def entropy(value):\n",
        "    # .............your code is here function that calculates the entropy of an numpy array\n",
        "    return # entropy\n",
        "\n",
        "value = np.array([50,50,0,10,0,10])\n",
        "print(\"gini: {:.03}, entropy: {:.04} \".format(gini(value),entropy(value)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAuWzXn9iICZ"
      },
      "source": [
        "Let's write a function that calculates the information gain that determines how good was the splitting process.\n",
        "\n",
        "Recall the information gain formula:\n",
        "\n",
        "\n",
        "\n",
        "![link text](https://miro.medium.com/v2/resize:fit:720/format:webp/1*Z3Nn9hlcDudba1TXEKaPWQ.png)\n",
        "\n",
        "- a represents a specific attribute or class label\n",
        "- Entropy(S) is the entropy of dataset, S\n",
        "- |Sv|/ |S| represents the proportion of the values in Sv to the number of values in dataset, S\n",
        "- Entropy(Sv) is the entropy of dataset, Sv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNXGstn_iICa"
      },
      "outputs": [],
      "source": [
        "def information_gain(parent, children):\n",
        "    total = parent.sum()\n",
        "    s = entropy(parent)\n",
        "    for child in children:\n",
        "        s -= child.sum()/total * entropy(child)\n",
        "    return s\n",
        "parent = np.array([50,50,50])\n",
        "children = np.array([[50,0,0],[0,50,50]])\n",
        "ig = information_gain(parent, children)\n",
        "print(\"information gain: {:.03}\".format(ig))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9A9ci92iICa"
      },
      "source": [
        "## Tree pruning\n",
        "\n",
        "A simple tree prediction can lead to a model which overfits the data and produce bad results with the test data. Tree Pruning is the way to reduce overfitting by creating smaller trees.\n",
        "\n",
        "Using the sklearn library, the DecisionTreeClassifier provides parameters such as min_samples_leaf and max_depth to prevent a tree from overfiting. Cost complexity pruning provides another option to control the size of a tree. In DecisionTreeClassifier, this pruning technique is parameterized by the cost complexity parameter, ccp_alpha. Greater values of ccp_alpha increase the number of nodes pruned. Here we only show the effect of ccp_alpha on regularizing the trees and how to choose a ccp_alpha based on validation scores.\n",
        "\n",
        "\n",
        "We will use simple data to check the effect of pruning on the Decision Tree. Letâ€™s first get the data and split it accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_HYzBIEiICa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhv2U40YiICa"
      },
      "source": [
        "To get an idea of what values of ccp_alpha could be appropriate, scikit-learn provides a method for DecisionTreeClassifier that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 3:\n",
        "\n",
        "complete the following code. write one line of code to get the a path variable which contains alphas and impurities"
      ],
      "metadata": {
        "id": "AsjBMDnpmn90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiVcuE-1iICa"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "\n",
        "path = ...........# your code here............\n",
        "\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs93YRoCiICb"
      },
      "source": [
        "Let's plot the relationship between alpha and impurities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0exvC_aBiICb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(ccp_alphas, impurities)\n",
        "ax.set_xlabel(\"effective alpha\")\n",
        "ax.set_ylabel(\"total impurity of leaves\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Di8FGeSi_w7"
      },
      "source": [
        "Finding an optimal value of alpha."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 4:\n",
        "\n",
        "complete the following code. write 2 lines of code to train the decision tree model on the train dataset and add the model into the clfs array"
      ],
      "metadata": {
        "id": "D39faNC9nfuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c07v0bxxiICb"
      },
      "outputs": [],
      "source": [
        "clfs = []\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "\n",
        "    # .........your code is here .............."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhq6VcXNiICc"
      },
      "source": [
        "As we already know that there is a strong relation between, alpha and the depth of the tree. We can find the relation using this plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtCqPipqiICc"
      },
      "outputs": [],
      "source": [
        "tree_depths = [clf.tree_.max_depth for clf in clfs]\n",
        "plt.figure(figsize=(10,  6))\n",
        "plt.plot(ccp_alphas[:-1], tree_depths[:-1])\n",
        "plt.xlabel(\"effective alpha\")\n",
        "plt.ylabel(\"total depth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZBCUKX8iICc"
      },
      "source": [
        "The following graph demonstrates the relationship between alpha and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 5:\n",
        "\n",
        "complete the following code. calculate the accuracy scores of the decision tree models in the clfs array, and add the scores into the acc_scores"
      ],
      "metadata": {
        "id": "M31fdxrSnuLZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn3cjSFiiICc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "acc_scores = # your code here, you can write more than one line\n",
        "\n",
        "tree_depths = [clf.tree_.max_depth for clf in clfs]\n",
        "plt.figure(figsize=(10,  6))\n",
        "plt.grid()\n",
        "plt.plot(ccp_alphas[:-1], acc_scores[:-1])\n",
        "plt.xlabel(\"effective alpha\")\n",
        "plt.ylabel(\"Accuracy scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akKUQx-AiICc"
      },
      "source": [
        "We can clearly see that somewhere around 0.013 alpha, we get a very good value of accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp8CZUTciICd"
      },
      "source": [
        "## Comparison Between models\n",
        "Now that you know multiple types of classifiers:\n",
        "1. Linear\n",
        "  * Logistic Regression\n",
        "  * SVM-(with linear kernel)\n",
        "2. Non-Linear\n",
        "  * SVM-(with non linear kernel)\n",
        "  * KNN\n",
        "  * Decision Trees\n",
        "  * Gaussian Naive Bayes\n",
        "  \n",
        "Let's take a look how their decision boundries look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4GpISyFiICd"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initializing Classifiers\n",
        "clf1 = LogisticRegression(random_state=1,\n",
        "                          solver='newton-cg',\n",
        "                          multi_class='multinomial')\n",
        "\n",
        "clf2 = DecisionTreeClassifier()\n",
        "clf3 = GaussianNB()\n",
        "clf4 = SVC(gamma='auto')\n",
        "\n",
        "# Loading some example data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, [0, 2]]\n",
        "y = iris.target\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.gridspec as gridspec\n",
        "import itertools\n",
        "\n",
        "gs = gridspec.GridSpec(2, 2)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "labels = ['Logistic Regression', 'Decision Tree', 'Naive Bayes', 'SVM']\n",
        "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
        "                         labels,\n",
        "                         itertools.product([0, 1], repeat=2)):\n",
        "    clf.fit(X, y)\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    plt.title(lab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Boundaries with Different Datasets"
      ],
      "metadata": {
        "id": "fMIakBKMuQRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsLZ0X43iICj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=100, random_state=123)\n",
        "\n",
        "gs = gridspec.GridSpec(2, 2)\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "labels = ['Logistic Regression', 'Decision Tree', 'Naive Bayes', 'SVM']\n",
        "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
        "                         labels,\n",
        "                         itertools.product([0, 1], repeat=2)):\n",
        "\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    plt.title(lab)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Another Dataset Example with Circles"
      ],
      "metadata": {
        "id": "Xk56FCTFuU2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8feufLJiICj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
        "\n",
        "gs = gridspec.GridSpec(2, 2)\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "labels = ['Logistic Regression', 'Decision Tree', 'Naive Bayes', 'SVM']\n",
        "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
        "                         labels,\n",
        "                         itertools.product([0, 1], repeat=2)):\n",
        "\n",
        "    clf.fit(X, y)\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    plt.title(lab)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Tree Regression Example\n"
      ],
      "metadata": {
        "id": "pYvuaH4DuYes"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGXY3zzViICj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random dataset\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
        "y = np.sin(X).ravel()\n",
        "y[::5] += 3 * (0.5 - rng.rand(16))\n",
        "\n",
        "# Fit regression model\n",
        "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
        "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
        "regr_1.fit(X, y)\n",
        "regr_2.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
        "y_1 = regr_1.predict(X_test)\n",
        "y_2 = regr_2.predict(X_test)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure()\n",
        "plt.scatter(X, y, s=20, edgecolor=\"black\",\n",
        "            c=\"darkorange\", label=\"data\")\n",
        "plt.plot(X_test, y_1, color=\"cornflowerblue\",\n",
        "         label=\"max_depth=2\", linewidth=2)\n",
        "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
        "plt.xlabel(\"data\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.title(\"Decision Tree Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion:\n",
        "In this lab, we explored the use of Decision Trees (DT) and Artificial Neural Networks (ANN) for classification tasks, specifically using the IRIS dataset. Here's a summary of the key learnings and insights:\n",
        "\n",
        "1. Advantages and Disadvantages of Decision Trees:\n",
        "\n",
        "Decision Trees are easy to understand and interpret, require minimal data preparation, and can handle both numerical and categorical data. However, they are prone to overfitting, can be unstable, and may not always produce the optimal solution.\n",
        "2. Comparing Decision Trees with Neural Networks:\n",
        "\n",
        "While Decision Trees offer simplicity and interpretability, Neural Networks are more powerful and can handle complex patterns but require extensive data preparation and tuning. Decision Trees serve as a great starting point and baseline model, whereas Neural Networks can be leveraged for more complex tasks.\n",
        "3. Implementation of Decision Trees and Neural Networks:\n",
        "\n",
        "We implemented both a Decision Tree classifier and a Neural Network on the IRIS dataset. The Decision Tree required minimal preprocessing and was quick to train, while the Neural Network required scaling of features and careful tuning of hyperparameters.\n",
        "4. Training and Evaluating Models:\n",
        "\n",
        "We trained the Neural Network using PyTorch and visualized the accuracy and loss over epochs, highlighting the importance of proper data preparation and training processes. For Decision Trees, we explored the training process, visualized the tree structure, and discussed the ease of interpreting the results.\n",
        "5. Pruning Decision Trees:\n",
        "\n",
        "To address overfitting, we applied pruning techniques to the Decision Tree model. We explored the impact of the pruning parameter (ccp_alpha) on the treeâ€™s performance and depth, demonstrating how pruning can improve model generalization.\n",
        "Comparison of Different Classifiers:\n",
        "\n",
        "By comparing Decision Trees with other classifiers such as Logistic Regression, Naive Bayes, and SVM, we gained insights into their decision boundaries and performance on different datasets. This comparison emphasized the importance of selecting the appropriate model based on the dataset characteristics.\n",
        "Decision Tree Regression:\n",
        "s"
      ],
      "metadata": {
        "id": "qvYkhc1Yulkr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObiJWwpfiICj"
      },
      "source": [
        "References:\n",
        "- https://scikit-learn.org/stable/modules/tree.html\n",
        "- https://towardsdatascience.com/when-and-why-tree-based-models-often-outperform-neural-networks-ceba9ecd0fd8\n",
        "- https://ranvir.xyz/blog/practical-approach-to-tree-pruning-using-sklearn/\n",
        "- https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
        "- https://janakiev.com/blog/pytorch-iris/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO0HFCJHiICj"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "6df0ddd77085922c773681b1c23afa6ec355a7eb5a25c833f534ec75c0111436"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
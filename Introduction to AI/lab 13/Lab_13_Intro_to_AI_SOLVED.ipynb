{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "RVj6b8Yej1tT",
      "metadata": {
        "id": "RVj6b8Yej1tT"
      },
      "source": [
        "# **Introduction to AI - Lab 13**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tuYT26Dlj1tX",
      "metadata": {
        "id": "tuYT26Dlj1tX"
      },
      "source": [
        "## **Neural Networks and Perceptrons**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9062c55",
      "metadata": {},
      "source": [
        "### Perceptron Training: Loss Function and Gradient Descent\n",
        "\n",
        "The perceptron is trained using a simple loss function known as the perceptron criterion. This criterion seeks to minimize the number of misclassified samples. The loss for a single sample is defined as:\n",
        "\n",
        "$\n",
        "\\text{Loss} = -y(x \\cdot W)\n",
        "$\n",
        "\n",
        "if the sample is misclassified, where $( y )$ is the true label and $( x )$ is the input vector. If the sample is correctly classified, the loss is zero.\n",
        "\n",
        "The training process involves updating the weights using gradient descent. The update rule for the weights is derived from minimizing the loss function:\n",
        "\n",
        "$\n",
        "W = W + \\eta \\cdot (y - \\hat{y}) \\cdot x\n",
        "$\n",
        "\n",
        "where:\n",
        "- $( \\eta )$ is the learning rate,\n",
        "- $( y )$ is the true label,\n",
        "- $( \\hat{y} )$ is the predicted label,\n",
        "- $( x )$ is the input vector.\n",
        "\n",
        "The perceptron update rule adjusts the weights to reduce the classification error for the misclassified samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zi3wFRK2j1tY",
      "metadata": {
        "id": "zi3wFRK2j1tY"
      },
      "source": [
        "### Motivation\n",
        "In this lab, we will explore the basics of neural networks, focusing on perceptrons. We will build and train a perceptron to perform binary classification on a simple dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8O0Kuov9j1tY",
      "metadata": {
        "id": "8O0Kuov9j1tY"
      },
      "source": [
        "### Components of a Neural Network\n",
        "- **Neurons:** Basic units of the network that perform computations.\n",
        "- **Activation Functions:** Functions applied to the neuron's output to introduce non-linearity.\n",
        "- **Layers:** Stacked neurons that form the architecture of the network.\n",
        "- **Weights:** Parameters that are adjusted during training to minimize the error.\n",
        "- **Biases:** Additional parameters that allow the activation function to be shifted left or right."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wnu_BTRzj1tZ",
      "metadata": {
        "id": "Wnu_BTRzj1tZ"
      },
      "source": [
        "### Perceptron\n",
        "A perceptron is the simplest type of neural network, consisting of a single layer of output nodes connected to the input nodes. It is a linear classifier used for binary classification tasks.\n",
        "- **Activation Function:** The perceptron uses a step function as its activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8OZs7XvWj1tZ",
      "metadata": {
        "id": "8OZs7XvWj1tZ"
      },
      "source": [
        "## **Task 1:** **Implementing a Perceptron**\n",
        "In this task, we will implement a simple perceptron from scratch and use it to classify data points from the Sklearn Moons dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YHF8z8gnj1ta",
      "metadata": {
        "id": "YHF8z8gnj1ta"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=42)\n",
        "\n",
        "# Plot the dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
        "plt.title('Moons Dataset')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sPpW13k_j1tc",
      "metadata": {
        "id": "sPpW13k_j1tc"
      },
      "source": [
        "### Perceptron Model\n",
        "We will define a perceptron model with a simple training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J4eWoEGUj1tc",
      "metadata": {
        "id": "J4eWoEGUj1tc"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, input_size, lr=0.1, epochs=1000):\n",
        "        self.W = np.zeros(input_size + 1)  # Initialize weights (including bias)\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation_fn(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = self.W.T.dot(np.insert(x, 0, 1))  # Insert bias term (1) into input\n",
        "        a = self.activation_fn(z)\n",
        "        return a\n",
        "\n",
        "    def compute_loss(self, y, y_hat):\n",
        "        # Compute the loss for a single sample\n",
        "        return -y * (y_hat - y)  # Simple perceptron loss\n",
        "\n",
        "    def update_weights(self, x, y, y_hat):\n",
        "        # Update the weights using the perceptron update rule\n",
        "        error = y - y_hat\n",
        "        self.W += self.lr * error * np.insert(x, 0, 1)\n",
        "\n",
        "    def train_epoch(self, X, y):\n",
        "        # Train for one epoch\n",
        "        for i in range(y.shape[0]):\n",
        "            y_hat = self.predict(X[i])\n",
        "            self.update_weights(X[i], y[i], y_hat)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Train the perceptron for the specified number of epochs\n",
        "        for _ in range(self.epochs):\n",
        "            self.train_epoch(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec6d352",
      "metadata": {},
      "source": [
        "### Derivation of the Perceptron Training Rule\n",
        "\n",
        "Starting with the perceptron criterion, the goal is to minimize the number of misclassified samples. The loss for a misclassified sample is:\n",
        "\n",
        "$\n",
        "\\text{Loss} = -y(x \\cdot W)\n",
        "$\n",
        "\n",
        "To minimize this loss, we adjust the weights using the gradient of the loss function with respect to the weights. The gradient is computed as:\n",
        "\n",
        "$\n",
        "\\frac{\\partial \\text{Loss}}{\\partial W} = -y \\cdot x\n",
        "$\n",
        "\n",
        "The weight update rule derived from gradient descent is:\n",
        "\n",
        "$\n",
        "W = W - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}\n",
        "$\n",
        "\n",
        "Substituting the gradient, we get:\n",
        "\n",
        "$\n",
        "W = W + \\eta \\cdot y \\cdot x\n",
        "$\n",
        "\n",
        "This leads to the final update rule used in the perceptron algorithm, where the update is applied only when a sample is misclassified:\n",
        "\n",
        "$\n",
        "W = W + \\eta \\cdot (y - \\hat{y}) \\cdot x\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HoZVtswLj1tc",
      "metadata": {
        "id": "HoZVtswLj1tc"
      },
      "source": [
        "### Training the Perceptron\n",
        "We will now train our perceptron on the Moons dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5-chffyFj1td",
      "metadata": {
        "id": "5-chffyFj1td"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the perceptron\n",
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "# Predict and plot decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = np.array([perceptron.predict(np.array([x1, x2])) for x1, x2 in zip(xx.ravel(), yy.ravel())])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
        "plt.title('Decision Boundary')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1jUKowtJj1td",
      "metadata": {
        "id": "1jUKowtJj1td"
      },
      "source": [
        "## **Conclusion**\n",
        "In this lab, we implemented a simple perceptron and used it to classify data points from the Moons dataset. We visualized the decision boundary of our perceptron and observed how it separates the data points into two classes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
